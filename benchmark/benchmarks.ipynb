{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d98322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import string\n",
    "import pathlib\n",
    "import random\n",
    "import threading\n",
    "import time\n",
    "from urllib.parse import urlsplit, urlunsplit\n",
    "import requests\n",
    "import json\n",
    "from py4j.protocol import Py4JJavaError, Py4JError\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9a04030-0a23-45df-8e70-6a296a4f582d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global configuration\n",
    "SPARK_MEMORY = 900\n",
    "SPARK_CORES = 60\n",
    "DBHOST = 'postgres'\n",
    "group_in_leaves = False\n",
    "QUERY_TIMEOUT = 60 * 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7397f3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"app\") \\\n",
    "        .master(f'local[{SPARK_CORES}]') \\\n",
    "        .config(\"spark.driver.memory\", f'{SPARK_MEMORY}g') \\\n",
    "        .config(\"spark.executor.memory\", f'{SPARK_MEMORY}g') \\\n",
    "        .config(\"spark.memory.offHeap.enabled\",False) \\\n",
    "        .config(\"spark.jars\", \"postgresql-42.3.3.jar\") \\\n",
    "        .getOrCreate()\n",
    "    if (group_in_leaves):\n",
    "        spark.sql(\"SET spark.sql.yannakakis.countGroupInLeaves = true\").show()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4170360b-134b-4619-915b-391e877bc203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metrics(spark, group_id):\n",
    "    parsed = list(urlsplit(spark.sparkContext.uiWebUrl))\n",
    "    host_port = parsed[1]\n",
    "    parsed[1] = 'localhost' + host_port[host_port.find(':'):]\n",
    "    API_URL = f'{urlunsplit(parsed)}/api/v1'\n",
    "\n",
    "    app_id = spark.sparkContext.applicationId\n",
    "    sql_queries = requests.get(API_URL + f'/applications/{app_id}/sql', params={'length': '100000'}).json()\n",
    "    query_ids = [q['id'] for q in sql_queries if q['description'] == group_id]\n",
    "    if (len(query_ids) == 0):\n",
    "        print(f'query with group {group_id} not found')\n",
    "        return None\n",
    "    query_id = query_ids[0]\n",
    "    print(f'query id: {query_id}')\n",
    "    \n",
    "    query_details = requests.get(API_URL + f'/applications/{app_id}/sql/{query_id}',\n",
    "                                 params={'details': 'true', 'planDescription': 'true'}).json()\n",
    "    \n",
    "    success_job_ids = query_details['successJobIds']\n",
    "    running_job_ids = query_details['runningJobIds']\n",
    "    failed_job_ids = query_details['failedJobIds']\n",
    "    \n",
    "    job_ids = success_job_ids + running_job_ids + failed_job_ids\n",
    "    \n",
    "    job_details = [requests.get(API_URL + f'/applications/{app_id}/jobs/{jid}').json() for jid in job_ids]\n",
    "    \n",
    "    job_stages = {}\n",
    "    \n",
    "    for j in job_details:\n",
    "        stage_ids = j['stageIds']\n",
    "        \n",
    "        stage_params = {'details': 'true', 'withSummaries': 'true'}\n",
    "        stages = [requests.get(API_URL + f'/applications/{app_id}/stages/{sid}', stage_params) for sid in stage_ids]\n",
    "        \n",
    "        job_stages[j['jobId']] = [stage.json() for stage in stages if stage.status_code == 200] # can be 404\n",
    "    \n",
    "    return query_details, job_details, job_stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acbbca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_db(spark, dbname):\n",
    "    \n",
    "    username = dbname\n",
    "    password = dbname\n",
    "    dbname = dbname\n",
    "\n",
    "    df_tables = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", f'jdbc:postgresql://{DBHOST}:5432/{dbname}') \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"dbtable\", \"information_schema.tables\") \\\n",
    "    .option(\"user\", username) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .load()\n",
    "\n",
    "    for idx, row in df_tables.toPandas().iterrows():\n",
    "        if row.table_schema == 'public':\n",
    "            table_name = row.table_name\n",
    "            df = spark.read.format(\"jdbc\") \\\n",
    "                .option(\"url\", f'jdbc:postgresql://{DBHOST}:5432/{dbname}') \\\n",
    "                .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "                .option(\"dbtable\", table_name) \\\n",
    "                .option(\"user\", username) \\\n",
    "                .option(\"password\", password) \\\n",
    "                .load()\n",
    "    \n",
    "            print(table_name)\n",
    "            #print(df.show())\n",
    "            df.createOrReplaceTempView(table_name)\n",
    "\n",
    "def random_str(size=16, chars=string.ascii_uppercase + string.digits):\n",
    "    return ''.join(random.choice(chars) for _ in range(size))\n",
    "\n",
    "def set_group_id(spark):\n",
    "    group_id = random_str()\n",
    "    spark.sparkContext.setJobGroup(group_id, group_id)\n",
    "    return group_id\n",
    "\n",
    "def cancel_query(seconds, group_id):\n",
    "    time.sleep(seconds)\n",
    "    print(\"cancelling jobs with id \" + group_id)\n",
    "    print(spark.sparkContext.cancelJobGroup(group_id))\n",
    "    print(\"cancelled job\")\n",
    "\n",
    "def cancel_query_after(spark, seconds):\n",
    "    group_id = random_str()\n",
    "    spark.sparkContext.setJobGroup(group_id, group_id)\n",
    "    threading.Thread(target=cancel_query, args=(seconds,group_id,)).start()\n",
    "    return group_id\n",
    "    \n",
    "def run_query(file):\n",
    "    with open(file, 'r') as f:\n",
    "        query = '\\n'.join(filter(lambda line: not line.startswith('limit') and not line.startswith('-'), f.readlines()))\n",
    "        \n",
    "        print(\"running query: \\n\" + query)\n",
    "        return spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ead489e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_query(spark, query, respath, run):\n",
    "    start_time = time.time()\n",
    "\n",
    "    group_id = cancel_query_after(spark, QUERY_TIMEOUT)\n",
    "    df1 = run_query(query)\n",
    "    df1.show()\n",
    "\n",
    "    end_time = time.time()\n",
    "    diff_time = end_time - start_time\n",
    "\n",
    "    execution, jobs, job_stages = extract_metrics(spark, group_id)\n",
    "\n",
    "    if execution is not None:\n",
    "            with open(respath + f'/execution-{run}.json', 'w') as f:\n",
    "                f.write(json.dumps(execution, indent=2))\n",
    "            with open(respath + f'/jobs-{run}.json', 'w') as f:\n",
    "                f.write(json.dumps(jobs, indent=2))\n",
    "            with open(respath + f'/stages-{run}.json', 'w') as f:\n",
    "                f.write(json.dumps(job_stages, indent=2))\n",
    "    return diff_time\n",
    "\n",
    "def benchmark(spark, dbname, query_file, mode, run):\n",
    "    #spark.sql(\"SET spark.sql.yannakakis.enabled = false\").show()\n",
    "    # run the query once to warm up Spark (load the relation in memory)\n",
    "    #df0 = run_query(query)\n",
    "    #df0.show()\n",
    "    \n",
    "    query_name = os.path.basename(query_file)\n",
    "\n",
    "    respath = f'benchmark-results-{dbname}/' + query_name + \"/\" + mode\n",
    "    pathlib.Path(respath).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if mode == \"opt\":\n",
    "        spark.sql(\"SET spark.sql.yannakakis.enabled = true\").show()\n",
    "    elif mode == \"ref\":\n",
    "        spark.sql(\"SET spark.sql.yannakakis.enabled = false\").show()\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        runtime = benchmark_query(spark, query_file, respath, run)\n",
    "        return [query_name, runtime, mode, run]\n",
    "    except Py4JError as e:\n",
    "        print('timeout or error: ' + str(e))\n",
    "        return [query_name, None, mode, run]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398c3837-5334-46f7-9f4e-b01966e8749a",
   "metadata": {},
   "source": [
    "## SNAP Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be8c1d5-2fe0-4853-81ae-56f40f8a8097",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 13:28:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/15 13:28:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patents\n",
      "wiki\n",
      "google\n",
      "dblp\n",
      "running queries: ['snap-queries/wiki/path02.sql', 'snap-queries/wiki/path03.sql', 'snap-queries/wiki/path04.sql', 'snap-queries/wiki/path05.sql', 'snap-queries/wiki/path06.sql', 'snap-queries/wiki/path07.sql', 'snap-queries/wiki/path08.sql', 'snap-queries/wiki/tree01.sql', 'snap-queries/wiki/tree02.sql', 'snap-queries/wiki/tree03.sql']\n",
      "benchmark-results-snap/results-ref.csv\n",
      "+--------------------+-----+\n",
      "|                 key|value|\n",
      "+--------------------+-----+\n",
      "|spark.sql.yannaka...|false|\n",
      "+--------------------+-----+\n",
      "\n",
      "running query: \n",
      "select count(*) from wiki p1, wiki p2, wiki p3 where p1.toNode = p2.fromNode AND p2.toNode = p3.fromNode\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|    count(1)|\n",
      "+------------+\n",
      "|238984482772|\n",
      "+------------+\n",
      "\n",
      "query id: 7\n",
      "   Unnamed: 0       query     runtime mode  run\n",
      "0         0.0  tree01.sql  145.612812  ref  NaN\n",
      "0         NaN  path02.sql  514.217696  ref  1.0\n",
      "+--------------------+-----+\n",
      "|                 key|value|\n",
      "+--------------------+-----+\n",
      "|spark.sql.yannaka...|false|\n",
      "+--------------------+-----+\n",
      "\n",
      "running query: \n",
      "select count(*) from wiki p1, wiki p2, wiki p3, wiki p4 where p1.toNode = p2.fromNode AND p2.toNode = p3.fromNode AND p3.toNode = p4.fromNode\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:================================================>       (58 + 9) / 67]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cancelling jobs with id HDZ3USFX1NC6GGFC\n",
      "None\n",
      "cancelled job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:>                                                      (0 + 60) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cancelling jobs with id ZN77JI18TJTOR547\n",
      "None\n",
      "cancelled job\n",
      "timeout or error: An error occurred while calling o93.showString.\n",
      ": org.apache.spark.SparkException: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2719)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleJobGroupCancelled$4(DAGScheduler.scala:1193)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:1192)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3004)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\n",
      "   Unnamed: 0       query     runtime mode  run\n",
      "0         0.0  tree01.sql  145.612812  ref  NaN\n",
      "0         NaN  path02.sql  514.217696  ref  1.0\n",
      "0         NaN  path03.sql         NaN  ref  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 23.0 in stage 33.0 (TID 300) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 40.0 in stage 33.0 (TID 317) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 50.0 in stage 33.0 (TID 327) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 7.0 in stage 33.0 (TID 284) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 41.0 in stage 33.0 (TID 318) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 52.0 in stage 33.0 (TID 329) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 9.0 in stage 33.0 (TID 286) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 16.0 in stage 33.0 (TID 293) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 6.0 in stage 33.0 (TID 283) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 32.0 in stage 33.0 (TID 309) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 26.0 in stage 33.0 (TID 303) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 37.0 in stage 33.0 (TID 314) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 54.0 in stage 33.0 (TID 331) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 10.0 in stage 33.0 (TID 287) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 25.0 in stage 33.0 (TID 302) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 53.0 in stage 33.0 (TID 330) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 13.0 in stage 33.0 (TID 290) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 46.0 in stage 33.0 (TID 323) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 29.0 in stage 33.0 (TID 306) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 31.0 in stage 33.0 (TID 308) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 24.0 in stage 33.0 (TID 301) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 36.0 in stage 33.0 (TID 313) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 39.0 in stage 33.0 (TID 316) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 43.0 in stage 33.0 (TID 320) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 38.0 in stage 33.0 (TID 315) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 8.0 in stage 33.0 (TID 285) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 33.0 in stage 33.0 (TID 310) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 49.0 in stage 33.0 (TID 326) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 12.0 in stage 33.0 (TID 289) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 34.0 in stage 33.0 (TID 311) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 27.0 in stage 33.0 (TID 304) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 18.0 in stage 33.0 (TID 295) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 30.0 in stage 33.0 (TID 307) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 5.0 in stage 33.0 (TID 282) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 28.0 in stage 33.0 (TID 305) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 51.0 in stage 33.0 (TID 328) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 45.0 in stage 33.0 (TID 322) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 2.0 in stage 33.0 (TID 279) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 15.0 in stage 33.0 (TID 292) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 11.0 in stage 33.0 (TID 288) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 55.0 in stage 33.0 (TID 332) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 44.0 in stage 33.0 (TID 321) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 47.0 in stage 33.0 (TID 324) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 58.0 in stage 33.0 (TID 335) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 48.0 in stage 33.0 (TID 325) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 1.0 in stage 33.0 (TID 278) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 3.0 in stage 33.0 (TID 280) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 42.0 in stage 33.0 (TID 319) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 14.0 in stage 33.0 (TID 291) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 0.0 in stage 33.0 (TID 277) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 57.0 in stage 33.0 (TID 334) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 4.0 in stage 33.0 (TID 281) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 20.0 in stage 33.0 (TID 297) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 56.0 in stage 33.0 (TID 333) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 59.0 in stage 33.0 (TID 336) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 21.0 in stage 33.0 (TID 298) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 19.0 in stage 33.0 (TID 296) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 17.0 in stage 33.0 (TID 294) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "23/11/15 14:07:19 WARN TaskSetManager: Lost task 22.0 in stage 33.0 (TID 299) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "/tmp/ipykernel_76774/723827150.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, new_df])\n",
      "[Stage 33:>                                                       (0 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                 key|value|\n",
      "+--------------------+-----+\n",
      "|spark.sql.yannaka...|false|\n",
      "+--------------------+-----+\n",
      "\n",
      "running query: \n",
      "select count(*) from wiki p1, wiki p2, wiki p3, wiki p4, wiki p5 where p1.toNode = p2.fromNode AND p2.toNode = p3.fromNode AND p3.toNode = p4.fromNode AND p4.toNode = p5.fromNode\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 14:07:23 WARN TaskSetManager: Lost task 35.0 in stage 33.0 (TID 312) (389ea96bb8b5 executor driver): TaskKilled (Stage cancelled: Job 12 cancelled part of cancelled job group ZN77JI18TJTOR547)\n",
      "ERROR:root:Exception while sending command.                      (27 + 40) / 67]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=58>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 381, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 2446, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o23.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "/tmp/ipykernel_76774/723827150.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, new_df])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeout or error: An error occurred while calling o102.showString\n",
      "   Unnamed: 0       query     runtime mode  run\n",
      "0         0.0  tree01.sql  145.612812  ref  NaN\n",
      "0         NaN  path02.sql  514.217696  ref  1.0\n",
      "0         NaN  path03.sql         NaN  ref  1.0\n",
      "0         NaN  path04.sql         NaN  ref  1.0\n",
      "+--------------------+-----+\n",
      "|                 key|value|\n",
      "+--------------------+-----+\n",
      "|spark.sql.yannaka...|false|\n",
      "+--------------------+-----+\n",
      "\n",
      "running query: \n",
      "select count(*) from wiki p1, wiki p2, wiki p3, wiki p4, wiki p5, wiki p6 where p1.toNode = p2.fromNode AND p2.toNode = p3.fromNode AND p3.toNode = p4.fromNode AND p4.toNode = p5.fromNode AND p5.toNode = p6.fromNode;\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.+ 1) / 1][Stage 45:>   (0 + 1) / 1]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=58>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 381, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 2446, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o23.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "/tmp/ipykernel_76774/723827150.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, new_df])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeout or error: An error occurred while calling o108.showString\n",
      "   Unnamed: 0       query     runtime mode  run\n",
      "0         0.0  tree01.sql  145.612812  ref  NaN\n",
      "0         NaN  path02.sql  514.217696  ref  1.0\n",
      "0         NaN  path03.sql         NaN  ref  1.0\n",
      "0         NaN  path04.sql         NaN  ref  1.0\n",
      "0         NaN  path05.sql         NaN  ref  1.0\n",
      "+--------------------+-----+\n",
      "|                 key|value|\n",
      "+--------------------+-----+\n",
      "|spark.sql.yannaka...|false|\n",
      "+--------------------+-----+\n",
      "\n",
      "running query: \n",
      "select count(*) from wiki p1, wiki p2, wiki p3, wiki p4, wiki p5, wiki p6, wiki p7 where p1.toNode = p2.fromNode AND p2.toNode = p3.fromNode AND p3.toNode = p4.fromNode AND p4.toNode = p5.fromNode AND p5.toNode = p6.fromNode AND p6.toNode = p7.fromNode;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### benchmark configuration\n",
    "dbname = 'snap'\n",
    "tablename = 'wiki'\n",
    "mode = 'ref'\n",
    "runs = [1]\n",
    "###\n",
    "\n",
    "spark = create_spark()\n",
    "import_db(spark, dbname)\n",
    "\n",
    "queries = ['snap-queries/patents/path02.sql',\n",
    "          'snap-queries/patents/path03.sql',\n",
    "          'snap-queries/patents/path04.sql',\n",
    "          'snap-queries/patents/path05.sql',\n",
    "          'snap-queries/patents/path06.sql',\n",
    "          'snap-queries/patents/path07.sql',\n",
    "          'snap-queries/patents/path08.sql',\n",
    "          'snap-queries/patents/tree01.sql',\n",
    "          'snap-queries/patents/tree02.sql',\n",
    "          'snap-queries/patents/tree03.sql']\n",
    "\n",
    "queries = sorted(glob.glob(f'snap-queries/{tablename}/*'))\n",
    "#queries = sorted(glob.glob(f'snap-queries/{tablename}/tree*'))\n",
    "#queries = ['snap-queries/patents/tree01.sql']\n",
    "\n",
    "print('running queries: ' + str(queries))\n",
    "\n",
    "results_df = df = pd.DataFrame([], columns = ['query', 'runtime', 'mode', 'run'])\n",
    "results_file = f'benchmark-results-{dbname}/results-{mode}.csv'\n",
    "if (os.path.exists(results_file)):\n",
    "    results_df = pd.read_csv(results_file)\n",
    "    print(results_file)\n",
    "\n",
    "for run in runs:\n",
    "    for q in queries:\n",
    "        results = [benchmark(spark, dbname, q, mode, run)]\n",
    "        new_df = pd.DataFrame(results, columns = ['query', 'runtime', 'mode', 'run'])\n",
    "        results_df = pd.concat([results_df, new_df])\n",
    "        results_df.to_csv(f'benchmark-results-{dbname}/results-{mode}.csv')\n",
    "        print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a937d945-3c03-494f-8a5f-8f1f69bf5a22",
   "metadata": {},
   "source": [
    "## LSQB Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7beb471-30e3-41c8-a6ba-4aac25bc4102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1283025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.conf.set(\"spark.sql.legacy.setCommandRejectsSparkCoreConfs\",\"false\")\n",
    "#spark.conf.set(\"spark.executor.cores\", \"1\")\n",
    "#spark.conf.set(\"spark.executor.instances\", \"1\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09492e05",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67544c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
