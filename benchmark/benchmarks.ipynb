{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d98322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import string\n",
    "import pathlib\n",
    "import random\n",
    "import threading\n",
    "import time\n",
    "from urllib.parse import urlsplit, urlunsplit\n",
    "import requests\n",
    "import json\n",
    "from py4j.protocol import Py4JJavaError, Py4JError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9a04030-0a23-45df-8e70-6a296a4f582d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global configuration\n",
    "SPARK_MEMORY = 900\n",
    "SPARK_CORES = 60\n",
    "DBHOST = 'postgres'\n",
    "group_in_leaves = False\n",
    "QUERY_TIMEOUT = 60 * 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7397f3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"app\") \\\n",
    "        .master(f'local[{SPARK_CORES}]') \\\n",
    "        .config(\"spark.driver.memory\", f'{SPARK_MEMORY}g') \\\n",
    "        .config(\"spark.executor.memory\", f'{SPARK_MEMORY}g') \\\n",
    "        .config(\"spark.jars\", \"postgresql-42.3.3.jar\") \\\n",
    "        .getOrCreate()\n",
    "    if (group_in_leaves):\n",
    "        spark.sql(\"SET spark.sql.yannakakis.countGroupInLeaves = true\").show()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4170360b-134b-4619-915b-391e877bc203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metrics(spark, group_id):\n",
    "    parsed = list(urlsplit(spark.sparkContext.uiWebUrl))\n",
    "    host_port = parsed[1]\n",
    "    parsed[1] = 'localhost' + host_port[host_port.find(':'):]\n",
    "    API_URL = f'{urlunsplit(parsed)}/api/v1'\n",
    "\n",
    "    app_id = spark.sparkContext.applicationId\n",
    "    sql_queries = requests.get(API_URL + f'/applications/{app_id}/sql', params={'length': '100000'}).json()\n",
    "    query_ids = [q['id'] for q in sql_queries if q['description'] == group_id]\n",
    "    if (len(query_ids) == 0):\n",
    "        print(f'query with group {group_id} not found')\n",
    "        return None\n",
    "    query_id = query_ids[0]\n",
    "    print(f'query id: {query_id}')\n",
    "    \n",
    "    query_details = requests.get(API_URL + f'/applications/{app_id}/sql/{query_id}',\n",
    "                                 params={'details': 'true', 'planDescription': 'true'}).json()\n",
    "    \n",
    "    success_job_ids = query_details['successJobIds']\n",
    "    running_job_ids = query_details['runningJobIds']\n",
    "    failed_job_ids = query_details['failedJobIds']\n",
    "    \n",
    "    job_ids = success_job_ids + running_job_ids + failed_job_ids\n",
    "    \n",
    "    job_details = [requests.get(API_URL + f'/applications/{app_id}/jobs/{jid}').json() for jid in job_ids]\n",
    "    \n",
    "    job_stages = {}\n",
    "    \n",
    "    for j in job_details:\n",
    "        stage_ids = j['stageIds']\n",
    "        \n",
    "        stage_params = {'details': 'true', 'withSummaries': 'true'}\n",
    "        stages = [requests.get(API_URL + f'/applications/{app_id}/stages/{sid}', stage_params) for sid in stage_ids]\n",
    "        \n",
    "        job_stages[j['jobId']] = [stage.json() for stage in stages if stage.status_code == 200] # can be 404\n",
    "    \n",
    "    return query_details, job_details, job_stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acbbca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_db(spark, dbname):\n",
    "    \n",
    "    username = dbname\n",
    "    password = dbname\n",
    "    dbname = dbname\n",
    "\n",
    "    df_tables = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", f'jdbc:postgresql://{DBHOST}:5432/{dbname}') \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"dbtable\", \"information_schema.tables\") \\\n",
    "    .option(\"user\", username) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .load()\n",
    "\n",
    "    for idx, row in df_tables.toPandas().iterrows():\n",
    "        if row.table_schema == 'public':\n",
    "            table_name = row.table_name\n",
    "            df = spark.read.format(\"jdbc\") \\\n",
    "                .option(\"url\", f'jdbc:postgresql://{DBHOST}:5432/{dbname}') \\\n",
    "                .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "                .option(\"dbtable\", table_name) \\\n",
    "                .option(\"user\", username) \\\n",
    "                .option(\"password\", password) \\\n",
    "                .load()\n",
    "    \n",
    "            print(table_name)\n",
    "            #print(df.show())\n",
    "            df.createOrReplaceTempView(table_name)\n",
    "\n",
    "def random_str(size=16, chars=string.ascii_uppercase + string.digits):\n",
    "    return ''.join(random.choice(chars) for _ in range(size))\n",
    "\n",
    "def set_group_id(spark):\n",
    "    group_id = random_str()\n",
    "    spark.sparkContext.setJobGroup(group_id, group_id)\n",
    "    return group_id\n",
    "\n",
    "def cancel_query(seconds, group_id):\n",
    "    time.sleep(seconds)\n",
    "    print(\"cancelling jobs with id \" + group_id)\n",
    "    print(spark.sparkContext.cancelJobGroup(group_id))\n",
    "    print(\"cancelled job\")\n",
    "\n",
    "def cancel_query_after(spark, seconds):\n",
    "    group_id = random_str()\n",
    "    spark.sparkContext.setJobGroup(group_id, group_id)\n",
    "    threading.Thread(target=cancel_query, args=(seconds,group_id,)).start()\n",
    "    return group_id\n",
    "    \n",
    "def run_query(file):\n",
    "    with open(file, 'r') as f:\n",
    "        query = '\\n'.join(filter(lambda line: not line.startswith('limit') and not line.startswith('-'), f.readlines()))\n",
    "        \n",
    "        print(\"running query: \\n\" + query)\n",
    "        return spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ead489e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_query(spark, query, respath):\n",
    "    start_time = time.time()\n",
    "\n",
    "    group_id = cancel_query_after(spark, QUERY_TIMEOUT)\n",
    "    df1 = run_query(query)\n",
    "    df1.show()\n",
    "\n",
    "    end_time = time.time()\n",
    "    diff_time = end_time - start_time\n",
    "\n",
    "    execution, jobs, job_stages = extract_metrics(spark, group_id)\n",
    "\n",
    "    if execution is not None:\n",
    "            with open(respath + f'/execution.json', 'w') as f:\n",
    "                f.write(json.dumps(execution, indent=2))\n",
    "            with open(respath + f'/jobs.json', 'w') as f:\n",
    "                f.write(json.dumps(jobs, indent=2))\n",
    "            with open(respath + f'/stages.json', 'w') as f:\n",
    "                f.write(json.dumps(job_stages, indent=2))\n",
    "    return diff_time\n",
    "\n",
    "def benchmark(spark, dbname, query_file, mode):\n",
    "    #spark.sql(\"SET spark.sql.yannakakis.enabled = false\").show()\n",
    "    # run the query once to warm up Spark (load the relation in memory)\n",
    "    #df0 = run_query(query)\n",
    "    #df0.show()\n",
    "    \n",
    "    query_name = os.path.basename(query_file)\n",
    "\n",
    "    respath = f'benchmark-results-{dbname}/' + query_name + \"/\" + mode\n",
    "    pathlib.Path(respath).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if mode == \"opt\":\n",
    "        spark.sql(\"SET spark.sql.yannakakis.enabled = true\").show()\n",
    "    elif mode == \"ref\":\n",
    "        spark.sql(\"SET spark.sql.yannakakis.enabled = false\").show()\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        runtime = benchmark_query(spark, query_file, respath)\n",
    "        return [query_name, runtime, mode]\n",
    "    except Py4JError as e:\n",
    "        print('timeout or error: ' + e)\n",
    "        return [query_name, None, mode]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398c3837-5334-46f7-9f4e-b01966e8749a",
   "metadata": {},
   "source": [
    "## SNAP Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8be8c1d5-2fe0-4853-81ae-56f40f8a8097",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/14 22:45:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/14 22:45:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patents\n",
      "+--------------------+-----+\n",
      "|                 key|value|\n",
      "+--------------------+-----+\n",
      "|spark.sql.yannaka...|false|\n",
      "+--------------------+-----+\n",
      "\n",
      "running query: \n",
      "SELECT COUNT(*) FROM patents p1, patents p2, patents p3, patents p4a, patents p4b\n",
      "\n",
      "WHERE p1.toNode = p2.fromNode\n",
      "\n",
      "AND p2.toNode = p3.fromNode\n",
      "\n",
      "    AND p3.toNode = p4a.fromNode\n",
      "\n",
      "    AND p3.toNode = p4b.fromNode\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|   count(1)|\n",
      "+-----------+\n",
      "|15961425879|\n",
      "+-----------+\n",
      "\n",
      "query id: 4\n",
      "        query     runtime mode\n",
      "0  tree01.sql  145.612812  ref\n"
     ]
    }
   ],
   "source": [
    "dbname = 'snap'\n",
    "\n",
    "spark = create_spark()\n",
    "import_db(spark, dbname)\n",
    "\n",
    "queries = ['snap-queries/patents/path02.sql',\n",
    "          'snap-queries/patents/path03.sql',\n",
    "          'snap-queries/patents/path04.sql',\n",
    "          'snap-queries/patents/path05.sql',\n",
    "          'snap-queries/patents/path06.sql',\n",
    "          'snap-queries/patents/path07.sql',\n",
    "          'snap-queries/patents/path08.sql',\n",
    "          'snap-queries/patents/tree01.sql',\n",
    "          'snap-queries/patents/tree02.sql',\n",
    "          'snap-queries/patents/tree03.sql']\n",
    "\n",
    "queries = ['snap-queries/patents/tree01.sql']\n",
    "\n",
    "mode = 'ref'\n",
    "\n",
    "results = []\n",
    "\n",
    "for q in queries:\n",
    "    results = results + [benchmark(spark, dbname, q, mode)]\n",
    "    df = pd.DataFrame(results, columns = ['query', 'runtime', 'mode'])\n",
    "    df.to_csv(f'results-{mode}.csv')\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1283025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.conf.set(\"spark.sql.legacy.setCommandRejectsSparkCoreConfs\",\"false\")\n",
    "#spark.conf.set(\"spark.executor.cores\", \"1\")\n",
    "#spark.conf.set(\"spark.executor.instances\", \"1\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd2cc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compare result\n",
    "import time\n",
    "#query = 'tpch-kit/dbgen/queries/postgres/2.sql'\n",
    "#query = 'tpch-kit/dbgen/queries/postgres/13.sql'\n",
    "#query = 'count-3.sql'\n",
    "#query = 'tpch-kit/dbgen/queries/postgres/11.sql'\n",
    "#query = '11-simple.sql'\n",
    "query = 'median-2-hint.sql'\n",
    "#query = 'tpch-kit/dbgen/queries/postgres/7.sql'\n",
    "#query = '13-simple.sql'\n",
    "#query = 'dsb-queries/agg_queries/query101.sql'\n",
    "query = 'snap-queries/patents/tree01.sql'\n",
    "\n",
    "spark.sql(\"SET spark.sql.yannakakis.enabled = true\").show()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df1 = run_query(query)\n",
    "df1.show()\n",
    "df1.explain(mode=\"extended\")\n",
    "\n",
    "end_time = time.time()\n",
    "yannakakis_time = end_time - start_time\n",
    "\n",
    "spark.sql(\"SET spark.sql.yannakakis.enabled = false\").show()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df2 = run_query(query)\n",
    "df2.show()\n",
    "df2.explain(mode=\"extended\")\n",
    "\n",
    "end_time = time.time()\n",
    "ref_time = end_time - start_time\n",
    "\n",
    "#print(f'row count: {df1.count()} vs. {df2.count()}' )\n",
    "print(f'time ref: {ref_time}\\ntime yannakakis: {yannakakis_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09492e05",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67544c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
