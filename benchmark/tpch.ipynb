{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d98322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7397f3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/10 15:24:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[60]\") \\\n",
    "        .appName(\"app\") \\\n",
    "        .config(\"spark.driver.memory\", \"900g\") \\\n",
    "        .config(\"spark.executor.memory\", \"900g\") \\\n",
    "        .config(\"spark.memory.offHeap.enabled\",False) \\\n",
    "        .config(\"spark.jars\", \"postgresql-42.3.3.jar\") \\\n",
    "        .getOrCreate()\n",
    "#spark.sparkContext.setLogLevel(\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d1e816",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"WARN\") # ALL, DEBUG, WARN,\n",
    "#spark.sparkContext.setLogLevel(\"ALL\") # ALL, DEBUG, WARN,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acbbca1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part\n",
      "supplier\n",
      "partsupp\n",
      "customer\n",
      "orders\n",
      "lineitem\n",
      "nation\n",
      "region\n"
     ]
    }
   ],
   "source": [
    "username = os.environ.get('USERNAME', 'tpch')\n",
    "password = os.environ.get('PASSWORD', 'tpch')\n",
    "dbname = os.environ.get('DBNAME', 'tpch')\n",
    "dbhost = os.environ.get('DBHOST', 'postgres')\n",
    "\n",
    "df_tables = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", f'jdbc:postgresql://{dbhost}:5432/{dbname}') \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"dbtable\", \"information_schema.tables\") \\\n",
    "    .option(\"user\", username) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .load()\n",
    "\n",
    "for idx, row in df_tables.toPandas().iterrows():\n",
    "        if row.table_schema == 'public':\n",
    "            table_name = row.table_name\n",
    "            df = spark.read.format(\"jdbc\") \\\n",
    "                .option(\"url\", f'jdbc:postgresql://{dbhost}:5432/{dbname}') \\\n",
    "                .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "                .option(\"dbtable\", table_name) \\\n",
    "                .option(\"user\", username) \\\n",
    "                .option(\"password\", password) \\\n",
    "                .load()\n",
    "    \n",
    "            print(table_name)\n",
    "            #print(df.show())\n",
    "            df.createOrReplaceTempView(table_name)\n",
    "            spark.catalog.cacheTable(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ebbb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SET spark.sql.yannakakis.countGroupInLeaves = false\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa45949",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SET spark.sql.yannakakis.enabled = false\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a728c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SET spark.sql.yannakakis.enabled = true\").show()\n",
    "spark.sql(\"SET spark.sql.yannakakis.countGroupInLeaves = false\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2ddf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SET spark.local.dir\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5f51e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"ANALYZE TABLE part COMPUTE STATISTICS;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979d2464",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"\"\"select\n",
    "        /*+ FK(ps_partkey, p_partkey), FK(n_regionkey, r_regionkey) */\n",
    "\t\tMEDIAN(p_size)\n",
    "\t\tfrom\n",
    "            part,\n",
    "\t\t\tpartsupp,\n",
    "\t\t\tsupplier,\n",
    "\t\t\tnation,\n",
    "\t\t\tregion\n",
    "\t\twhere\n",
    "\t\t\tp_partkey = ps_partkey\n",
    "\t\t\tand s_suppkey = ps_suppkey\n",
    "\t\t\tand s_nationkey = n_nationkey\n",
    "\t\t\tand n_regionkey = r_regionkey\"\"\")\n",
    "\n",
    "df.show(500)\n",
    "\n",
    "df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c11b3515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(file):\n",
    "    with open(file, 'r') as f:\n",
    "        query = '\\n'.join(filter(lambda line: not line.startswith('limit') and not line.startswith('-'), f.readlines()))\n",
    "        \n",
    "        print(\"running query: \\n\" + query)\n",
    "        return spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5587e1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"\"\"\n",
    "SELECT ps_partkey, count(*) from part, partsupp\n",
    "WHERE p_partkey = ps_partkey\n",
    "GROUP BY ps_partkey\n",
    "\"\"\")\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d50f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT COUNT(p_size) / COUNT(DISTINCT p_size) FROM part\")\n",
    "df.show()\n",
    "df = spark.sql(\"SELECT COUNT(p_retailprice) / COUNT(DISTINCT p_retailprice) FROM part\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008e187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t1 = spark.createDataFrame([(1,1), (2,1), (2,2), (3,2), (3,3), (4,3), (4,3), (5,2), (5,1), (6,4)], schema=(\"a\",\"b\"))\n",
    "df_t1.createOrReplaceTempView(\"t1\")\n",
    "df_t2 = spark.createDataFrame([(1,1), (2,1), (3,2), (3,2), (3,3), (3,3), (4,3), (4,2), (5,1), (6,4)], schema=(\"c\",\"d\"))\n",
    "df_t2.createOrReplaceTempView(\"t2\")\n",
    "df_t3 = spark.createDataFrame([(1,1), (2,1), (3,2), (3,2), (3,3), (3,3), (4,3), (4,2), (5,1), (6,4)], schema=(\"e\",\"f\"))\n",
    "df_t3.createOrReplaceTempView(\"t3\")\n",
    "\n",
    "query = \"select median(a) from t1, t2 where b = c\"\n",
    "#query = \"select percentile(a, 0.5, b) from t1, t2 where b = c\"\n",
    "#query = \"select median(a) from t1 where EXISTS (SELECT 1 FROM t2 WHERE b = c)\"\n",
    "#query = \"select count(*) from t1, t2 where b = c\"\n",
    "#query = \"select *a from t1 where EXISTS (SELECT 1 FROM t2 WHERE b = c)\"\n",
    "\n",
    "spark.sql(\"SET spark.sql.yannakakis.enabled = false\").show()\n",
    "\n",
    "df = spark.sql(query)\n",
    "df.show()\n",
    "\n",
    "spark.sql(\"SET spark.sql.yannakakis.enabled = true\").show()\n",
    "\n",
    "df = spark.sql(query)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44762b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.setCommandRejectsSparkCoreConfs\",\"false\")\n",
    "#spark.conf.set(\"spark.executor.cores\", \"6\")\n",
    "#spark.conf.set(\"spark.executor.instances\", \"6\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead489e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def benchmark(query):\n",
    "    df0 = run_query(query)\n",
    "    df0.show()\n",
    "    \n",
    "    spark.sql(\"SET spark.sql.yannakakis.enabled = true\").show()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    df1 = run_query(query)\n",
    "    df1.show()\n",
    "    #df1.explain(True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    yannakakis_time = end_time - start_time\n",
    "\n",
    "    spark.sql(\"SET spark.sql.yannakakis.enabled = false\").show()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    df2 = run_query(query)\n",
    "    df2.show()\n",
    "    #df2.explain(True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    ref_time = end_time - start_time\n",
    "    \n",
    "    #return [query, ref_time, yannakakis_time]\n",
    "    return [query, ref_time, yannakakis_time]\n",
    "\n",
    "\n",
    "queries = ['tpch-kit/dbgen/queries/postgres/2.sql',\n",
    "           'tpch-kit/dbgen/queries/postgres/11.sql', \n",
    "           'tpch-kit/dbgen/queries/postgres/11-hint.sql',\n",
    "           'median-1.sql',\n",
    "           'median-2.sql', \n",
    "           'median-3.sql', \n",
    "           'median-4.sql', \n",
    "           'median-5.sql',\n",
    "            'median-1-hint.sql',\n",
    "           'median-2-hint.sql', \n",
    "           'median-3-hint.sql', \n",
    "           'median-4-hint.sql', \n",
    "           'median-5-hint.sql']\n",
    "\n",
    "results = [benchmark(q) for q in queries]\n",
    "\n",
    "df = pd.DataFrame(results, columns = ['query', 'ref_time', 'yannakakis_time'])\n",
    "\n",
    "print(df)\n",
    "\n",
    "df.to_csv(\"results.csv\")\n",
    "    \n",
    "\n",
    "#print(f'row count: {df1.count()} vs. {df2.count()}' )\n",
    "    #print(f'time ref: {ref_time}\\ntime yannakakis: {yannakakis_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1283025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.conf.set(\"spark.sql.legacy.setCommandRejectsSparkCoreConfs\",\"false\")\n",
    "#spark.conf.set(\"spark.executor.cores\", \"1\")\n",
    "#spark.conf.set(\"spark.executor.instances\", \"1\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bd2cc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                 key|value|\n",
      "+--------------------+-----+\n",
      "|spark.sql.yannaka...| true|\n",
      "+--------------------+-----+\n",
      "\n",
      "running query: \n",
      "select\n",
      "\n",
      "/*+ FK(ps_partkey, p_partkey), FK(n_regionkey, r_regionkey), FK(ps_suppkey, s_suppkey), FK(s_nationkey, n_nationkey), PK(ps_partkey, ps_suppkey) */\n",
      "\n",
      "        median(s_acctbal)\n",
      "\n",
      "\t\tfrom\n",
      "\n",
      "            part,\n",
      "\n",
      "\t\t\tpartsupp,\n",
      "\n",
      "\t\t\tsupplier,\n",
      "\n",
      "\t\t\tnation,\n",
      "\n",
      "\t\t\tregion\n",
      "\n",
      "\t\twhere\n",
      "\n",
      "\t\t\tp_partkey = ps_partkey\n",
      "\n",
      "\t\t\tand s_suppkey = ps_suppkey\n",
      "\n",
      "\t\t\tand s_nationkey = n_nationkey\n",
      "\n",
      "\t\t\tand n_regionkey = r_regionkey\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/02 14:44:33 WARN RewriteJoinsAsSemijoins: applying yannakakis rewriting to join: Aggregate [toprettystring(percentile(s_acctbal#104, 0.5, 1, 0, 0, false), Some(Etc/UTC)) AS toprettystring(median(s_acctbal))#3033]\n",
      "+- Project [s_acctbal#104]\n",
      "   +- FKHint [[ps_partkey#157L, p_partkey#24], [ps_suppkey#158L, s_suppkey#99], [n_regionkey#472L, r_regionkey#503], [s_nationkey#102L, n_nationkey#470]], [[ps_partkey#157L, ps_suppkey#158L]]\n",
      "      +- Join Inner, (n_regionkey#472L = cast(r_regionkey#503 as bigint))\n",
      "         :- Join Inner, (s_nationkey#102L = cast(n_nationkey#470 as bigint))\n",
      "         :  :- Join Inner, (cast(s_suppkey#99 as bigint) = ps_suppkey#158L)\n",
      "         :  :  :- Join Inner, (cast(p_partkey#24 as bigint) = ps_partkey#157L)\n",
      "         :  :  :  :- Project [p_partkey#24]\n",
      "         :  :  :  :  +- Filter isnotnull(p_partkey#24)\n",
      "         :  :  :  :     +- InMemoryRelation [p_partkey#24, p_name#25, p_mfgr#33, p_brand#34, p_type#28, p_size#29, p_container#35, p_retailprice#31, p_comment#32], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         :  :  :  :           +- *(1) Project [p_partkey#24, p_name#25, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_mfgr#26, 25, true, false, true) AS p_mfgr#33, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_brand#27, 10, true, false, true) AS p_brand#34, p_type#28, p_size#29, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_container#30, 10, true, false, true) AS p_container#35, p_retailprice#31, p_comment#32]\n",
      "         :  :  :  :              +- *(1) Scan JDBCRelation(part) [numPartitions=1] [p_partkey#24,p_name#25,p_mfgr#26,p_brand#27,p_type#28,p_size#29,p_container#30,p_retailprice#31,p_comment#32] PushedFilters: [], ReadSchema: struct<p_partkey:int,p_name:string,p_mfgr:string,p_brand:string,p_type:string,p_size:int,p_contai...\n",
      "         :  :  :  +- Project [ps_partkey#157L, ps_suppkey#158L]\n",
      "         :  :  :     +- Filter (isnotnull(ps_partkey#157L) AND isnotnull(ps_suppkey#158L))\n",
      "         :  :  :        +- InMemoryRelation [ps_partkey#157L, ps_suppkey#158L, ps_availqty#159, ps_supplycost#160, ps_comment#161], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         :  :  :              +- *(1) Scan JDBCRelation(partsupp) [numPartitions=1] [ps_partkey#157L,ps_suppkey#158L,ps_availqty#159,ps_supplycost#160,ps_comment#161] PushedFilters: [], ReadSchema: struct<ps_partkey:bigint,ps_suppkey:bigint,ps_availqty:int,ps_supplycost:decimal(38,18),ps_commen...\n",
      "         :  :  +- Project [s_suppkey#99, s_nationkey#102L, s_acctbal#104]\n",
      "         :  :     +- Filter (isnotnull(s_suppkey#99) AND isnotnull(s_nationkey#102L))\n",
      "         :  :        +- InMemoryRelation [s_suppkey#99, s_name#106, s_address#101, s_nationkey#102L, s_phone#107, s_acctbal#104, s_comment#105], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         :  :              +- *(1) Project [s_suppkey#99, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, s_name#100, 25, true, false, true) AS s_name#106, s_address#101, s_nationkey#102L, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, s_phone#103, 15, true, false, true) AS s_phone#107, s_acctbal#104, s_comment#105]\n",
      "         :  :                 +- *(1) Scan JDBCRelation(supplier) [numPartitions=1] [s_suppkey#99,s_name#100,s_address#101,s_nationkey#102L,s_phone#103,s_acctbal#104,s_comment#105] PushedFilters: [], ReadSchema: struct<s_suppkey:int,s_name:string,s_address:string,s_nationkey:bigint,s_phone:string,s_acctbal:d...\n",
      "         :  +- Project [n_nationkey#470, n_regionkey#472L]\n",
      "         :     +- Filter (isnotnull(n_nationkey#470) AND isnotnull(n_regionkey#472L))\n",
      "         :        +- InMemoryRelation [n_nationkey#470, n_name#474, n_regionkey#472L, n_comment#473], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         :              +- *(1) Project [n_nationkey#470, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, n_name#471, 25, true, false, true) AS n_name#474, n_regionkey#472L, n_comment#473]\n",
      "         :                 +- *(1) Scan JDBCRelation(nation) [numPartitions=1] [n_nationkey#470,n_name#471,n_regionkey#472L,n_comment#473] PushedFilters: [], ReadSchema: struct<n_nationkey:int,n_name:string,n_regionkey:bigint,n_comment:string>\n",
      "         +- Project [r_regionkey#503]\n",
      "            +- Filter isnotnull(r_regionkey#503)\n",
      "               +- InMemoryRelation [r_regionkey#503, r_name#506, r_comment#505], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                     +- *(1) Project [r_regionkey#503, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, r_name#504, 25, true, false, true) AS r_name#506, r_comment#505]\n",
      "                        +- *(1) Scan JDBCRelation(region) [numPartitions=1] [r_regionkey#503,r_name#504,r_comment#505] PushedFilters: [], ReadSchema: struct<r_regionkey:int,r_name:string,r_comment:string>\n",
      "\n",
      "23/11/02 14:44:33 WARN RewriteJoinsAsSemijoins: agg(project(join))\n",
      "23/11/02 14:44:33 WARN RewriteJoinsAsSemijoins: items: List(Project [p_partkey#24]\n",
      "+- Filter isnotnull(p_partkey#24)\n",
      "   +- InMemoryRelation [p_partkey#24, p_name#25, p_mfgr#33, p_brand#34, p_type#28, p_size#29, p_container#35, p_retailprice#31, p_comment#32], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(1) Project [p_partkey#24, p_name#25, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_mfgr#26, 25, true, false, true) AS p_mfgr#33, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_brand#27, 10, true, false, true) AS p_brand#34, p_type#28, p_size#29, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_container#30, 10, true, false, true) AS p_container#35, p_retailprice#31, p_comment#32]\n",
      "            +- *(1) Scan JDBCRelation(part) [numPartitions=1] [p_partkey#24,p_name#25,p_mfgr#26,p_brand#27,p_type#28,p_size#29,p_container#30,p_retailprice#31,p_comment#32] PushedFilters: [], ReadSchema: struct<p_partkey:int,p_name:string,p_mfgr:string,p_brand:string,p_type:string,p_size:int,p_contai...\n",
      ", Project [ps_partkey#157L, ps_suppkey#158L]\n",
      "+- Filter (isnotnull(ps_partkey#157L) AND isnotnull(ps_suppkey#158L))\n",
      "   +- InMemoryRelation [ps_partkey#157L, ps_suppkey#158L, ps_availqty#159, ps_supplycost#160, ps_comment#161], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(1) Scan JDBCRelation(partsupp) [numPartitions=1] [ps_partkey#157L,ps_suppkey#158L,ps_availqty#159,ps_supplycost#160,ps_comment#161] PushedFilters: [], ReadSchema: struct<ps_partkey:bigint,ps_suppkey:bigint,ps_availqty:int,ps_supplycost:decimal(38,18),ps_commen...\n",
      ", Project [s_suppkey#99, s_nationkey#102L, s_acctbal#104]\n",
      "+- Filter (isnotnull(s_suppkey#99) AND isnotnull(s_nationkey#102L))\n",
      "   +- InMemoryRelation [s_suppkey#99, s_name#106, s_address#101, s_nationkey#102L, s_phone#107, s_acctbal#104, s_comment#105], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(1) Project [s_suppkey#99, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, s_name#100, 25, true, false, true) AS s_name#106, s_address#101, s_nationkey#102L, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, s_phone#103, 15, true, false, true) AS s_phone#107, s_acctbal#104, s_comment#105]\n",
      "            +- *(1) Scan JDBCRelation(supplier) [numPartitions=1] [s_suppkey#99,s_name#100,s_address#101,s_nationkey#102L,s_phone#103,s_acctbal#104,s_comment#105] PushedFilters: [], ReadSchema: struct<s_suppkey:int,s_name:string,s_address:string,s_nationkey:bigint,s_phone:string,s_acctbal:d...\n",
      ", Project [n_nationkey#470, n_regionkey#472L]\n",
      "+- Filter (isnotnull(n_nationkey#470) AND isnotnull(n_regionkey#472L))\n",
      "   +- InMemoryRelation [n_nationkey#470, n_name#474, n_regionkey#472L, n_comment#473], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(1) Project [n_nationkey#470, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, n_name#471, 25, true, false, true) AS n_name#474, n_regionkey#472L, n_comment#473]\n",
      "            +- *(1) Scan JDBCRelation(nation) [numPartitions=1] [n_nationkey#470,n_name#471,n_regionkey#472L,n_comment#473] PushedFilters: [], ReadSchema: struct<n_nationkey:int,n_name:string,n_regionkey:bigint,n_comment:string>\n",
      ", Project [r_regionkey#503]\n",
      "+- Filter isnotnull(r_regionkey#503)\n",
      "   +- InMemoryRelation [r_regionkey#503, r_name#506, r_comment#505], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(1) Project [r_regionkey#503, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, r_name#504, 25, true, false, true) AS r_name#506, r_comment#505]\n",
      "            +- *(1) Scan JDBCRelation(region) [numPartitions=1] [r_regionkey#503,r_name#504,r_comment#505] PushedFilters: [], ReadSchema: struct<r_regionkey:int,r_name:string,r_comment:string>\n",
      ")\n",
      "23/11/02 14:44:33 WARN RewriteJoinsAsSemijoins: conditions: Set((cast(p_partkey#24 as bigint) = ps_partkey#157L), (cast(s_suppkey#99 as bigint) = ps_suppkey#158L), (s_nationkey#102L = cast(n_nationkey#470 as bigint)), (n_regionkey#472L = cast(r_regionkey#503 as bigint)))\n",
      "23/11/02 14:44:33 WARN RewriteJoinsAsSemijoins: agg: toprettystring(percentile(s_acctbal#104, 0.5, 1, 0, 0, false), Some(Etc/UTC)) AS toprettystring(median(s_acctbal))#3033\n",
      "23/11/02 14:44:33 WARN RewriteJoinsAsSemijoins: is 0MA: false\n",
      "23/11/02 14:44:33 WARN RewriteJoinsAsSemijoins: is counting: false\n",
      "23/11/02 14:44:33 WARN RewriteJoinsAsSemijoins: is percentile: true\n",
      "23/11/02 14:44:33 WARN RewriteJoinsAsSemijoins: is sum: false\n",
      "23/11/02 14:44:33 WARN RewriteJoinsAsSemijoins: is avg: false\n",
      "23/11/02 14:44:33 WARN RewriteJoinsAsSemijoins: is non-agg: false\n",
      "23/11/02 14:44:33 WARN RewriteJoinsAsSemijoins: aggregate attributes: {s_acctbal#104}\n",
      "23/11/02 14:44:33 WARN RewriteJoinsAsSemijoins: groupingExpressions: List()\n",
      "23/11/02 14:44:33 WARN RewriteJoinsAsSemijoins: group attributes: {}\n",
      "23/11/02 14:44:33 WARN RewriteJoinsAsSemijoins: counting aggregates: List()\n",
      "23/11/02 14:44:33 WARN Hypergraph: equivalence classes: Set(Set(p_partkey#24, ps_partkey#157L), Set(s_suppkey#99, ps_suppkey#158L), Set(s_nationkey#102L, n_nationkey#470), Set(n_regionkey#472L, r_regionkey#503))\n",
      "23/11/02 14:44:33 WARN Hypergraph: vertex to attribute mapping: Map(s_suppkey#99 -> Set(s_suppkey#99, ps_suppkey#158L), s_nationkey#102L -> Set(s_nationkey#102L, n_nationkey#470), n_regionkey#472L -> Set(n_regionkey#472L, r_regionkey#503), p_partkey#24 -> Set(p_partkey#24, ps_partkey#157L))\n",
      "23/11/02 14:44:33 WARN Hypergraph: attribute to vertex mapping: Map(ExprId(472,03bd54f3-2d6e-4be3-b491-3cb116cf2bb7) -> n_regionkey#472L, ExprId(158,03bd54f3-2d6e-4be3-b491-3cb116cf2bb7) -> s_suppkey#99, ExprId(157,03bd54f3-2d6e-4be3-b491-3cb116cf2bb7) -> p_partkey#24, ExprId(503,03bd54f3-2d6e-4be3-b491-3cb116cf2bb7) -> n_regionkey#472L, ExprId(470,03bd54f3-2d6e-4be3-b491-3cb116cf2bb7) -> s_nationkey#102L, ExprId(99,03bd54f3-2d6e-4be3-b491-3cb116cf2bb7) -> s_suppkey#99, ExprId(102,03bd54f3-2d6e-4be3-b491-3cb116cf2bb7) -> s_nationkey#102L, ExprId(24,03bd54f3-2d6e-4be3-b491-3cb116cf2bb7) -> p_partkey#24)\n",
      "23/11/02 14:44:33 WARN Hypergraph: join item: Project [p_partkey#24]\n",
      "+- Filter isnotnull(p_partkey#24)\n",
      "   +- InMemoryRelation [p_partkey#24, p_name#25, p_mfgr#33, p_brand#34, p_type#28, p_size#29, p_container#35, p_retailprice#31, p_comment#32], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(1) Project [p_partkey#24, p_name#25, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_mfgr#26, 25, true, false, true) AS p_mfgr#33, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_brand#27, 10, true, false, true) AS p_brand#34, p_type#28, p_size#29, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_container#30, 10, true, false, true) AS p_container#35, p_retailprice#31, p_comment#32]\n",
      "            +- *(1) Scan JDBCRelation(part) [numPartitions=1] [p_partkey#24,p_name#25,p_mfgr#26,p_brand#27,p_type#28,p_size#29,p_container#30,p_retailprice#31,p_comment#32] PushedFilters: [], ReadSchema: struct<p_partkey:int,p_name:string,p_mfgr:string,p_brand:string,p_type:string,p_size:int,p_contai...\n",
      "\n",
      "23/11/02 14:44:33 WARN Hypergraph: join item: Project [ps_partkey#157L, ps_suppkey#158L]\n",
      "+- Filter (isnotnull(ps_partkey#157L) AND isnotnull(ps_suppkey#158L))\n",
      "   +- InMemoryRelation [ps_partkey#157L, ps_suppkey#158L, ps_availqty#159, ps_supplycost#160, ps_comment#161], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(1) Scan JDBCRelation(partsupp) [numPartitions=1] [ps_partkey#157L,ps_suppkey#158L,ps_availqty#159,ps_supplycost#160,ps_comment#161] PushedFilters: [], ReadSchema: struct<ps_partkey:bigint,ps_suppkey:bigint,ps_availqty:int,ps_supplycost:decimal(38,18),ps_commen...\n",
      "\n",
      "23/11/02 14:44:33 WARN Hypergraph: join item: Project [s_suppkey#99, s_nationkey#102L, s_acctbal#104]\n",
      "+- Filter (isnotnull(s_suppkey#99) AND isnotnull(s_nationkey#102L))\n",
      "   +- InMemoryRelation [s_suppkey#99, s_name#106, s_address#101, s_nationkey#102L, s_phone#107, s_acctbal#104, s_comment#105], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(1) Project [s_suppkey#99, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, s_name#100, 25, true, false, true) AS s_name#106, s_address#101, s_nationkey#102L, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, s_phone#103, 15, true, false, true) AS s_phone#107, s_acctbal#104, s_comment#105]\n",
      "            +- *(1) Scan JDBCRelation(supplier) [numPartitions=1] [s_suppkey#99,s_name#100,s_address#101,s_nationkey#102L,s_phone#103,s_acctbal#104,s_comment#105] PushedFilters: [], ReadSchema: struct<s_suppkey:int,s_name:string,s_address:string,s_nationkey:bigint,s_phone:string,s_acctbal:d...\n",
      "\n",
      "23/11/02 14:44:33 WARN Hypergraph: join item: Project [n_nationkey#470, n_regionkey#472L]\n",
      "+- Filter (isnotnull(n_nationkey#470) AND isnotnull(n_regionkey#472L))\n",
      "   +- InMemoryRelation [n_nationkey#470, n_name#474, n_regionkey#472L, n_comment#473], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(1) Project [n_nationkey#470, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, n_name#471, 25, true, false, true) AS n_name#474, n_regionkey#472L, n_comment#473]\n",
      "            +- *(1) Scan JDBCRelation(nation) [numPartitions=1] [n_nationkey#470,n_name#471,n_regionkey#472L,n_comment#473] PushedFilters: [], ReadSchema: struct<n_nationkey:int,n_name:string,n_regionkey:bigint,n_comment:string>\n",
      "\n",
      "23/11/02 14:44:33 WARN Hypergraph: join item: Project [r_regionkey#503]\n",
      "+- Filter isnotnull(r_regionkey#503)\n",
      "   +- InMemoryRelation [r_regionkey#503, r_name#506, r_comment#505], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(1) Project [r_regionkey#503, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, r_name#504, 25, true, false, true) AS r_name#506, r_comment#505]\n",
      "            +- *(1) Scan JDBCRelation(region) [numPartitions=1] [r_regionkey#503,r_name#504,r_comment#505] PushedFilters: [], ReadSchema: struct<r_regionkey:int,r_name:string,r_comment:string>\n",
      "\n",
      "23/11/02 14:44:33 WARN Hypergraph: hyperedges: Set(E3(s_suppkey#99, s_nationkey#102L), E1(p_partkey#24), E2(p_partkey#24, s_suppkey#99), E5(n_regionkey#472L), E4(s_nationkey#102L, n_regionkey#472L))\n",
      "23/11/02 14:44:33 WARN Hypergraph: gyo edge: E5(n_regionkey#472L)\n",
      "23/11/02 14:44:33 WARN Hypergraph: supersets: Set(E4(s_nationkey#102L, n_regionkey#472L))\n",
      "23/11/02 14:44:33 WARN Hypergraph: gyo edge: E4(s_nationkey#102L, n_regionkey#472L)\n",
      "23/11/02 14:44:33 WARN Hypergraph: supersets: Set()\n",
      "23/11/02 14:44:33 WARN Hypergraph: parentNode: TreeNode(Set(E4(s_nationkey#102L, n_regionkey#472L)))[Set({n_nationkey#470, n_regionkey#472L})] [[parent: false]]\n",
      "\n",
      "23/11/02 14:44:33 WARN Hypergraph: subsets: Set(TreeNode(Set(E5(n_regionkey#472L)))[Set({r_regionkey#503})] [[parent: false]]\n",
      ")\n",
      "23/11/02 14:44:33 WARN Hypergraph: gyo edge: E1(p_partkey#24)\n",
      "23/11/02 14:44:33 WARN Hypergraph: supersets: Set(E2(p_partkey#24, s_suppkey#99))\n",
      "23/11/02 14:44:33 WARN Hypergraph: gyo edge: E2(p_partkey#24, s_suppkey#99)\n",
      "23/11/02 14:44:33 WARN Hypergraph: supersets: Set()\n",
      "23/11/02 14:44:33 WARN Hypergraph: parentNode: TreeNode(Set(E2(p_partkey#24, s_suppkey#99)))[Set({ps_partkey#157L, ps_suppkey#158L})] [[parent: false]]\n",
      "\n",
      "23/11/02 14:44:33 WARN Hypergraph: subsets: Set(TreeNode(Set(E1(p_partkey#24)))[Set({p_partkey#24})] [[parent: false]]\n",
      ")\n",
      "23/11/02 14:44:33 WARN Hypergraph: gyo edge: E3(s_suppkey#99, s_nationkey#102L)\n",
      "23/11/02 14:44:33 WARN Hypergraph: supersets: Set()\n",
      "23/11/02 14:44:33 WARN Hypergraph: parentNode: TreeNode(Set(E3(s_suppkey#99, s_nationkey#102L)))[Set({s_suppkey#99, s_nationkey#102L, s_acctbal#104})] [[parent: false]]\n",
      "\n",
      "23/11/02 14:44:33 WARN Hypergraph: subsets: Set()\n",
      "23/11/02 14:44:33 WARN Hypergraph: gyo edge: E3(s_suppkey#99, s_nationkey#102L)\n",
      "23/11/02 14:44:33 WARN Hypergraph: supersets: Set()\n",
      "23/11/02 14:44:33 WARN Hypergraph: parentNode: TreeNode(Set(E3(s_suppkey#99, s_nationkey#102L)))[Set({s_suppkey#99, s_nationkey#102L, s_acctbal#104})] [[parent: false]]\n",
      "\n",
      "23/11/02 14:44:33 WARN Hypergraph: subsets: Set(TreeNode(Set(E4(s_nationkey#102L, n_regionkey#472L)))[Set({n_nationkey#470, n_regionkey#472L})] [[parent: false]]\n",
      "-- TreeNode(Set(E5(n_regionkey#472L)))[Set({r_regionkey#503})] [[parent: true]]\n",
      ", TreeNode(Set(E2(p_partkey#24, s_suppkey#99)))[Set({ps_partkey#157L, ps_suppkey#158L})] [[parent: false]]\n",
      "-- TreeNode(Set(E1(p_partkey#24)))[Set({p_partkey#24})] [[parent: true]]\n",
      ")\n",
      "23/11/02 14:44:33 WARN RewriteJoinsAsSemijoins: join tree: \n",
      "TreeNode(Set(E3(s_suppkey#99, s_nationkey#102L)))[Set({s_suppkey#99, s_nationkey#102L, s_acctbal#104})] [[parent: false]]\n",
      "-- TreeNode(Set(E4(s_nationkey#102L, n_regionkey#472L)))[Set({n_nationkey#470, n_regionkey#472L})] [[parent: true]]\n",
      "-- -- TreeNode(Set(E5(n_regionkey#472L)))[Set({r_regionkey#503})] [[parent: true]]\n",
      "\n",
      "-- TreeNode(Set(E2(p_partkey#24, s_suppkey#99)))[Set({ps_partkey#157L, ps_suppkey#158L})] [[parent: true]]\n",
      "-- -- TreeNode(Set(E1(p_partkey#24)))[Set({p_partkey#24})] [[parent: true]]\n",
      "\n",
      "23/11/02 14:44:33 WARN HTNode: aggAttributes: {s_acctbal#104}\n",
      "23/11/02 14:44:33 WARN HTNode: nodeAttributes: {s_suppkey#99, s_nationkey#102L, s_acctbal#104}\n",
      "23/11/02 14:44:33 WARN HTNode: found subset in:\n",
      "TreeNode(Set(E3(s_suppkey#99, s_nationkey#102L)))[Set({s_suppkey#99, s_nationkey#102L, s_acctbal#104})] [[parent: false]]\n",
      "-- TreeNode(Set(E4(s_nationkey#102L, n_regionkey#472L)))[Set({n_nationkey#470, n_regionkey#472L})] [[parent: true]]\n",
      "-- -- TreeNode(Set(E5(n_regionkey#472L)))[Set({r_regionkey#503})] [[parent: true]]\n",
      "\n",
      "-- TreeNode(Set(E2(p_partkey#24, s_suppkey#99)))[Set({ps_partkey#157L, ps_suppkey#158L})] [[parent: true]]\n",
      "-- -- TreeNode(Set(E1(p_partkey#24)))[Set({p_partkey#24})] [[parent: true]]\n",
      "\n",
      "23/11/02 14:44:33 WARN RewriteJoinsAsSemijoins: rerooted: \n",
      "TreeNode(Set(E3(s_suppkey#99, s_nationkey#102L)))[Set({s_suppkey#99, s_nationkey#102L, s_acctbal#104})] [[parent: false]]\n",
      "-- TreeNode(Set(E4(s_nationkey#102L, n_regionkey#472L)))[Set({n_nationkey#470, n_regionkey#472L})] [[parent: true]]\n",
      "-- -- TreeNode(Set(E5(n_regionkey#472L)))[Set({r_regionkey#503})] [[parent: true]]\n",
      "\n",
      "-- TreeNode(Set(E2(p_partkey#24, s_suppkey#99)))[Set({ps_partkey#157L, ps_suppkey#158L})] [[parent: true]]\n",
      "-- -- TreeNode(Set(E1(p_partkey#24)))[Set({p_partkey#24})] [[parent: true]]\n",
      "\n",
      "23/11/02 14:44:33 WARN RewriteJoinsAsSemijoins: percentile aggregates: List(toprettystring(percentile(s_acctbal#104, 0.5, 1, 0, 0, false), Some(Etc/UTC)) AS toprettystring(median(s_acctbal))#3033)\n",
      "23/11/02 14:44:33 WARN HTNode: keyRefs: List(List(ps_partkey#157L, p_partkey#24), List(ps_suppkey#158L, s_suppkey#99), List(n_regionkey#472L, r_regionkey#503), List(s_nationkey#102L, n_nationkey#470))\n",
      "23/11/02 14:44:33 WARN HTNode: edge: E4(s_nationkey#102L, n_regionkey#472L)\n",
      "23/11/02 14:44:33 WARN HTNode: overlapping vertices: Set((n_regionkey#472L,r_regionkey#503))\n",
      "23/11/02 14:44:33 WARN HTNode: can semi join: true\n",
      "23/11/02 14:44:33 WARN HTNode: join output: List(n_nationkey#470, n_regionkey#472L, c#3316L)\n",
      "23/11/02 14:44:33 WARN HTNode: keyRefs: List(List(ps_partkey#157L, p_partkey#24), List(ps_suppkey#158L, s_suppkey#99), List(n_regionkey#472L, r_regionkey#503), List(s_nationkey#102L, n_nationkey#470))\n",
      "23/11/02 14:44:33 WARN HTNode: edge: E3(s_suppkey#99, s_nationkey#102L)\n",
      "23/11/02 14:44:33 WARN HTNode: overlapping vertices: Set((s_nationkey#102L,n_nationkey#470))\n",
      "23/11/02 14:44:33 WARN HTNode: can semi join: true\n",
      "23/11/02 14:44:33 WARN HTNode: join output: List(s_suppkey#99, s_nationkey#102L, s_acctbal#104, c#3315L)\n",
      "23/11/02 14:44:33 WARN HTNode: keyRefs: List(List(ps_partkey#157L, p_partkey#24), List(ps_suppkey#158L, s_suppkey#99), List(n_regionkey#472L, r_regionkey#503), List(s_nationkey#102L, n_nationkey#470))\n",
      "23/11/02 14:44:33 WARN HTNode: edge: E2(p_partkey#24, s_suppkey#99)\n",
      "23/11/02 14:44:33 WARN HTNode: overlapping vertices: Set((ps_partkey#157L,p_partkey#24))\n",
      "23/11/02 14:44:33 WARN HTNode: can semi join: true\n",
      "23/11/02 14:44:33 WARN HTNode: join output: List(ps_partkey#157L, ps_suppkey#158L, c#3326L)\n",
      "23/11/02 14:44:33 WARN HTNode: keyRefs: List(List(ps_partkey#157L, p_partkey#24), List(ps_suppkey#158L, s_suppkey#99), List(n_regionkey#472L, r_regionkey#503), List(s_nationkey#102L, n_nationkey#470))\n",
      "23/11/02 14:44:33 WARN HTNode: edge: E3(s_suppkey#99, s_nationkey#102L)\n",
      "23/11/02 14:44:33 WARN HTNode: overlapping vertices: Set((s_suppkey#99,ps_suppkey#158L))\n",
      "23/11/02 14:44:33 WARN HTNode: can semi join: false\n",
      "23/11/02 14:44:33 WARN HTNode: join output: List(s_suppkey#99, s_nationkey#102L, s_acctbal#104, c#3315L, c#3335L, ps_suppkey#158L)\n",
      "23/11/02 14:44:33 WARN RewriteJoinsAsSemijoins: new aggregate: Aggregate [toprettystring(percentile(s_acctbal#104, 0.5, 1, 0, 0, false), Some(Etc/UTC)) AS toprettystring(median(s_acctbal))#3033]\n",
      "+- Project [s_acctbal#104, c#3336L]\n",
      "   +- Project [(cast(c#3315L as bigint) * c#3335L) AS c#3336L, s_suppkey#99, s_nationkey#102L, s_acctbal#104, c#3315L, c#3335L, ps_suppkey#158L]\n",
      "      +- Join Inner, (s_suppkey#99 = cast(ps_suppkey#158L as int))\n",
      "         :- Project [s_suppkey#99, s_nationkey#102L, s_acctbal#104, c#3315L]\n",
      "         :  +- Join LeftSemi, (s_nationkey#102L = cast(n_nationkey#470 as bigint))\n",
      "         :     :- Project [s_suppkey#99, s_nationkey#102L, s_acctbal#104, 1 AS c#3315L]\n",
      "         :     :  +- Project [s_suppkey#99, s_nationkey#102L, s_acctbal#104]\n",
      "         :     :     +- Filter (isnotnull(s_suppkey#99) AND isnotnull(s_nationkey#102L))\n",
      "         :     :        +- InMemoryRelation [s_suppkey#99, s_name#106, s_address#101, s_nationkey#102L, s_phone#107, s_acctbal#104, s_comment#105], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         :     :              +- *(1) Project [s_suppkey#99, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, s_name#100, 25, true, false, true) AS s_name#106, s_address#101, s_nationkey#102L, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, s_phone#103, 15, true, false, true) AS s_phone#107, s_acctbal#104, s_comment#105]\n",
      "         :     :                 +- *(1) Scan JDBCRelation(supplier) [numPartitions=1] [s_suppkey#99,s_name#100,s_address#101,s_nationkey#102L,s_phone#103,s_acctbal#104,s_comment#105] PushedFilters: [], ReadSchema: struct<s_suppkey:int,s_name:string,s_address:string,s_nationkey:bigint,s_phone:string,s_acctbal:d...\n",
      "         :     +- Project [n_nationkey#470, n_regionkey#472L, c#3316L]\n",
      "         :        +- Join LeftSemi, (n_regionkey#472L = cast(r_regionkey#503 as bigint))\n",
      "         :           :- Project [n_nationkey#470, n_regionkey#472L, 1 AS c#3316L]\n",
      "         :           :  +- Project [n_nationkey#470, n_regionkey#472L]\n",
      "         :           :     +- Filter (isnotnull(n_nationkey#470) AND isnotnull(n_regionkey#472L))\n",
      "         :           :        +- InMemoryRelation [n_nationkey#470, n_name#474, n_regionkey#472L, n_comment#473], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         :           :              +- *(1) Project [n_nationkey#470, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, n_name#471, 25, true, false, true) AS n_name#474, n_regionkey#472L, n_comment#473]\n",
      "         :           :                 +- *(1) Scan JDBCRelation(nation) [numPartitions=1] [n_nationkey#470,n_name#471,n_regionkey#472L,n_comment#473] PushedFilters: [], ReadSchema: struct<n_nationkey:int,n_name:string,n_regionkey:bigint,n_comment:string>\n",
      "         :           +- Project [r_regionkey#503, 1 AS c#3317L]\n",
      "         :              +- Project [r_regionkey#503]\n",
      "         :                 +- Filter isnotnull(r_regionkey#503)\n",
      "         :                    +- InMemoryRelation [r_regionkey#503, r_name#506, r_comment#505], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         :                          +- *(1) Project [r_regionkey#503, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, r_name#504, 25, true, false, true) AS r_name#506, r_comment#505]\n",
      "         :                             +- *(1) Scan JDBCRelation(region) [numPartitions=1] [r_regionkey#503,r_name#504,r_comment#505] PushedFilters: [], ReadSchema: struct<r_regionkey:int,r_name:string,r_comment:string>\n",
      "         +- Aggregate [ps_suppkey#158L], [sum(c#3326L) AS c#3335L, ps_suppkey#158L]\n",
      "            +- Project [ps_partkey#157L, ps_suppkey#158L, c#3326L]\n",
      "               +- Join LeftSemi, (ps_partkey#157L = cast(p_partkey#24 as bigint))\n",
      "                  :- Project [ps_partkey#157L, ps_suppkey#158L, 1 AS c#3326L]\n",
      "                  :  +- Project [ps_partkey#157L, ps_suppkey#158L]\n",
      "                  :     +- Filter (isnotnull(ps_partkey#157L) AND isnotnull(ps_suppkey#158L))\n",
      "                  :        +- InMemoryRelation [ps_partkey#157L, ps_suppkey#158L, ps_availqty#159, ps_supplycost#160, ps_comment#161], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                  :              +- *(1) Scan JDBCRelation(partsupp) [numPartitions=1] [ps_partkey#157L,ps_suppkey#158L,ps_availqty#159,ps_supplycost#160,ps_comment#161] PushedFilters: [], ReadSchema: struct<ps_partkey:bigint,ps_suppkey:bigint,ps_availqty:int,ps_supplycost:decimal(38,18),ps_commen...\n",
      "                  +- Project [p_partkey#24, 1 AS c#3327L]\n",
      "                     +- Project [p_partkey#24]\n",
      "                        +- Filter isnotnull(p_partkey#24)\n",
      "                           +- InMemoryRelation [p_partkey#24, p_name#25, p_mfgr#33, p_brand#34, p_type#28, p_size#29, p_container#35, p_retailprice#31, p_comment#32], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                                 +- *(1) Project [p_partkey#24, p_name#25, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_mfgr#26, 25, true, false, true) AS p_mfgr#33, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_brand#27, 10, true, false, true) AS p_brand#34, p_type#28, p_size#29, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_container#30, 10, true, false, true) AS p_container#35, p_retailprice#31, p_comment#32]\n",
      "                                    +- *(1) Scan JDBCRelation(part) [numPartitions=1] [p_partkey#24,p_name#25,p_mfgr#26,p_brand#27,p_type#28,p_size#29,p_container#30,p_retailprice#31,p_comment#32] PushedFilters: [], ReadSchema: struct<p_partkey:int,p_name:string,p_mfgr:string,p_brand:string,p_type:string,p_size:int,p_contai...\n",
      "\n",
      "23/11/02 14:44:33 WARN RewriteJoinsAsSemijoins: not applicable to aggregate: Aggregate [ps_suppkey#158L], [sum(c#3326L) AS c#3335L, ps_suppkey#158L]\n",
      "+- Project [ps_partkey#157L, ps_suppkey#158L, c#3326L]\n",
      "   +- Join LeftSemi, (ps_partkey#157L = cast(p_partkey#24 as bigint))\n",
      "      :- Project [ps_partkey#157L, ps_suppkey#158L, 1 AS c#3326L]\n",
      "      :  +- Project [ps_partkey#157L, ps_suppkey#158L]\n",
      "      :     +- Filter (isnotnull(ps_partkey#157L) AND isnotnull(ps_suppkey#158L))\n",
      "      :        +- InMemoryRelation [ps_partkey#157L, ps_suppkey#158L, ps_availqty#159, ps_supplycost#160, ps_comment#161], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "      :              +- *(1) Scan JDBCRelation(partsupp) [numPartitions=1] [ps_partkey#157L,ps_suppkey#158L,ps_availqty#159,ps_supplycost#160,ps_comment#161] PushedFilters: [], ReadSchema: struct<ps_partkey:bigint,ps_suppkey:bigint,ps_availqty:int,ps_supplycost:decimal(38,18),ps_commen...\n",
      "      +- Project [p_partkey#24, 1 AS c#3327L]\n",
      "         +- Project [p_partkey#24]\n",
      "            +- Filter isnotnull(p_partkey#24)\n",
      "               +- InMemoryRelation [p_partkey#24, p_name#25, p_mfgr#33, p_brand#34, p_type#28, p_size#29, p_container#35, p_retailprice#31, p_comment#32], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                     +- *(1) Project [p_partkey#24, p_name#25, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_mfgr#26, 25, true, false, true) AS p_mfgr#33, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_brand#27, 10, true, false, true) AS p_brand#34, p_type#28, p_size#29, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_container#30, 10, true, false, true) AS p_container#35, p_retailprice#31, p_comment#32]\n",
      "                        +- *(1) Scan JDBCRelation(part) [numPartitions=1] [p_partkey#24,p_name#25,p_mfgr#26,p_brand#27,p_type#28,p_size#29,p_container#30,p_retailprice#31,p_comment#32] PushedFilters: [], ReadSchema: struct<p_partkey:int,p_name:string,p_mfgr:string,p_brand:string,p_type:string,p_size:int,p_contai...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/02 14:44:44 WARN RewriteJoinsAsSemijoins: applying yannakakis rewriting to join: Aggregate [percentile(s_acctbal#104, 0.5, 1, 0, 0, false) AS median(s_acctbal)#3030]\n",
      "+- Project [s_acctbal#104]\n",
      "   +- FKHint [[ps_partkey#157L, p_partkey#24], [ps_suppkey#158L, s_suppkey#99], [n_regionkey#472L, r_regionkey#503], [s_nationkey#102L, n_nationkey#470]], [[ps_partkey#157L, ps_suppkey#158L]]\n",
      "      +- Join Inner, (n_regionkey#472L = cast(r_regionkey#503 as bigint))\n",
      "         :- Join Inner, (s_nationkey#102L = cast(n_nationkey#470 as bigint))\n",
      "         :  :- Join Inner, (cast(s_suppkey#99 as bigint) = ps_suppkey#158L)\n",
      "         :  :  :- Join Inner, (cast(p_partkey#24 as bigint) = ps_partkey#157L)\n",
      "         :  :  :  :- Project [p_partkey#24]\n",
      "         :  :  :  :  +- Filter isnotnull(p_partkey#24)\n",
      "         :  :  :  :     +- InMemoryRelation [p_partkey#24, p_name#25, p_mfgr#33, p_brand#34, p_type#28, p_size#29, p_container#35, p_retailprice#31, p_comment#32], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         :  :  :  :           +- *(1) Project [p_partkey#24, p_name#25, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_mfgr#26, 25, true, false, true) AS p_mfgr#33, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_brand#27, 10, true, false, true) AS p_brand#34, p_type#28, p_size#29, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_container#30, 10, true, false, true) AS p_container#35, p_retailprice#31, p_comment#32]\n",
      "         :  :  :  :              +- *(1) Scan JDBCRelation(part) [numPartitions=1] [p_partkey#24,p_name#25,p_mfgr#26,p_brand#27,p_type#28,p_size#29,p_container#30,p_retailprice#31,p_comment#32] PushedFilters: [], ReadSchema: struct<p_partkey:int,p_name:string,p_mfgr:string,p_brand:string,p_type:string,p_size:int,p_contai...\n",
      "         :  :  :  +- Project [ps_partkey#157L, ps_suppkey#158L]\n",
      "         :  :  :     +- Filter (isnotnull(ps_partkey#157L) AND isnotnull(ps_suppkey#158L))\n",
      "         :  :  :        +- InMemoryRelation [ps_partkey#157L, ps_suppkey#158L, ps_availqty#159, ps_supplycost#160, ps_comment#161], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         :  :  :              +- *(1) Scan JDBCRelation(partsupp) [numPartitions=1] [ps_partkey#157L,ps_suppkey#158L,ps_availqty#159,ps_supplycost#160,ps_comment#161] PushedFilters: [], ReadSchema: struct<ps_partkey:bigint,ps_suppkey:bigint,ps_availqty:int,ps_supplycost:decimal(38,18),ps_commen...\n",
      "         :  :  +- Project [s_suppkey#99, s_nationkey#102L, s_acctbal#104]\n",
      "         :  :     +- Filter (isnotnull(s_suppkey#99) AND isnotnull(s_nationkey#102L))\n",
      "         :  :        +- InMemoryRelation [s_suppkey#99, s_name#106, s_address#101, s_nationkey#102L, s_phone#107, s_acctbal#104, s_comment#105], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         :  :              +- *(1) Project [s_suppkey#99, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, s_name#100, 25, true, false, true) AS s_name#106, s_address#101, s_nationkey#102L, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, s_phone#103, 15, true, false, true) AS s_phone#107, s_acctbal#104, s_comment#105]\n",
      "         :  :                 +- *(1) Scan JDBCRelation(supplier) [numPartitions=1] [s_suppkey#99,s_name#100,s_address#101,s_nationkey#102L,s_phone#103,s_acctbal#104,s_comment#105] PushedFilters: [], ReadSchema: struct<s_suppkey:int,s_name:string,s_address:string,s_nationkey:bigint,s_phone:string,s_acctbal:d...\n",
      "         :  +- Project [n_nationkey#470, n_regionkey#472L]\n",
      "         :     +- Filter (isnotnull(n_nationkey#470) AND isnotnull(n_regionkey#472L))\n",
      "         :        +- InMemoryRelation [n_nationkey#470, n_name#474, n_regionkey#472L, n_comment#473], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         :              +- *(1) Project [n_nationkey#470, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, n_name#471, 25, true, false, true) AS n_name#474, n_regionkey#472L, n_comment#473]\n",
      "         :                 +- *(1) Scan JDBCRelation(nation) [numPartitions=1] [n_nationkey#470,n_name#471,n_regionkey#472L,n_comment#473] PushedFilters: [], ReadSchema: struct<n_nationkey:int,n_name:string,n_regionkey:bigint,n_comment:string>\n",
      "         +- Project [r_regionkey#503]\n",
      "            +- Filter isnotnull(r_regionkey#503)\n",
      "               +- InMemoryRelation [r_regionkey#503, r_name#506, r_comment#505], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                     +- *(1) Project [r_regionkey#503, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, r_name#504, 25, true, false, true) AS r_name#506, r_comment#505]\n",
      "                        +- *(1) Scan JDBCRelation(region) [numPartitions=1] [r_regionkey#503,r_name#504,r_comment#505] PushedFilters: [], ReadSchema: struct<r_regionkey:int,r_name:string,r_comment:string>\n",
      "\n",
      "23/11/02 14:44:44 WARN RewriteJoinsAsSemijoins: agg(project(join))\n",
      "23/11/02 14:44:44 WARN RewriteJoinsAsSemijoins: items: List(Project [p_partkey#24]\n",
      "+- Filter isnotnull(p_partkey#24)\n",
      "   +- InMemoryRelation [p_partkey#24, p_name#25, p_mfgr#33, p_brand#34, p_type#28, p_size#29, p_container#35, p_retailprice#31, p_comment#32], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(1) Project [p_partkey#24, p_name#25, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_mfgr#26, 25, true, false, true) AS p_mfgr#33, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_brand#27, 10, true, false, true) AS p_brand#34, p_type#28, p_size#29, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_container#30, 10, true, false, true) AS p_container#35, p_retailprice#31, p_comment#32]\n",
      "            +- *(1) Scan JDBCRelation(part) [numPartitions=1] [p_partkey#24,p_name#25,p_mfgr#26,p_brand#27,p_type#28,p_size#29,p_container#30,p_retailprice#31,p_comment#32] PushedFilters: [], ReadSchema: struct<p_partkey:int,p_name:string,p_mfgr:string,p_brand:string,p_type:string,p_size:int,p_contai...\n",
      ", Project [ps_partkey#157L, ps_suppkey#158L]\n",
      "+- Filter (isnotnull(ps_partkey#157L) AND isnotnull(ps_suppkey#158L))\n",
      "   +- InMemoryRelation [ps_partkey#157L, ps_suppkey#158L, ps_availqty#159, ps_supplycost#160, ps_comment#161], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(1) Scan JDBCRelation(partsupp) [numPartitions=1] [ps_partkey#157L,ps_suppkey#158L,ps_availqty#159,ps_supplycost#160,ps_comment#161] PushedFilters: [], ReadSchema: struct<ps_partkey:bigint,ps_suppkey:bigint,ps_availqty:int,ps_supplycost:decimal(38,18),ps_commen...\n",
      ", Project [s_suppkey#99, s_nationkey#102L, s_acctbal#104]\n",
      "+- Filter (isnotnull(s_suppkey#99) AND isnotnull(s_nationkey#102L))\n",
      "   +- InMemoryRelation [s_suppkey#99, s_name#106, s_address#101, s_nationkey#102L, s_phone#107, s_acctbal#104, s_comment#105], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(1) Project [s_suppkey#99, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, s_name#100, 25, true, false, true) AS s_name#106, s_address#101, s_nationkey#102L, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, s_phone#103, 15, true, false, true) AS s_phone#107, s_acctbal#104, s_comment#105]\n",
      "            +- *(1) Scan JDBCRelation(supplier) [numPartitions=1] [s_suppkey#99,s_name#100,s_address#101,s_nationkey#102L,s_phone#103,s_acctbal#104,s_comment#105] PushedFilters: [], ReadSchema: struct<s_suppkey:int,s_name:string,s_address:string,s_nationkey:bigint,s_phone:string,s_acctbal:d...\n",
      ", Project [n_nationkey#470, n_regionkey#472L]\n",
      "+- Filter (isnotnull(n_nationkey#470) AND isnotnull(n_regionkey#472L))\n",
      "   +- InMemoryRelation [n_nationkey#470, n_name#474, n_regionkey#472L, n_comment#473], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(1) Project [n_nationkey#470, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, n_name#471, 25, true, false, true) AS n_name#474, n_regionkey#472L, n_comment#473]\n",
      "            +- *(1) Scan JDBCRelation(nation) [numPartitions=1] [n_nationkey#470,n_name#471,n_regionkey#472L,n_comment#473] PushedFilters: [], ReadSchema: struct<n_nationkey:int,n_name:string,n_regionkey:bigint,n_comment:string>\n",
      ", Project [r_regionkey#503]\n",
      "+- Filter isnotnull(r_regionkey#503)\n",
      "   +- InMemoryRelation [r_regionkey#503, r_name#506, r_comment#505], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(1) Project [r_regionkey#503, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, r_name#504, 25, true, false, true) AS r_name#506, r_comment#505]\n",
      "            +- *(1) Scan JDBCRelation(region) [numPartitions=1] [r_regionkey#503,r_name#504,r_comment#505] PushedFilters: [], ReadSchema: struct<r_regionkey:int,r_name:string,r_comment:string>\n",
      ")\n",
      "23/11/02 14:44:44 WARN RewriteJoinsAsSemijoins: conditions: Set((cast(p_partkey#24 as bigint) = ps_partkey#157L), (cast(s_suppkey#99 as bigint) = ps_suppkey#158L), (s_nationkey#102L = cast(n_nationkey#470 as bigint)), (n_regionkey#472L = cast(r_regionkey#503 as bigint)))\n",
      "23/11/02 14:44:44 WARN RewriteJoinsAsSemijoins: agg: percentile(s_acctbal#104, 0.5, 1, 0, 0, false) AS median(s_acctbal)#3030\n",
      "23/11/02 14:44:44 WARN RewriteJoinsAsSemijoins: is 0MA: false\n",
      "23/11/02 14:44:44 WARN RewriteJoinsAsSemijoins: is counting: false\n",
      "23/11/02 14:44:44 WARN RewriteJoinsAsSemijoins: is percentile: true\n",
      "23/11/02 14:44:44 WARN RewriteJoinsAsSemijoins: is sum: false\n",
      "23/11/02 14:44:44 WARN RewriteJoinsAsSemijoins: is avg: false\n",
      "23/11/02 14:44:44 WARN RewriteJoinsAsSemijoins: is non-agg: false\n",
      "23/11/02 14:44:44 WARN RewriteJoinsAsSemijoins: aggregate attributes: {s_acctbal#104}\n",
      "23/11/02 14:44:44 WARN RewriteJoinsAsSemijoins: groupingExpressions: List()\n",
      "23/11/02 14:44:44 WARN RewriteJoinsAsSemijoins: group attributes: {}\n",
      "23/11/02 14:44:44 WARN RewriteJoinsAsSemijoins: counting aggregates: List()\n",
      "23/11/02 14:44:44 WARN Hypergraph: equivalence classes: Set(Set(p_partkey#24, ps_partkey#157L), Set(s_suppkey#99, ps_suppkey#158L), Set(s_nationkey#102L, n_nationkey#470), Set(n_regionkey#472L, r_regionkey#503))\n",
      "23/11/02 14:44:44 WARN Hypergraph: vertex to attribute mapping: Map(s_suppkey#99 -> Set(s_suppkey#99, ps_suppkey#158L), s_nationkey#102L -> Set(s_nationkey#102L, n_nationkey#470), n_regionkey#472L -> Set(n_regionkey#472L, r_regionkey#503), p_partkey#24 -> Set(p_partkey#24, ps_partkey#157L))\n",
      "23/11/02 14:44:44 WARN Hypergraph: attribute to vertex mapping: Map(ExprId(472,03bd54f3-2d6e-4be3-b491-3cb116cf2bb7) -> n_regionkey#472L, ExprId(158,03bd54f3-2d6e-4be3-b491-3cb116cf2bb7) -> s_suppkey#99, ExprId(157,03bd54f3-2d6e-4be3-b491-3cb116cf2bb7) -> p_partkey#24, ExprId(503,03bd54f3-2d6e-4be3-b491-3cb116cf2bb7) -> n_regionkey#472L, ExprId(470,03bd54f3-2d6e-4be3-b491-3cb116cf2bb7) -> s_nationkey#102L, ExprId(99,03bd54f3-2d6e-4be3-b491-3cb116cf2bb7) -> s_suppkey#99, ExprId(102,03bd54f3-2d6e-4be3-b491-3cb116cf2bb7) -> s_nationkey#102L, ExprId(24,03bd54f3-2d6e-4be3-b491-3cb116cf2bb7) -> p_partkey#24)\n",
      "23/11/02 14:44:44 WARN Hypergraph: join item: Project [p_partkey#24]\n",
      "+- Filter isnotnull(p_partkey#24)\n",
      "   +- InMemoryRelation [p_partkey#24, p_name#25, p_mfgr#33, p_brand#34, p_type#28, p_size#29, p_container#35, p_retailprice#31, p_comment#32], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(1) Project [p_partkey#24, p_name#25, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_mfgr#26, 25, true, false, true) AS p_mfgr#33, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_brand#27, 10, true, false, true) AS p_brand#34, p_type#28, p_size#29, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_container#30, 10, true, false, true) AS p_container#35, p_retailprice#31, p_comment#32]\n",
      "            +- *(1) Scan JDBCRelation(part) [numPartitions=1] [p_partkey#24,p_name#25,p_mfgr#26,p_brand#27,p_type#28,p_size#29,p_container#30,p_retailprice#31,p_comment#32] PushedFilters: [], ReadSchema: struct<p_partkey:int,p_name:string,p_mfgr:string,p_brand:string,p_type:string,p_size:int,p_contai...\n",
      "\n",
      "23/11/02 14:44:44 WARN Hypergraph: join item: Project [ps_partkey#157L, ps_suppkey#158L]\n",
      "+- Filter (isnotnull(ps_partkey#157L) AND isnotnull(ps_suppkey#158L))\n",
      "   +- InMemoryRelation [ps_partkey#157L, ps_suppkey#158L, ps_availqty#159, ps_supplycost#160, ps_comment#161], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(1) Scan JDBCRelation(partsupp) [numPartitions=1] [ps_partkey#157L,ps_suppkey#158L,ps_availqty#159,ps_supplycost#160,ps_comment#161] PushedFilters: [], ReadSchema: struct<ps_partkey:bigint,ps_suppkey:bigint,ps_availqty:int,ps_supplycost:decimal(38,18),ps_commen...\n",
      "\n",
      "23/11/02 14:44:44 WARN Hypergraph: join item: Project [s_suppkey#99, s_nationkey#102L, s_acctbal#104]\n",
      "+- Filter (isnotnull(s_suppkey#99) AND isnotnull(s_nationkey#102L))\n",
      "   +- InMemoryRelation [s_suppkey#99, s_name#106, s_address#101, s_nationkey#102L, s_phone#107, s_acctbal#104, s_comment#105], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(1) Project [s_suppkey#99, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, s_name#100, 25, true, false, true) AS s_name#106, s_address#101, s_nationkey#102L, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, s_phone#103, 15, true, false, true) AS s_phone#107, s_acctbal#104, s_comment#105]\n",
      "            +- *(1) Scan JDBCRelation(supplier) [numPartitions=1] [s_suppkey#99,s_name#100,s_address#101,s_nationkey#102L,s_phone#103,s_acctbal#104,s_comment#105] PushedFilters: [], ReadSchema: struct<s_suppkey:int,s_name:string,s_address:string,s_nationkey:bigint,s_phone:string,s_acctbal:d...\n",
      "\n",
      "23/11/02 14:44:44 WARN Hypergraph: join item: Project [n_nationkey#470, n_regionkey#472L]\n",
      "+- Filter (isnotnull(n_nationkey#470) AND isnotnull(n_regionkey#472L))\n",
      "   +- InMemoryRelation [n_nationkey#470, n_name#474, n_regionkey#472L, n_comment#473], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(1) Project [n_nationkey#470, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, n_name#471, 25, true, false, true) AS n_name#474, n_regionkey#472L, n_comment#473]\n",
      "            +- *(1) Scan JDBCRelation(nation) [numPartitions=1] [n_nationkey#470,n_name#471,n_regionkey#472L,n_comment#473] PushedFilters: [], ReadSchema: struct<n_nationkey:int,n_name:string,n_regionkey:bigint,n_comment:string>\n",
      "\n",
      "23/11/02 14:44:44 WARN Hypergraph: join item: Project [r_regionkey#503]\n",
      "+- Filter isnotnull(r_regionkey#503)\n",
      "   +- InMemoryRelation [r_regionkey#503, r_name#506, r_comment#505], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(1) Project [r_regionkey#503, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, r_name#504, 25, true, false, true) AS r_name#506, r_comment#505]\n",
      "            +- *(1) Scan JDBCRelation(region) [numPartitions=1] [r_regionkey#503,r_name#504,r_comment#505] PushedFilters: [], ReadSchema: struct<r_regionkey:int,r_name:string,r_comment:string>\n",
      "\n",
      "23/11/02 14:44:44 WARN Hypergraph: hyperedges: Set(E2(p_partkey#24, s_suppkey#99), E4(s_nationkey#102L, n_regionkey#472L), E1(p_partkey#24), E3(s_suppkey#99, s_nationkey#102L), E5(n_regionkey#472L))\n",
      "23/11/02 14:44:44 WARN Hypergraph: gyo edge: E3(s_suppkey#99, s_nationkey#102L)\n",
      "23/11/02 14:44:44 WARN Hypergraph: supersets: Set()\n",
      "23/11/02 14:44:44 WARN Hypergraph: parentNode: TreeNode(Set(E3(s_suppkey#99, s_nationkey#102L)))[Set({s_suppkey#99, s_nationkey#102L, s_acctbal#104})] [[parent: false]]\n",
      "\n",
      "23/11/02 14:44:44 WARN Hypergraph: subsets: Set()\n",
      "23/11/02 14:44:44 WARN Hypergraph: gyo edge: E4(s_nationkey#102L, n_regionkey#472L)\n",
      "23/11/02 14:44:44 WARN Hypergraph: supersets: Set()\n",
      "23/11/02 14:44:44 WARN Hypergraph: parentNode: TreeNode(Set(E4(s_nationkey#102L, n_regionkey#472L)))[Set({n_nationkey#470, n_regionkey#472L})] [[parent: false]]\n",
      "\n",
      "23/11/02 14:44:44 WARN Hypergraph: subsets: Set(TreeNode(Set(E5(n_regionkey#472L)))[Set({r_regionkey#503})] [[parent: false]]\n",
      ")\n",
      "23/11/02 14:44:44 WARN Hypergraph: gyo edge: E1(p_partkey#24)\n",
      "23/11/02 14:44:44 WARN Hypergraph: supersets: Set(E2(p_partkey#24, s_suppkey#99))\n",
      "23/11/02 14:44:44 WARN Hypergraph: gyo edge: E2(p_partkey#24, s_suppkey#99)\n",
      "23/11/02 14:44:44 WARN Hypergraph: supersets: Set()\n",
      "23/11/02 14:44:44 WARN Hypergraph: parentNode: TreeNode(Set(E2(p_partkey#24, s_suppkey#99)))[Set({ps_partkey#157L, ps_suppkey#158L})] [[parent: false]]\n",
      "\n",
      "23/11/02 14:44:44 WARN Hypergraph: subsets: Set(TreeNode(Set(E1(p_partkey#24)))[Set({p_partkey#24})] [[parent: false]]\n",
      ")\n",
      "23/11/02 14:44:44 WARN Hypergraph: gyo edge: E4(s_nationkey#102L)\n",
      "23/11/02 14:44:44 WARN Hypergraph: supersets: Set(E3(s_suppkey#99, s_nationkey#102L))\n",
      "23/11/02 14:44:44 WARN Hypergraph: gyo edge: E3(s_suppkey#99, s_nationkey#102L)\n",
      "23/11/02 14:44:44 WARN Hypergraph: supersets: Set()\n",
      "23/11/02 14:44:44 WARN Hypergraph: parentNode: TreeNode(Set(E3(s_suppkey#99, s_nationkey#102L)))[Set({s_suppkey#99, s_nationkey#102L, s_acctbal#104})] [[parent: false]]\n",
      "\n",
      "23/11/02 14:44:44 WARN Hypergraph: subsets: Set(TreeNode(Set(E4(s_nationkey#102L, n_regionkey#472L)))[Set({n_nationkey#470, n_regionkey#472L})] [[parent: false]]\n",
      "-- TreeNode(Set(E5(n_regionkey#472L)))[Set({r_regionkey#503})] [[parent: true]]\n",
      ", TreeNode(Set(E2(p_partkey#24, s_suppkey#99)))[Set({ps_partkey#157L, ps_suppkey#158L})] [[parent: false]]\n",
      "-- TreeNode(Set(E1(p_partkey#24)))[Set({p_partkey#24})] [[parent: true]]\n",
      ")\n",
      "23/11/02 14:44:44 WARN RewriteJoinsAsSemijoins: join tree: \n",
      "TreeNode(Set(E3(s_suppkey#99, s_nationkey#102L)))[Set({s_suppkey#99, s_nationkey#102L, s_acctbal#104})] [[parent: false]]\n",
      "-- TreeNode(Set(E4(s_nationkey#102L, n_regionkey#472L)))[Set({n_nationkey#470, n_regionkey#472L})] [[parent: true]]\n",
      "-- -- TreeNode(Set(E5(n_regionkey#472L)))[Set({r_regionkey#503})] [[parent: true]]\n",
      "\n",
      "-- TreeNode(Set(E2(p_partkey#24, s_suppkey#99)))[Set({ps_partkey#157L, ps_suppkey#158L})] [[parent: true]]\n",
      "-- -- TreeNode(Set(E1(p_partkey#24)))[Set({p_partkey#24})] [[parent: true]]\n",
      "\n",
      "23/11/02 14:44:44 WARN HTNode: aggAttributes: {s_acctbal#104}\n",
      "23/11/02 14:44:44 WARN HTNode: nodeAttributes: {s_suppkey#99, s_nationkey#102L, s_acctbal#104}\n",
      "23/11/02 14:44:44 WARN HTNode: found subset in:\n",
      "TreeNode(Set(E3(s_suppkey#99, s_nationkey#102L)))[Set({s_suppkey#99, s_nationkey#102L, s_acctbal#104})] [[parent: false]]\n",
      "-- TreeNode(Set(E4(s_nationkey#102L, n_regionkey#472L)))[Set({n_nationkey#470, n_regionkey#472L})] [[parent: true]]\n",
      "-- -- TreeNode(Set(E5(n_regionkey#472L)))[Set({r_regionkey#503})] [[parent: true]]\n",
      "\n",
      "-- TreeNode(Set(E2(p_partkey#24, s_suppkey#99)))[Set({ps_partkey#157L, ps_suppkey#158L})] [[parent: true]]\n",
      "-- -- TreeNode(Set(E1(p_partkey#24)))[Set({p_partkey#24})] [[parent: true]]\n",
      "\n",
      "23/11/02 14:44:44 WARN RewriteJoinsAsSemijoins: rerooted: \n",
      "TreeNode(Set(E3(s_suppkey#99, s_nationkey#102L)))[Set({s_suppkey#99, s_nationkey#102L, s_acctbal#104})] [[parent: false]]\n",
      "-- TreeNode(Set(E4(s_nationkey#102L, n_regionkey#472L)))[Set({n_nationkey#470, n_regionkey#472L})] [[parent: true]]\n",
      "-- -- TreeNode(Set(E5(n_regionkey#472L)))[Set({r_regionkey#503})] [[parent: true]]\n",
      "\n",
      "-- TreeNode(Set(E2(p_partkey#24, s_suppkey#99)))[Set({ps_partkey#157L, ps_suppkey#158L})] [[parent: true]]\n",
      "-- -- TreeNode(Set(E1(p_partkey#24)))[Set({p_partkey#24})] [[parent: true]]\n",
      "\n",
      "23/11/02 14:44:44 WARN RewriteJoinsAsSemijoins: percentile aggregates: List(percentile(s_acctbal#104, 0.5, 1, 0, 0, false) AS median(s_acctbal)#3030)\n",
      "23/11/02 14:44:44 WARN HTNode: keyRefs: List(List(ps_partkey#157L, p_partkey#24), List(ps_suppkey#158L, s_suppkey#99), List(n_regionkey#472L, r_regionkey#503), List(s_nationkey#102L, n_nationkey#470))\n",
      "23/11/02 14:44:44 WARN HTNode: edge: E4(s_nationkey#102L, n_regionkey#472L)\n",
      "23/11/02 14:44:44 WARN HTNode: overlapping vertices: Set((n_regionkey#472L,r_regionkey#503))\n",
      "23/11/02 14:44:44 WARN HTNode: can semi join: true\n",
      "23/11/02 14:44:44 WARN HTNode: join output: List(n_nationkey#470, n_regionkey#472L, c#4092L)\n",
      "23/11/02 14:44:44 WARN HTNode: keyRefs: List(List(ps_partkey#157L, p_partkey#24), List(ps_suppkey#158L, s_suppkey#99), List(n_regionkey#472L, r_regionkey#503), List(s_nationkey#102L, n_nationkey#470))\n",
      "23/11/02 14:44:44 WARN HTNode: edge: E3(s_suppkey#99, s_nationkey#102L)\n",
      "23/11/02 14:44:44 WARN HTNode: overlapping vertices: Set((s_nationkey#102L,n_nationkey#470))\n",
      "23/11/02 14:44:44 WARN HTNode: can semi join: true\n",
      "23/11/02 14:44:44 WARN HTNode: join output: List(s_suppkey#99, s_nationkey#102L, s_acctbal#104, c#4091L)\n",
      "23/11/02 14:44:44 WARN HTNode: keyRefs: List(List(ps_partkey#157L, p_partkey#24), List(ps_suppkey#158L, s_suppkey#99), List(n_regionkey#472L, r_regionkey#503), List(s_nationkey#102L, n_nationkey#470))\n",
      "23/11/02 14:44:44 WARN HTNode: edge: E2(p_partkey#24, s_suppkey#99)\n",
      "23/11/02 14:44:44 WARN HTNode: overlapping vertices: Set((ps_partkey#157L,p_partkey#24))\n",
      "23/11/02 14:44:44 WARN HTNode: can semi join: true\n",
      "23/11/02 14:44:44 WARN HTNode: join output: List(ps_partkey#157L, ps_suppkey#158L, c#4102L)\n",
      "23/11/02 14:44:44 WARN HTNode: keyRefs: List(List(ps_partkey#157L, p_partkey#24), List(ps_suppkey#158L, s_suppkey#99), List(n_regionkey#472L, r_regionkey#503), List(s_nationkey#102L, n_nationkey#470))\n",
      "23/11/02 14:44:44 WARN HTNode: edge: E3(s_suppkey#99, s_nationkey#102L)\n",
      "23/11/02 14:44:44 WARN HTNode: overlapping vertices: Set((s_suppkey#99,ps_suppkey#158L))\n",
      "23/11/02 14:44:44 WARN HTNode: can semi join: false\n",
      "23/11/02 14:44:44 WARN HTNode: join output: List(s_suppkey#99, s_nationkey#102L, s_acctbal#104, c#4091L, c#4111L, ps_suppkey#158L)\n",
      "23/11/02 14:44:44 WARN RewriteJoinsAsSemijoins: new aggregate: Aggregate [percentile(s_acctbal#104, 0.5, 1, 0, 0, false) AS median(s_acctbal)#3030]\n",
      "+- Project [s_acctbal#104, c#4112L]\n",
      "   +- Project [(cast(c#4091L as bigint) * c#4111L) AS c#4112L, s_suppkey#99, s_nationkey#102L, s_acctbal#104, c#4091L, c#4111L, ps_suppkey#158L]\n",
      "      +- Join Inner, (s_suppkey#99 = cast(ps_suppkey#158L as int))\n",
      "         :- Project [s_suppkey#99, s_nationkey#102L, s_acctbal#104, c#4091L]\n",
      "         :  +- Join LeftSemi, (s_nationkey#102L = cast(n_nationkey#470 as bigint))\n",
      "         :     :- Project [s_suppkey#99, s_nationkey#102L, s_acctbal#104, 1 AS c#4091L]\n",
      "         :     :  +- Project [s_suppkey#99, s_nationkey#102L, s_acctbal#104]\n",
      "         :     :     +- Filter (isnotnull(s_suppkey#99) AND isnotnull(s_nationkey#102L))\n",
      "         :     :        +- InMemoryRelation [s_suppkey#99, s_name#106, s_address#101, s_nationkey#102L, s_phone#107, s_acctbal#104, s_comment#105], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         :     :              +- *(1) Project [s_suppkey#99, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, s_name#100, 25, true, false, true) AS s_name#106, s_address#101, s_nationkey#102L, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, s_phone#103, 15, true, false, true) AS s_phone#107, s_acctbal#104, s_comment#105]\n",
      "         :     :                 +- *(1) Scan JDBCRelation(supplier) [numPartitions=1] [s_suppkey#99,s_name#100,s_address#101,s_nationkey#102L,s_phone#103,s_acctbal#104,s_comment#105] PushedFilters: [], ReadSchema: struct<s_suppkey:int,s_name:string,s_address:string,s_nationkey:bigint,s_phone:string,s_acctbal:d...\n",
      "         :     +- Project [n_nationkey#470, n_regionkey#472L, c#4092L]\n",
      "         :        +- Join LeftSemi, (n_regionkey#472L = cast(r_regionkey#503 as bigint))\n",
      "         :           :- Project [n_nationkey#470, n_regionkey#472L, 1 AS c#4092L]\n",
      "         :           :  +- Project [n_nationkey#470, n_regionkey#472L]\n",
      "         :           :     +- Filter (isnotnull(n_nationkey#470) AND isnotnull(n_regionkey#472L))\n",
      "         :           :        +- InMemoryRelation [n_nationkey#470, n_name#474, n_regionkey#472L, n_comment#473], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         :           :              +- *(1) Project [n_nationkey#470, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, n_name#471, 25, true, false, true) AS n_name#474, n_regionkey#472L, n_comment#473]\n",
      "         :           :                 +- *(1) Scan JDBCRelation(nation) [numPartitions=1] [n_nationkey#470,n_name#471,n_regionkey#472L,n_comment#473] PushedFilters: [], ReadSchema: struct<n_nationkey:int,n_name:string,n_regionkey:bigint,n_comment:string>\n",
      "         :           +- Project [r_regionkey#503, 1 AS c#4093L]\n",
      "         :              +- Project [r_regionkey#503]\n",
      "         :                 +- Filter isnotnull(r_regionkey#503)\n",
      "         :                    +- InMemoryRelation [r_regionkey#503, r_name#506, r_comment#505], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         :                          +- *(1) Project [r_regionkey#503, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, r_name#504, 25, true, false, true) AS r_name#506, r_comment#505]\n",
      "         :                             +- *(1) Scan JDBCRelation(region) [numPartitions=1] [r_regionkey#503,r_name#504,r_comment#505] PushedFilters: [], ReadSchema: struct<r_regionkey:int,r_name:string,r_comment:string>\n",
      "         +- Aggregate [ps_suppkey#158L], [sum(c#4102L) AS c#4111L, ps_suppkey#158L]\n",
      "            +- Project [ps_partkey#157L, ps_suppkey#158L, c#4102L]\n",
      "               +- Join LeftSemi, (ps_partkey#157L = cast(p_partkey#24 as bigint))\n",
      "                  :- Project [ps_partkey#157L, ps_suppkey#158L, 1 AS c#4102L]\n",
      "                  :  +- Project [ps_partkey#157L, ps_suppkey#158L]\n",
      "                  :     +- Filter (isnotnull(ps_partkey#157L) AND isnotnull(ps_suppkey#158L))\n",
      "                  :        +- InMemoryRelation [ps_partkey#157L, ps_suppkey#158L, ps_availqty#159, ps_supplycost#160, ps_comment#161], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                  :              +- *(1) Scan JDBCRelation(partsupp) [numPartitions=1] [ps_partkey#157L,ps_suppkey#158L,ps_availqty#159,ps_supplycost#160,ps_comment#161] PushedFilters: [], ReadSchema: struct<ps_partkey:bigint,ps_suppkey:bigint,ps_availqty:int,ps_supplycost:decimal(38,18),ps_commen...\n",
      "                  +- Project [p_partkey#24, 1 AS c#4103L]\n",
      "                     +- Project [p_partkey#24]\n",
      "                        +- Filter isnotnull(p_partkey#24)\n",
      "                           +- InMemoryRelation [p_partkey#24, p_name#25, p_mfgr#33, p_brand#34, p_type#28, p_size#29, p_container#35, p_retailprice#31, p_comment#32], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                                 +- *(1) Project [p_partkey#24, p_name#25, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_mfgr#26, 25, true, false, true) AS p_mfgr#33, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_brand#27, 10, true, false, true) AS p_brand#34, p_type#28, p_size#29, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_container#30, 10, true, false, true) AS p_container#35, p_retailprice#31, p_comment#32]\n",
      "                                    +- *(1) Scan JDBCRelation(part) [numPartitions=1] [p_partkey#24,p_name#25,p_mfgr#26,p_brand#27,p_type#28,p_size#29,p_container#30,p_retailprice#31,p_comment#32] PushedFilters: [], ReadSchema: struct<p_partkey:int,p_name:string,p_mfgr:string,p_brand:string,p_type:string,p_size:int,p_contai...\n",
      "\n",
      "23/11/02 14:44:44 WARN RewriteJoinsAsSemijoins: not applicable to aggregate: Aggregate [ps_suppkey#158L], [sum(c#4102L) AS c#4111L, ps_suppkey#158L]\n",
      "+- Project [ps_partkey#157L, ps_suppkey#158L, c#4102L]\n",
      "   +- Join LeftSemi, (ps_partkey#157L = cast(p_partkey#24 as bigint))\n",
      "      :- Project [ps_partkey#157L, ps_suppkey#158L, 1 AS c#4102L]\n",
      "      :  +- Project [ps_partkey#157L, ps_suppkey#158L]\n",
      "      :     +- Filter (isnotnull(ps_partkey#157L) AND isnotnull(ps_suppkey#158L))\n",
      "      :        +- InMemoryRelation [ps_partkey#157L, ps_suppkey#158L, ps_availqty#159, ps_supplycost#160, ps_comment#161], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "      :              +- *(1) Scan JDBCRelation(partsupp) [numPartitions=1] [ps_partkey#157L,ps_suppkey#158L,ps_availqty#159,ps_supplycost#160,ps_comment#161] PushedFilters: [], ReadSchema: struct<ps_partkey:bigint,ps_suppkey:bigint,ps_availqty:int,ps_supplycost:decimal(38,18),ps_commen...\n",
      "      +- Project [p_partkey#24, 1 AS c#4103L]\n",
      "         +- Project [p_partkey#24]\n",
      "            +- Filter isnotnull(p_partkey#24)\n",
      "               +- InMemoryRelation [p_partkey#24, p_name#25, p_mfgr#33, p_brand#34, p_type#28, p_size#29, p_container#35, p_retailprice#31, p_comment#32], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                     +- *(1) Project [p_partkey#24, p_name#25, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_mfgr#26, 25, true, false, true) AS p_mfgr#33, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_brand#27, 10, true, false, true) AS p_brand#34, p_type#28, p_size#29, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_container#30, 10, true, false, true) AS p_container#35, p_retailprice#31, p_comment#32]\n",
      "                        +- *(1) Scan JDBCRelation(part) [numPartitions=1] [p_partkey#24,p_name#25,p_mfgr#26,p_brand#27,p_type#28,p_size#29,p_container#30,p_retailprice#31,p_comment#32] PushedFilters: [], ReadSchema: struct<p_partkey:int,p_name:string,p_mfgr:string,p_brand:string,p_type:string,p_size:int,p_contai...\n",
      "\n",
      "23/11/02 14:44:45 WARN ResolveHints$ResolveForeignKeyHints: cannot not apply foreign key hint if yannakakis is not enabled\n",
      "23/11/02 14:44:45 WARN ResolveHints$ResolveForeignKeyHints: cannot not apply foreign key hint if yannakakis is not enabled\n",
      "23/11/02 14:44:45 WARN ResolveHints$ResolveForeignKeyHints: cannot not apply foreign key hint if yannakakis is not enabled\n",
      "23/11/02 14:44:45 WARN ResolveHints$ResolveForeignKeyHints: cannot not apply foreign key hint if yannakakis is not enabled\n",
      "23/11/02 14:44:45 WARN ResolveHints$ResolveForeignKeyHints: cannot not apply primary key hint if yannakakis is not enabled\n",
      "23/11/02 14:44:45 WARN HintErrorLogger: Unrecognized hint: PK(ps_partkey, ps_suppkey)\n",
      "23/11/02 14:44:45 WARN HintErrorLogger: Unrecognized hint: FK(s_nationkey, n_nationkey)\n",
      "23/11/02 14:44:45 WARN HintErrorLogger: Unrecognized hint: FK(ps_suppkey, s_suppkey)\n",
      "23/11/02 14:44:45 WARN HintErrorLogger: Unrecognized hint: FK(n_regionkey, r_regionkey)\n",
      "23/11/02 14:44:45 WARN HintErrorLogger: Unrecognized hint: FK(ps_partkey, p_partkey)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|median(s_acctbal)|\n",
      "+-----------------+\n",
      "|4499.360000000001|\n",
      "+-----------------+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'UnresolvedHint FK, ['ps_partkey, 'p_partkey]\n",
      "+- 'UnresolvedHint FK, ['n_regionkey, 'r_regionkey]\n",
      "   +- 'UnresolvedHint FK, ['ps_suppkey, 's_suppkey]\n",
      "      +- 'UnresolvedHint FK, ['s_nationkey, 'n_nationkey]\n",
      "         +- 'UnresolvedHint PK, ['ps_partkey, 'ps_suppkey]\n",
      "            +- 'Project [unresolvedalias('median('s_acctbal), None)]\n",
      "               +- 'Filter ((('p_partkey = 'ps_partkey) AND ('s_suppkey = 'ps_suppkey)) AND (('s_nationkey = 'n_nationkey) AND ('n_regionkey = 'r_regionkey)))\n",
      "                  +- 'Join Inner\n",
      "                     :- 'Join Inner\n",
      "                     :  :- 'Join Inner\n",
      "                     :  :  :- 'Join Inner\n",
      "                     :  :  :  :- 'UnresolvedRelation [part], [], false\n",
      "                     :  :  :  +- 'UnresolvedRelation [partsupp], [], false\n",
      "                     :  :  +- 'UnresolvedRelation [supplier], [], false\n",
      "                     :  +- 'UnresolvedRelation [nation], [], false\n",
      "                     +- 'UnresolvedRelation [region], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "median(s_acctbal): double\n",
      "Aggregate [median(s_acctbal#104) AS median(s_acctbal)#3030]\n",
      "+- FKHint [[ps_partkey#157L, p_partkey#24], [ps_suppkey#158L, s_suppkey#99], [n_regionkey#472L, r_regionkey#503], [s_nationkey#102L, n_nationkey#470]], [[ps_partkey#157L, ps_suppkey#158L]]\n",
      "   +- Filter (((cast(p_partkey#24 as bigint) = ps_partkey#157L) AND (cast(s_suppkey#99 as bigint) = ps_suppkey#158L)) AND ((s_nationkey#102L = cast(n_nationkey#470 as bigint)) AND (n_regionkey#472L = cast(r_regionkey#503 as bigint))))\n",
      "      +- Join Inner\n",
      "         :- Join Inner\n",
      "         :  :- Join Inner\n",
      "         :  :  :- Join Inner\n",
      "         :  :  :  :- SubqueryAlias part\n",
      "         :  :  :  :  +- View (`part`, [p_partkey#24,p_name#25,p_mfgr#33,p_brand#34,p_type#28,p_size#29,p_container#35,p_retailprice#31,p_comment#32])\n",
      "         :  :  :  :     +- Project [p_partkey#24, p_name#25, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_mfgr#26, 25, true, false, true) AS p_mfgr#33, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_brand#27, 10, true, false, true) AS p_brand#34, p_type#28, p_size#29, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_container#30, 10, true, false, true) AS p_container#35, p_retailprice#31, p_comment#32]\n",
      "         :  :  :  :        +- Relation [p_partkey#24,p_name#25,p_mfgr#26,p_brand#27,p_type#28,p_size#29,p_container#30,p_retailprice#31,p_comment#32] JDBCRelation(part) [numPartitions=1]\n",
      "         :  :  :  +- SubqueryAlias partsupp\n",
      "         :  :  :     +- View (`partsupp`, [ps_partkey#157L,ps_suppkey#158L,ps_availqty#159,ps_supplycost#160,ps_comment#161])\n",
      "         :  :  :        +- Relation [ps_partkey#157L,ps_suppkey#158L,ps_availqty#159,ps_supplycost#160,ps_comment#161] JDBCRelation(partsupp) [numPartitions=1]\n",
      "         :  :  +- SubqueryAlias supplier\n",
      "         :  :     +- View (`supplier`, [s_suppkey#99,s_name#106,s_address#101,s_nationkey#102L,s_phone#107,s_acctbal#104,s_comment#105])\n",
      "         :  :        +- Project [s_suppkey#99, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, s_name#100, 25, true, false, true) AS s_name#106, s_address#101, s_nationkey#102L, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, s_phone#103, 15, true, false, true) AS s_phone#107, s_acctbal#104, s_comment#105]\n",
      "         :  :           +- Relation [s_suppkey#99,s_name#100,s_address#101,s_nationkey#102L,s_phone#103,s_acctbal#104,s_comment#105] JDBCRelation(supplier) [numPartitions=1]\n",
      "         :  +- SubqueryAlias nation\n",
      "         :     +- View (`nation`, [n_nationkey#470,n_name#474,n_regionkey#472L,n_comment#473])\n",
      "         :        +- Project [n_nationkey#470, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, n_name#471, 25, true, false, true) AS n_name#474, n_regionkey#472L, n_comment#473]\n",
      "         :           +- Relation [n_nationkey#470,n_name#471,n_regionkey#472L,n_comment#473] JDBCRelation(nation) [numPartitions=1]\n",
      "         +- SubqueryAlias region\n",
      "            +- View (`region`, [r_regionkey#503,r_name#506,r_comment#505])\n",
      "               +- Project [r_regionkey#503, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, r_name#504, 25, true, false, true) AS r_name#506, r_comment#505]\n",
      "                  +- Relation [r_regionkey#503,r_name#504,r_comment#505] JDBCRelation(region) [numPartitions=1]\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [percentile(s_acctbal#104, 0.5, 1, 0, 0, false) AS median(s_acctbal)#3030]\n",
      "+- Project [s_acctbal#104]\n",
      "   +- Join Inner, (s_suppkey#99 = cast(ps_suppkey#158L as int))\n",
      "      :- Project [s_suppkey#99, s_acctbal#104]\n",
      "      :  +- Join LeftSemi, (s_nationkey#102L = cast(n_nationkey#470 as bigint))\n",
      "      :     :- Project [s_suppkey#99, s_nationkey#102L, s_acctbal#104]\n",
      "      :     :  +- Filter (isnotnull(s_suppkey#99) AND isnotnull(s_nationkey#102L))\n",
      "      :     :     +- InMemoryRelation [s_suppkey#99, s_name#106, s_address#101, s_nationkey#102L, s_phone#107, s_acctbal#104, s_comment#105], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "      :     :           +- *(1) Project [s_suppkey#99, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, s_name#100, 25, true, false, true) AS s_name#106, s_address#101, s_nationkey#102L, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, s_phone#103, 15, true, false, true) AS s_phone#107, s_acctbal#104, s_comment#105]\n",
      "      :     :              +- *(1) Scan JDBCRelation(supplier) [numPartitions=1] [s_suppkey#99,s_name#100,s_address#101,s_nationkey#102L,s_phone#103,s_acctbal#104,s_comment#105] PushedFilters: [], ReadSchema: struct<s_suppkey:int,s_name:string,s_address:string,s_nationkey:bigint,s_phone:string,s_acctbal:d...\n",
      "      :     +- Project [n_nationkey#470]\n",
      "      :        +- Join LeftSemi, (n_regionkey#472L = cast(r_regionkey#503 as bigint))\n",
      "      :           :- Project [n_nationkey#470, n_regionkey#472L]\n",
      "      :           :  +- Filter (isnotnull(n_nationkey#470) AND isnotnull(n_regionkey#472L))\n",
      "      :           :     +- InMemoryRelation [n_nationkey#470, n_name#474, n_regionkey#472L, n_comment#473], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "      :           :           +- *(1) Project [n_nationkey#470, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, n_name#471, 25, true, false, true) AS n_name#474, n_regionkey#472L, n_comment#473]\n",
      "      :           :              +- *(1) Scan JDBCRelation(nation) [numPartitions=1] [n_nationkey#470,n_name#471,n_regionkey#472L,n_comment#473] PushedFilters: [], ReadSchema: struct<n_nationkey:int,n_name:string,n_regionkey:bigint,n_comment:string>\n",
      "      :           +- Project [r_regionkey#503]\n",
      "      :              +- Filter isnotnull(r_regionkey#503)\n",
      "      :                 +- InMemoryRelation [r_regionkey#503, r_name#506, r_comment#505], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "      :                       +- *(1) Project [r_regionkey#503, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, r_name#504, 25, true, false, true) AS r_name#506, r_comment#505]\n",
      "      :                          +- *(1) Scan JDBCRelation(region) [numPartitions=1] [r_regionkey#503,r_name#504,r_comment#505] PushedFilters: [], ReadSchema: struct<r_regionkey:int,r_name:string,r_comment:string>\n",
      "      +- Aggregate [ps_suppkey#158L], [ps_suppkey#158L]\n",
      "         +- Project [ps_suppkey#158L]\n",
      "            +- Join LeftSemi, (ps_partkey#157L = cast(p_partkey#24 as bigint))\n",
      "               :- Project [ps_partkey#157L, ps_suppkey#158L]\n",
      "               :  +- Filter (isnotnull(ps_partkey#157L) AND isnotnull(ps_suppkey#158L))\n",
      "               :     +- InMemoryRelation [ps_partkey#157L, ps_suppkey#158L, ps_availqty#159, ps_supplycost#160, ps_comment#161], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "               :           +- *(1) Scan JDBCRelation(partsupp) [numPartitions=1] [ps_partkey#157L,ps_suppkey#158L,ps_availqty#159,ps_supplycost#160,ps_comment#161] PushedFilters: [], ReadSchema: struct<ps_partkey:bigint,ps_suppkey:bigint,ps_availqty:int,ps_supplycost:decimal(38,18),ps_commen...\n",
      "               +- Project [p_partkey#24]\n",
      "                  +- Filter isnotnull(p_partkey#24)\n",
      "                     +- InMemoryRelation [p_partkey#24, p_name#25, p_mfgr#33, p_brand#34, p_type#28, p_size#29, p_container#35, p_retailprice#31, p_comment#32], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                           +- *(1) Project [p_partkey#24, p_name#25, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_mfgr#26, 25, true, false, true) AS p_mfgr#33, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_brand#27, 10, true, false, true) AS p_brand#34, p_type#28, p_size#29, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_container#30, 10, true, false, true) AS p_container#35, p_retailprice#31, p_comment#32]\n",
      "                              +- *(1) Scan JDBCRelation(part) [numPartitions=1] [p_partkey#24,p_name#25,p_mfgr#26,p_brand#27,p_type#28,p_size#29,p_container#30,p_retailprice#31,p_comment#32] PushedFilters: [], ReadSchema: struct<p_partkey:int,p_name:string,p_mfgr:string,p_brand:string,p_type:string,p_size:int,p_contai...\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- ObjectHashAggregate(keys=[], functions=[percentile(s_acctbal#104, 0.5, 1, 0, 0, false)], output=[median(s_acctbal)#3030])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=3703]\n",
      "      +- ObjectHashAggregate(keys=[], functions=[partial_percentile(s_acctbal#104, 0.5, 1, 0, 0, false)], output=[buf#3478])\n",
      "         +- Project [s_acctbal#104]\n",
      "            +- SortMergeJoin [s_suppkey#99], [cast(ps_suppkey#158L as int)], Inner\n",
      "               :- Sort [s_suppkey#99 ASC NULLS FIRST], false, 0\n",
      "               :  +- Exchange hashpartitioning(s_suppkey#99, 200), ENSURE_REQUIREMENTS, [plan_id=3695]\n",
      "               :     +- Project [s_suppkey#99, s_acctbal#104]\n",
      "               :        +- BroadcastHashJoin [s_nationkey#102L], [cast(n_nationkey#470 as bigint)], LeftSemi, BuildRight, false\n",
      "               :           :- Filter (isnotnull(s_suppkey#99) AND isnotnull(s_nationkey#102L))\n",
      "               :           :  +- Scan In-memory table supplier [s_suppkey#99, s_nationkey#102L, s_acctbal#104], [isnotnull(s_suppkey#99), isnotnull(s_nationkey#102L)]\n",
      "               :           :        +- InMemoryRelation [s_suppkey#99, s_name#106, s_address#101, s_nationkey#102L, s_phone#107, s_acctbal#104, s_comment#105], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "               :           :              +- *(1) Project [s_suppkey#99, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, s_name#100, 25, true, false, true) AS s_name#106, s_address#101, s_nationkey#102L, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, s_phone#103, 15, true, false, true) AS s_phone#107, s_acctbal#104, s_comment#105]\n",
      "               :           :                 +- *(1) Scan JDBCRelation(supplier) [numPartitions=1] [s_suppkey#99,s_name#100,s_address#101,s_nationkey#102L,s_phone#103,s_acctbal#104,s_comment#105] PushedFilters: [], ReadSchema: struct<s_suppkey:int,s_name:string,s_address:string,s_nationkey:bigint,s_phone:string,s_acctbal:d...\n",
      "               :           +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=3679]\n",
      "               :              +- Project [n_nationkey#470]\n",
      "               :                 +- BroadcastHashJoin [n_regionkey#472L], [cast(r_regionkey#503 as bigint)], LeftSemi, BuildRight, false\n",
      "               :                    :- Filter (isnotnull(n_nationkey#470) AND isnotnull(n_regionkey#472L))\n",
      "               :                    :  +- Scan In-memory table nation [n_nationkey#470, n_regionkey#472L], [isnotnull(n_nationkey#470), isnotnull(n_regionkey#472L)]\n",
      "               :                    :        +- InMemoryRelation [n_nationkey#470, n_name#474, n_regionkey#472L, n_comment#473], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "               :                    :              +- *(1) Project [n_nationkey#470, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, n_name#471, 25, true, false, true) AS n_name#474, n_regionkey#472L, n_comment#473]\n",
      "               :                    :                 +- *(1) Scan JDBCRelation(nation) [numPartitions=1] [n_nationkey#470,n_name#471,n_regionkey#472L,n_comment#473] PushedFilters: [], ReadSchema: struct<n_nationkey:int,n_name:string,n_regionkey:bigint,n_comment:string>\n",
      "               :                    +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=3675]\n",
      "               :                       +- Filter isnotnull(r_regionkey#503)\n",
      "               :                          +- Scan In-memory table region [r_regionkey#503], [isnotnull(r_regionkey#503)]\n",
      "               :                                +- InMemoryRelation [r_regionkey#503, r_name#506, r_comment#505], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "               :                                      +- *(1) Project [r_regionkey#503, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, r_name#504, 25, true, false, true) AS r_name#506, r_comment#505]\n",
      "               :                                         +- *(1) Scan JDBCRelation(region) [numPartitions=1] [r_regionkey#503,r_name#504,r_comment#505] PushedFilters: [], ReadSchema: struct<r_regionkey:int,r_name:string,r_comment:string>\n",
      "               +- Sort [cast(ps_suppkey#158L as int) ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(cast(ps_suppkey#158L as int), 200), ENSURE_REQUIREMENTS, [plan_id=3696]\n",
      "                     +- HashAggregate(keys=[ps_suppkey#158L], functions=[], output=[ps_suppkey#158L])\n",
      "                        +- Exchange hashpartitioning(ps_suppkey#158L, 200), ENSURE_REQUIREMENTS, [plan_id=3691]\n",
      "                           +- HashAggregate(keys=[ps_suppkey#158L], functions=[], output=[ps_suppkey#158L])\n",
      "                              +- Project [ps_suppkey#158L]\n",
      "                                 +- SortMergeJoin [ps_partkey#157L], [cast(p_partkey#24 as bigint)], LeftSemi\n",
      "                                    :- Sort [ps_partkey#157L ASC NULLS FIRST], false, 0\n",
      "                                    :  +- Exchange hashpartitioning(ps_partkey#157L, 200), ENSURE_REQUIREMENTS, [plan_id=3683]\n",
      "                                    :     +- Filter (isnotnull(ps_partkey#157L) AND isnotnull(ps_suppkey#158L))\n",
      "                                    :        +- Scan In-memory table partsupp [ps_partkey#157L, ps_suppkey#158L], [isnotnull(ps_partkey#157L), isnotnull(ps_suppkey#158L)]\n",
      "                                    :              +- InMemoryRelation [ps_partkey#157L, ps_suppkey#158L, ps_availqty#159, ps_supplycost#160, ps_comment#161], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                                    :                    +- *(1) Scan JDBCRelation(partsupp) [numPartitions=1] [ps_partkey#157L,ps_suppkey#158L,ps_availqty#159,ps_supplycost#160,ps_comment#161] PushedFilters: [], ReadSchema: struct<ps_partkey:bigint,ps_suppkey:bigint,ps_availqty:int,ps_supplycost:decimal(38,18),ps_commen...\n",
      "                                    +- Sort [cast(p_partkey#24 as bigint) ASC NULLS FIRST], false, 0\n",
      "                                       +- Exchange hashpartitioning(cast(p_partkey#24 as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=3684]\n",
      "                                          +- Filter isnotnull(p_partkey#24)\n",
      "                                             +- Scan In-memory table part [p_partkey#24], [isnotnull(p_partkey#24)]\n",
      "                                                   +- InMemoryRelation [p_partkey#24, p_name#25, p_mfgr#33, p_brand#34, p_type#28, p_size#29, p_container#35, p_retailprice#31, p_comment#32], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                                                         +- *(1) Project [p_partkey#24, p_name#25, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_mfgr#26, 25, true, false, true) AS p_mfgr#33, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_brand#27, 10, true, false, true) AS p_brand#34, p_type#28, p_size#29, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_container#30, 10, true, false, true) AS p_container#35, p_retailprice#31, p_comment#32]\n",
      "                                                            +- *(1) Scan JDBCRelation(part) [numPartitions=1] [p_partkey#24,p_name#25,p_mfgr#26,p_brand#27,p_type#28,p_size#29,p_container#30,p_retailprice#31,p_comment#32] PushedFilters: [], ReadSchema: struct<p_partkey:int,p_name:string,p_mfgr:string,p_brand:string,p_type:string,p_size:int,p_contai...\n",
      "\n",
      "+--------------------+-----+\n",
      "|                 key|value|\n",
      "+--------------------+-----+\n",
      "|spark.sql.yannaka...|false|\n",
      "+--------------------+-----+\n",
      "\n",
      "running query: \n",
      "select\n",
      "\n",
      "/*+ FK(ps_partkey, p_partkey), FK(n_regionkey, r_regionkey), FK(ps_suppkey, s_suppkey), FK(s_nationkey, n_nationkey), PK(ps_partkey, ps_suppkey) */\n",
      "\n",
      "        median(s_acctbal)\n",
      "\n",
      "\t\tfrom\n",
      "\n",
      "            part,\n",
      "\n",
      "\t\t\tpartsupp,\n",
      "\n",
      "\t\t\tsupplier,\n",
      "\n",
      "\t\t\tnation,\n",
      "\n",
      "\t\t\tregion\n",
      "\n",
      "\t\twhere\n",
      "\n",
      "\t\t\tp_partkey = ps_partkey\n",
      "\n",
      "\t\t\tand s_suppkey = ps_suppkey\n",
      "\n",
      "\t\t\tand s_nationkey = n_nationkey\n",
      "\n",
      "\t\t\tand n_regionkey = r_regionkey\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 87:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|median(s_acctbal)|\n",
      "+-----------------+\n",
      "|4499.360000000001|\n",
      "+-----------------+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'UnresolvedHint FK, ['ps_partkey, 'p_partkey]\n",
      "+- 'UnresolvedHint FK, ['n_regionkey, 'r_regionkey]\n",
      "   +- 'UnresolvedHint FK, ['ps_suppkey, 's_suppkey]\n",
      "      +- 'UnresolvedHint FK, ['s_nationkey, 'n_nationkey]\n",
      "         +- 'UnresolvedHint PK, ['ps_partkey, 'ps_suppkey]\n",
      "            +- 'Project [unresolvedalias('median('s_acctbal), None)]\n",
      "               +- 'Filter ((('p_partkey = 'ps_partkey) AND ('s_suppkey = 'ps_suppkey)) AND (('s_nationkey = 'n_nationkey) AND ('n_regionkey = 'r_regionkey)))\n",
      "                  +- 'Join Inner\n",
      "                     :- 'Join Inner\n",
      "                     :  :- 'Join Inner\n",
      "                     :  :  :- 'Join Inner\n",
      "                     :  :  :  :- 'UnresolvedRelation [part], [], false\n",
      "                     :  :  :  +- 'UnresolvedRelation [partsupp], [], false\n",
      "                     :  :  +- 'UnresolvedRelation [supplier], [], false\n",
      "                     :  +- 'UnresolvedRelation [nation], [], false\n",
      "                     +- 'UnresolvedRelation [region], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "median(s_acctbal): double\n",
      "Aggregate [median(s_acctbal#104) AS median(s_acctbal)#4275]\n",
      "+- Filter (((cast(p_partkey#24 as bigint) = ps_partkey#157L) AND (cast(s_suppkey#99 as bigint) = ps_suppkey#158L)) AND ((s_nationkey#102L = cast(n_nationkey#470 as bigint)) AND (n_regionkey#472L = cast(r_regionkey#503 as bigint))))\n",
      "   +- Join Inner\n",
      "      :- Join Inner\n",
      "      :  :- Join Inner\n",
      "      :  :  :- Join Inner\n",
      "      :  :  :  :- SubqueryAlias part\n",
      "      :  :  :  :  +- View (`part`, [p_partkey#24,p_name#25,p_mfgr#33,p_brand#34,p_type#28,p_size#29,p_container#35,p_retailprice#31,p_comment#32])\n",
      "      :  :  :  :     +- Project [p_partkey#24, p_name#25, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_mfgr#26, 25, true, false, true) AS p_mfgr#33, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_brand#27, 10, true, false, true) AS p_brand#34, p_type#28, p_size#29, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_container#30, 10, true, false, true) AS p_container#35, p_retailprice#31, p_comment#32]\n",
      "      :  :  :  :        +- Relation [p_partkey#24,p_name#25,p_mfgr#26,p_brand#27,p_type#28,p_size#29,p_container#30,p_retailprice#31,p_comment#32] JDBCRelation(part) [numPartitions=1]\n",
      "      :  :  :  +- SubqueryAlias partsupp\n",
      "      :  :  :     +- View (`partsupp`, [ps_partkey#157L,ps_suppkey#158L,ps_availqty#159,ps_supplycost#160,ps_comment#161])\n",
      "      :  :  :        +- Relation [ps_partkey#157L,ps_suppkey#158L,ps_availqty#159,ps_supplycost#160,ps_comment#161] JDBCRelation(partsupp) [numPartitions=1]\n",
      "      :  :  +- SubqueryAlias supplier\n",
      "      :  :     +- View (`supplier`, [s_suppkey#99,s_name#106,s_address#101,s_nationkey#102L,s_phone#107,s_acctbal#104,s_comment#105])\n",
      "      :  :        +- Project [s_suppkey#99, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, s_name#100, 25, true, false, true) AS s_name#106, s_address#101, s_nationkey#102L, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, s_phone#103, 15, true, false, true) AS s_phone#107, s_acctbal#104, s_comment#105]\n",
      "      :  :           +- Relation [s_suppkey#99,s_name#100,s_address#101,s_nationkey#102L,s_phone#103,s_acctbal#104,s_comment#105] JDBCRelation(supplier) [numPartitions=1]\n",
      "      :  +- SubqueryAlias nation\n",
      "      :     +- View (`nation`, [n_nationkey#470,n_name#474,n_regionkey#472L,n_comment#473])\n",
      "      :        +- Project [n_nationkey#470, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, n_name#471, 25, true, false, true) AS n_name#474, n_regionkey#472L, n_comment#473]\n",
      "      :           +- Relation [n_nationkey#470,n_name#471,n_regionkey#472L,n_comment#473] JDBCRelation(nation) [numPartitions=1]\n",
      "      +- SubqueryAlias region\n",
      "         +- View (`region`, [r_regionkey#503,r_name#506,r_comment#505])\n",
      "            +- Project [r_regionkey#503, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, r_name#504, 25, true, false, true) AS r_name#506, r_comment#505]\n",
      "               +- Relation [r_regionkey#503,r_name#504,r_comment#505] JDBCRelation(region) [numPartitions=1]\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [percentile(s_acctbal#104, 0.5, 1, 0, 0, false) AS median(s_acctbal)#4275]\n",
      "+- Project [s_acctbal#104]\n",
      "   +- Join Inner, (n_regionkey#472L = cast(r_regionkey#503 as bigint))\n",
      "      :- Project [s_acctbal#104, n_regionkey#472L]\n",
      "      :  +- Join Inner, (s_nationkey#102L = cast(n_nationkey#470 as bigint))\n",
      "      :     :- Project [s_nationkey#102L, s_acctbal#104]\n",
      "      :     :  +- Join Inner, (cast(s_suppkey#99 as bigint) = ps_suppkey#158L)\n",
      "      :     :     :- Project [ps_suppkey#158L]\n",
      "      :     :     :  +- Join Inner, (cast(p_partkey#24 as bigint) = ps_partkey#157L)\n",
      "      :     :     :     :- Project [p_partkey#24]\n",
      "      :     :     :     :  +- Filter isnotnull(p_partkey#24)\n",
      "      :     :     :     :     +- InMemoryRelation [p_partkey#24, p_name#25, p_mfgr#33, p_brand#34, p_type#28, p_size#29, p_container#35, p_retailprice#31, p_comment#32], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "      :     :     :     :           +- *(1) Project [p_partkey#24, p_name#25, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_mfgr#26, 25, true, false, true) AS p_mfgr#33, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_brand#27, 10, true, false, true) AS p_brand#34, p_type#28, p_size#29, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_container#30, 10, true, false, true) AS p_container#35, p_retailprice#31, p_comment#32]\n",
      "      :     :     :     :              +- *(1) Scan JDBCRelation(part) [numPartitions=1] [p_partkey#24,p_name#25,p_mfgr#26,p_brand#27,p_type#28,p_size#29,p_container#30,p_retailprice#31,p_comment#32] PushedFilters: [], ReadSchema: struct<p_partkey:int,p_name:string,p_mfgr:string,p_brand:string,p_type:string,p_size:int,p_contai...\n",
      "      :     :     :     +- Project [ps_partkey#157L, ps_suppkey#158L]\n",
      "      :     :     :        +- Filter (isnotnull(ps_partkey#157L) AND isnotnull(ps_suppkey#158L))\n",
      "      :     :     :           +- InMemoryRelation [ps_partkey#157L, ps_suppkey#158L, ps_availqty#159, ps_supplycost#160, ps_comment#161], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "      :     :     :                 +- *(1) Scan JDBCRelation(partsupp) [numPartitions=1] [ps_partkey#157L,ps_suppkey#158L,ps_availqty#159,ps_supplycost#160,ps_comment#161] PushedFilters: [], ReadSchema: struct<ps_partkey:bigint,ps_suppkey:bigint,ps_availqty:int,ps_supplycost:decimal(38,18),ps_commen...\n",
      "      :     :     +- Project [s_suppkey#99, s_nationkey#102L, s_acctbal#104]\n",
      "      :     :        +- Filter (isnotnull(s_suppkey#99) AND isnotnull(s_nationkey#102L))\n",
      "      :     :           +- InMemoryRelation [s_suppkey#99, s_name#106, s_address#101, s_nationkey#102L, s_phone#107, s_acctbal#104, s_comment#105], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "      :     :                 +- *(1) Project [s_suppkey#99, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, s_name#100, 25, true, false, true) AS s_name#106, s_address#101, s_nationkey#102L, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, s_phone#103, 15, true, false, true) AS s_phone#107, s_acctbal#104, s_comment#105]\n",
      "      :     :                    +- *(1) Scan JDBCRelation(supplier) [numPartitions=1] [s_suppkey#99,s_name#100,s_address#101,s_nationkey#102L,s_phone#103,s_acctbal#104,s_comment#105] PushedFilters: [], ReadSchema: struct<s_suppkey:int,s_name:string,s_address:string,s_nationkey:bigint,s_phone:string,s_acctbal:d...\n",
      "      :     +- Project [n_nationkey#470, n_regionkey#472L]\n",
      "      :        +- Filter (isnotnull(n_nationkey#470) AND isnotnull(n_regionkey#472L))\n",
      "      :           +- InMemoryRelation [n_nationkey#470, n_name#474, n_regionkey#472L, n_comment#473], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "      :                 +- *(1) Project [n_nationkey#470, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, n_name#471, 25, true, false, true) AS n_name#474, n_regionkey#472L, n_comment#473]\n",
      "      :                    +- *(1) Scan JDBCRelation(nation) [numPartitions=1] [n_nationkey#470,n_name#471,n_regionkey#472L,n_comment#473] PushedFilters: [], ReadSchema: struct<n_nationkey:int,n_name:string,n_regionkey:bigint,n_comment:string>\n",
      "      +- Project [r_regionkey#503]\n",
      "         +- Filter isnotnull(r_regionkey#503)\n",
      "            +- InMemoryRelation [r_regionkey#503, r_name#506, r_comment#505], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                  +- *(1) Project [r_regionkey#503, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, r_name#504, 25, true, false, true) AS r_name#506, r_comment#505]\n",
      "                     +- *(1) Scan JDBCRelation(region) [numPartitions=1] [r_regionkey#503,r_name#504,r_comment#505] PushedFilters: [], ReadSchema: struct<r_regionkey:int,r_name:string,r_comment:string>\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- ObjectHashAggregate(keys=[], functions=[percentile(s_acctbal#104, 0.5, 1, 0, 0, false)], output=[median(s_acctbal)#4275])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=4792]\n",
      "      +- ObjectHashAggregate(keys=[], functions=[partial_percentile(s_acctbal#104, 0.5, 1, 0, 0, false)], output=[buf#4701])\n",
      "         +- Project [s_acctbal#104]\n",
      "            +- BroadcastHashJoin [n_regionkey#472L], [cast(r_regionkey#503 as bigint)], Inner, BuildRight, false\n",
      "               :- Project [s_acctbal#104, n_regionkey#472L]\n",
      "               :  +- BroadcastHashJoin [s_nationkey#102L], [cast(n_nationkey#470 as bigint)], Inner, BuildRight, false\n",
      "               :     :- Project [s_nationkey#102L, s_acctbal#104]\n",
      "               :     :  +- SortMergeJoin [ps_suppkey#158L], [cast(s_suppkey#99 as bigint)], Inner\n",
      "               :     :     :- Sort [ps_suppkey#158L ASC NULLS FIRST], false, 0\n",
      "               :     :     :  +- Exchange hashpartitioning(ps_suppkey#158L, 200), ENSURE_REQUIREMENTS, [plan_id=4776]\n",
      "               :     :     :     +- Project [ps_suppkey#158L]\n",
      "               :     :     :        +- SortMergeJoin [cast(p_partkey#24 as bigint)], [ps_partkey#157L], Inner\n",
      "               :     :     :           :- Sort [cast(p_partkey#24 as bigint) ASC NULLS FIRST], false, 0\n",
      "               :     :     :           :  +- Exchange hashpartitioning(cast(p_partkey#24 as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=4768]\n",
      "               :     :     :           :     +- Filter isnotnull(p_partkey#24)\n",
      "               :     :     :           :        +- Scan In-memory table part [p_partkey#24], [isnotnull(p_partkey#24)]\n",
      "               :     :     :           :              +- InMemoryRelation [p_partkey#24, p_name#25, p_mfgr#33, p_brand#34, p_type#28, p_size#29, p_container#35, p_retailprice#31, p_comment#32], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "               :     :     :           :                    +- *(1) Project [p_partkey#24, p_name#25, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_mfgr#26, 25, true, false, true) AS p_mfgr#33, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_brand#27, 10, true, false, true) AS p_brand#34, p_type#28, p_size#29, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_container#30, 10, true, false, true) AS p_container#35, p_retailprice#31, p_comment#32]\n",
      "               :     :     :           :                       +- *(1) Scan JDBCRelation(part) [numPartitions=1] [p_partkey#24,p_name#25,p_mfgr#26,p_brand#27,p_type#28,p_size#29,p_container#30,p_retailprice#31,p_comment#32] PushedFilters: [], ReadSchema: struct<p_partkey:int,p_name:string,p_mfgr:string,p_brand:string,p_type:string,p_size:int,p_contai...\n",
      "               :     :     :           +- Sort [ps_partkey#157L ASC NULLS FIRST], false, 0\n",
      "               :     :     :              +- Exchange hashpartitioning(ps_partkey#157L, 200), ENSURE_REQUIREMENTS, [plan_id=4769]\n",
      "               :     :     :                 +- Filter (isnotnull(ps_partkey#157L) AND isnotnull(ps_suppkey#158L))\n",
      "               :     :     :                    +- Scan In-memory table partsupp [ps_partkey#157L, ps_suppkey#158L], [isnotnull(ps_partkey#157L), isnotnull(ps_suppkey#158L)]\n",
      "               :     :     :                          +- InMemoryRelation [ps_partkey#157L, ps_suppkey#158L, ps_availqty#159, ps_supplycost#160, ps_comment#161], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "               :     :     :                                +- *(1) Scan JDBCRelation(partsupp) [numPartitions=1] [ps_partkey#157L,ps_suppkey#158L,ps_availqty#159,ps_supplycost#160,ps_comment#161] PushedFilters: [], ReadSchema: struct<ps_partkey:bigint,ps_suppkey:bigint,ps_availqty:int,ps_supplycost:decimal(38,18),ps_commen...\n",
      "               :     :     +- Sort [cast(s_suppkey#99 as bigint) ASC NULLS FIRST], false, 0\n",
      "               :     :        +- Exchange hashpartitioning(cast(s_suppkey#99 as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=4777]\n",
      "               :     :           +- Filter (isnotnull(s_suppkey#99) AND isnotnull(s_nationkey#102L))\n",
      "               :     :              +- Scan In-memory table supplier [s_suppkey#99, s_nationkey#102L, s_acctbal#104], [isnotnull(s_suppkey#99), isnotnull(s_nationkey#102L)]\n",
      "               :     :                    +- InMemoryRelation [s_suppkey#99, s_name#106, s_address#101, s_nationkey#102L, s_phone#107, s_acctbal#104, s_comment#105], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "               :     :                          +- *(1) Project [s_suppkey#99, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, s_name#100, 25, true, false, true) AS s_name#106, s_address#101, s_nationkey#102L, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, s_phone#103, 15, true, false, true) AS s_phone#107, s_acctbal#104, s_comment#105]\n",
      "               :     :                             +- *(1) Scan JDBCRelation(supplier) [numPartitions=1] [s_suppkey#99,s_name#100,s_address#101,s_nationkey#102L,s_phone#103,s_acctbal#104,s_comment#105] PushedFilters: [], ReadSchema: struct<s_suppkey:int,s_name:string,s_address:string,s_nationkey:bigint,s_phone:string,s_acctbal:d...\n",
      "               :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=4783]\n",
      "               :        +- Filter (isnotnull(n_nationkey#470) AND isnotnull(n_regionkey#472L))\n",
      "               :           +- Scan In-memory table nation [n_nationkey#470, n_regionkey#472L], [isnotnull(n_nationkey#470), isnotnull(n_regionkey#472L)]\n",
      "               :                 +- InMemoryRelation [n_nationkey#470, n_name#474, n_regionkey#472L, n_comment#473], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "               :                       +- *(1) Project [n_nationkey#470, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, n_name#471, 25, true, false, true) AS n_name#474, n_regionkey#472L, n_comment#473]\n",
      "               :                          +- *(1) Scan JDBCRelation(nation) [numPartitions=1] [n_nationkey#470,n_name#471,n_regionkey#472L,n_comment#473] PushedFilters: [], ReadSchema: struct<n_nationkey:int,n_name:string,n_regionkey:bigint,n_comment:string>\n",
      "               +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=4787]\n",
      "                  +- Filter isnotnull(r_regionkey#503)\n",
      "                     +- Scan In-memory table region [r_regionkey#503], [isnotnull(r_regionkey#503)]\n",
      "                           +- InMemoryRelation [r_regionkey#503, r_name#506, r_comment#505], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                                 +- *(1) Project [r_regionkey#503, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, r_name#504, 25, true, false, true) AS r_name#506, r_comment#505]\n",
      "                                    +- *(1) Scan JDBCRelation(region) [numPartitions=1] [r_regionkey#503,r_name#504,r_comment#505] PushedFilters: [], ReadSchema: struct<r_regionkey:int,r_name:string,r_comment:string>\n",
      "\n",
      "time ref: 35.03684973716736\n",
      "time yannakakis: 12.02369999885559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Compare result\n",
    "import time\n",
    "query = 'tpch-kit/dbgen/queries/postgres/2.sql'\n",
    "#query = 'tpch-kit/dbgen/queries/postgres/13.sql'\n",
    "#query = 'count-3.sql'\n",
    "#query = 'tpch-kit/dbgen/queries/postgres/11.sql'\n",
    "#query = '11-simple.sql'\n",
    "query = 'median-1-hint.sql'\n",
    "#query = 'median-1.sql'\n",
    "#query = 'tpch-kit/dbgen/queries/postgres/7.sql'\n",
    "#query = '13-simple.sql'\n",
    "#query = 'subselect-exists.sql'\n",
    "#query = 'min-1.sql'\n",
    "\n",
    "spark.sql(\"SET spark.sql.yannakakis.enabled = true\").show()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df1 = run_query(query)\n",
    "df1.show()\n",
    "df1.explain(mode=\"extended\")\n",
    "\n",
    "end_time = time.time()\n",
    "yannakakis_time = end_time - start_time\n",
    "\n",
    "spark.sql(\"SET spark.sql.yannakakis.enabled = false\").show()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df2 = run_query(query)\n",
    "df2.show()\n",
    "df2.explain(mode=\"extended\")\n",
    "\n",
    "end_time = time.time()\n",
    "ref_time = end_time - start_time\n",
    "\n",
    "#print(f'row count: {df1.count()} vs. {df2.count()}' )\n",
    "print(f'time ref: {ref_time}\\ntime yannakakis: {yannakakis_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09492e05",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67544c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
