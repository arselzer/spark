{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d98322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7397f3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/03 12:21:08 DEBUG FileSystem: Looking for FS supporting file\n",
      "23/10/03 12:21:08 DEBUG FileSystem: looking for configuration option fs.file.impl\n",
      "23/10/03 12:21:08 DEBUG FileSystem: Looking in service filesystems for implementation class\n",
      "23/10/03 12:21:08 DEBUG FileSystem: FS for file is class org.apache.hadoop.fs.LocalFileSystem\n",
      "23/10/03 12:21:08 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "23/10/03 12:21:08 DEBUG SharedState: Applying other initial session options to HadoopConf: spark.executor.memory -> 45g\n",
      "23/10/03 12:21:08 DEBUG SharedState: Applying other initial session options to HadoopConf: spark.driver.memory -> 45g\n",
      "23/10/03 12:21:08 DEBUG SharedState: Applying other initial session options to HadoopConf: spark.app.name -> app\n",
      "23/10/03 12:21:08 DEBUG SharedState: Applying other initial session options to HadoopConf: spark.jars -> postgresql-42.3.3.jar\n",
      "23/10/03 12:21:08 INFO SharedState: Warehouse path is 'file:/home/as/spark-eval/spark-warehouse'.\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: o.e.j.s.ServletContextHandler@34eca6ab{/,null,STOPPED} added {SymlinkAllowedResourceAliasChecker@191af92b{base=null,protected=null},MANAGED}\n",
      "23/10/03 12:21:08 DEBUG DecoratedObjectFactory: Adding Decorator: org.eclipse.jetty.util.DeprecationWarning@4e07ad4d\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: o.e.j.s.ServletContextHandler@34eca6ab{/,null,STOPPED} added {ServletHandler@43204cf{STOPPED},MANAGED}\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: ServletHandler@43204cf{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$1-553a331c==org.apache.spark.ui.JettyUtils$$anon$1@6a19de8e{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null,STOPPED},AUTO}\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: ServletHandler@43204cf{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$1-553a331c,POJO}\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: o.e.j.s.ServletContextHandler@1ba74b31{/,null,STOPPED} added {SymlinkAllowedResourceAliasChecker@6839e0cd{base=null,protected=null},MANAGED}\n",
      "23/10/03 12:21:08 DEBUG DecoratedObjectFactory: Adding Decorator: org.eclipse.jetty.util.DeprecationWarning@7c399579\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: o.e.j.s.ServletContextHandler@1ba74b31{/,null,STOPPED} added {ServletHandler@781ffb31{STOPPED},MANAGED}\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: ServletHandler@781ffb31{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$1-59c0c9c0==org.apache.spark.ui.JettyUtils$$anon$1@8cfed7e2{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null,STOPPED},AUTO}\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: ServletHandler@781ffb31{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$1-59c0c9c0,POJO}\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: ServletHandler@43204cf{STOPPED} added {org.apache.spark.ui.HttpSecurityFilter-7ed875bd==org.apache.spark.ui.HttpSecurityFilter@7ed875bd{inst=false,async=true,src=EMBEDDED:null},AUTO}\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: ServletHandler@43204cf{STOPPED} added {[/*]/[]/[REQUEST, ERROR, INCLUDE, FORWARD, ASYNC]=>org.apache.spark.ui.HttpSecurityFilter-7ed875bd,POJO}\n",
      "23/10/03 12:21:08 DEBUG ServletPathSpec: Creating ServletPathSpec[*.svgz] (group: SUFFIX_GLOB, prefix: \"null\", suffix: \"svgz\")\n",
      "23/10/03 12:21:08 DEBUG PathMappings: Added MappedResource[pathSpec=ServletPathSpec@9727ded2{*.svgz},resource=true] to PathMappings[size=1]\n",
      "23/10/03 12:21:08 DEBUG GzipHandler: GzipHandler@48ede2b3{STOPPED,min=32,inflate=-1} mime types IncludeExclude@4a4aa073{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, image/apng, audio/midi, image/webp, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/avif, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: GzipHandler@48ede2b3{STOPPED,min=32,inflate=-1} added {o.e.j.s.ServletContextHandler@34eca6ab{/SQL,null,STOPPED,@Spark},MANAGED}\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: ->[{GzipHandler@5d364122{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@380b3e1a{/,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: executors/heapHistogram/json->[{GzipHandler@3f897684{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@22d58ee0{/executors/heapHistogram/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: storage/rdd->[{GzipHandler@18d6f3bc{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@7f105e07{/storage/rdd,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: storage->[{GzipHandler@78091ed{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@513b76a9{/storage,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: storage/rdd/json->[{GzipHandler@6850ef6e{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@23e8c429{/storage/rdd/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: api->[{GzipHandler@1c1989b6{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@7264ad48{/api,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages/pool/json->[{GzipHandler@337dd29a{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@1466bd10{/stages/pool/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages/pool->[{GzipHandler@30f122e1{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@3656dfff{/stages/pool,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: jobs/json->[{GzipHandler@5f35800e{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@3d9e22a1{/jobs/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: static->[{GzipHandler@fb1e667{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@7cea766a{/static,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: executors/json->[{GzipHandler@52851c09{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@8aa2b98{/executors/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages/stage/json->[{GzipHandler@6b5ada92{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@298cc8d5{/stages/stage/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: executors/heapHistogram->[{GzipHandler@189c1207{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@38d1671{/executors/heapHistogram,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: executors/threadDump/json->[{GzipHandler@181e5dad{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@64cb19df{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: environment/json->[{GzipHandler@62017ff3{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@1e19efad{/environment/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: jobs/job/json->[{GzipHandler@5f32a76b{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@6900b20{/jobs/job/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: jobs->[{GzipHandler@27468aa0{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@32de47ea{/jobs,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages/json->[{GzipHandler@4ddf48d0{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@2e231eda{/stages/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages/stage->[{GzipHandler@24c8587f{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@77e698f{/stages/stage,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: storage/json->[{GzipHandler@6e0bba72{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@33529c71{/storage/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: SQL->[{GzipHandler@48ede2b3{STOPPED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@34eca6ab{/SQL,null,STOPPED,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages/stage/kill->[{GzipHandler@7d0b2e99{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@1eb45647{/stages/stage/kill,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: jobs/job->[{GzipHandler@4a257020{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@4942aebe{/jobs/job,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: environment->[{GzipHandler@2cc8c2bb{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@c32bd42{/environment,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages->[{GzipHandler@1f23357a{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@772fb9d8{/stages,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: executors->[{GzipHandler@3b78afe1{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@f571950{/executors,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: jobs/job/kill->[{GzipHandler@5a3d7cde{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@2b0bd31d{/jobs/job/kill,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: metrics/json->[{GzipHandler@308aa038{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@238deca4{/metrics/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: executors/threadDump->[{GzipHandler@3604dbb6{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@1ac0d892{/executors/threadDump,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: ContextHandlerCollection@48fd6d30{STARTED} added {GzipHandler@48ede2b3{STOPPED,min=32,inflate=-1},UNMANAGED}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler@34eca6ab{/SQL,null,STOPPED,@Spark}\n",
      "23/10/03 12:21:08 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler@34eca6ab{/SQL,null,STARTING,@Spark}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting SymlinkAllowedResourceAliasChecker@191af92b{base=null,protected=null}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6871ms SymlinkAllowedResourceAliasChecker@191af92b{base=null,protected=null}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting ServletHandler@43204cf{STOPPED}\n",
      "23/10/03 12:21:08 DEBUG ServletHandler: Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$1-553a331c[EMBEDDED:null]\n",
      "23/10/03 12:21:08 DEBUG PathMappings: Added MappedResource[pathSpec=ServletPathSpec@9457dcb7{/},resource=org.apache.spark.ui.JettyUtils$$anon$1-553a331c==org.apache.spark.ui.JettyUtils$$anon$1@6a19de8e{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null,STOPPED}] to PathMappings[size=1]\n",
      "23/10/03 12:21:08 DEBUG ServletHandler: filterNameMap={org.apache.spark.ui.HttpSecurityFilter-7ed875bd=org.apache.spark.ui.HttpSecurityFilter-7ed875bd==org.apache.spark.ui.HttpSecurityFilter@7ed875bd{inst=false,async=true,src=EMBEDDED:null}}\n",
      "23/10/03 12:21:08 DEBUG ServletHandler: pathFilters=[[/*]/[]/[REQUEST, ERROR, INCLUDE, FORWARD, ASYNC]=>org.apache.spark.ui.HttpSecurityFilter-7ed875bd]\n",
      "23/10/03 12:21:08 DEBUG ServletHandler: servletFilterMap={}\n",
      "23/10/03 12:21:08 DEBUG ServletHandler: servletPathMap=PathMappings[size=1]\n",
      "23/10/03 12:21:08 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$1-553a331c=org.apache.spark.ui.JettyUtils$$anon$1-553a331c==org.apache.spark.ui.JettyUtils$$anon$1@6a19de8e{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null,STOPPED}}\n",
      "23/10/03 12:21:08 DEBUG AbstractHandler: starting ServletHandler@43204cf{STARTING}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6873ms ServletHandler@43204cf{STARTED}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.HttpSecurityFilter-7ed875bd==org.apache.spark.ui.HttpSecurityFilter@7ed875bd{inst=false,async=true,src=EMBEDDED:null}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6873ms org.apache.spark.ui.HttpSecurityFilter-7ed875bd==org.apache.spark.ui.HttpSecurityFilter@7ed875bd{inst=false,async=true,src=EMBEDDED:null}\n",
      "23/10/03 12:21:08 DEBUG FilterHolder: Filter.init org.apache.spark.ui.HttpSecurityFilter@7917337d\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$1-553a331c==org.apache.spark.ui.JettyUtils$$anon$1@6a19de8e{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null,STOPPED}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6874ms org.apache.spark.ui.JettyUtils$$anon$1-553a331c==org.apache.spark.ui.JettyUtils$$anon$1@6a19de8e{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null,STARTED}\n",
      "23/10/03 12:21:08 DEBUG ServletHolder: Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$1-553a331c\n",
      "23/10/03 12:21:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@34eca6ab{/SQL,null,AVAILABLE,@Spark}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6875ms o.e.j.s.ServletContextHandler@34eca6ab{/SQL,null,AVAILABLE,@Spark}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting GzipHandler@48ede2b3{STOPPED,min=32,inflate=-1}\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: GzipHandler@48ede2b3{STARTING,min=32,inflate=-1} added {DeflaterPool@1c79d707{STOPPED,size=0,capacity=UNLIMITED},AUTO}\n",
      "23/10/03 12:21:08 DEBUG AbstractHandler: starting GzipHandler@48ede2b3{STARTING,min=32,inflate=-1}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting DeflaterPool@1c79d707{STOPPED,size=0,capacity=UNLIMITED}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6876ms DeflaterPool@1c79d707{STARTED,size=0,capacity=UNLIMITED}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6876ms GzipHandler@48ede2b3{STARTED,min=32,inflate=-1}\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: ServletHandler@781ffb31{STOPPED} added {org.apache.spark.ui.HttpSecurityFilter-c307655==org.apache.spark.ui.HttpSecurityFilter@c307655{inst=false,async=true,src=EMBEDDED:null},AUTO}\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: ServletHandler@781ffb31{STOPPED} added {[/*]/[]/[REQUEST, ERROR, INCLUDE, FORWARD, ASYNC]=>org.apache.spark.ui.HttpSecurityFilter-c307655,POJO}\n",
      "23/10/03 12:21:08 DEBUG ServletPathSpec: Creating ServletPathSpec[*.svgz] (group: SUFFIX_GLOB, prefix: \"null\", suffix: \"svgz\")\n",
      "23/10/03 12:21:08 DEBUG PathMappings: Added MappedResource[pathSpec=ServletPathSpec@9727ded2{*.svgz},resource=true] to PathMappings[size=1]\n",
      "23/10/03 12:21:08 DEBUG GzipHandler: GzipHandler@2f070e0e{STOPPED,min=32,inflate=-1} mime types IncludeExclude@6f4b4c48{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, image/apng, audio/midi, image/webp, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/avif, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: GzipHandler@2f070e0e{STOPPED,min=32,inflate=-1} added {o.e.j.s.ServletContextHandler@1ba74b31{/SQL/json,null,STOPPED,@Spark},MANAGED}\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: ->[{GzipHandler@5d364122{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@380b3e1a{/,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: executors/heapHistogram/json->[{GzipHandler@3f897684{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@22d58ee0{/executors/heapHistogram/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: storage/rdd->[{GzipHandler@18d6f3bc{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@7f105e07{/storage/rdd,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: storage->[{GzipHandler@78091ed{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@513b76a9{/storage,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: storage/rdd/json->[{GzipHandler@6850ef6e{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@23e8c429{/storage/rdd/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: api->[{GzipHandler@1c1989b6{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@7264ad48{/api,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages/pool/json->[{GzipHandler@337dd29a{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@1466bd10{/stages/pool/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages/pool->[{GzipHandler@30f122e1{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@3656dfff{/stages/pool,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: jobs/json->[{GzipHandler@5f35800e{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@3d9e22a1{/jobs/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: static->[{GzipHandler@fb1e667{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@7cea766a{/static,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: executors/json->[{GzipHandler@52851c09{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@8aa2b98{/executors/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages/stage/json->[{GzipHandler@6b5ada92{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@298cc8d5{/stages/stage/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: executors/heapHistogram->[{GzipHandler@189c1207{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@38d1671{/executors/heapHistogram,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: executors/threadDump/json->[{GzipHandler@181e5dad{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@64cb19df{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: environment/json->[{GzipHandler@62017ff3{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@1e19efad{/environment/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: jobs/job/json->[{GzipHandler@5f32a76b{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@6900b20{/jobs/job/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: jobs->[{GzipHandler@27468aa0{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@32de47ea{/jobs,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages/json->[{GzipHandler@4ddf48d0{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@2e231eda{/stages/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages/stage->[{GzipHandler@24c8587f{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@77e698f{/stages/stage,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: storage/json->[{GzipHandler@6e0bba72{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@33529c71{/storage/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: SQL->[{GzipHandler@48ede2b3{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@34eca6ab{/SQL,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages/stage/kill->[{GzipHandler@7d0b2e99{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@1eb45647{/stages/stage/kill,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: jobs/job->[{GzipHandler@4a257020{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@4942aebe{/jobs/job,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: environment->[{GzipHandler@2cc8c2bb{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@c32bd42{/environment,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages->[{GzipHandler@1f23357a{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@772fb9d8{/stages,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: executors->[{GzipHandler@3b78afe1{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@f571950{/executors,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: SQL/json->[{GzipHandler@2f070e0e{STOPPED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@1ba74b31{/SQL/json,null,STOPPED,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: jobs/job/kill->[{GzipHandler@5a3d7cde{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@2b0bd31d{/jobs/job/kill,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: metrics/json->[{GzipHandler@308aa038{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@238deca4{/metrics/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: executors/threadDump->[{GzipHandler@3604dbb6{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@1ac0d892{/executors/threadDump,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: ContextHandlerCollection@48fd6d30{STARTED} added {GzipHandler@2f070e0e{STOPPED,min=32,inflate=-1},UNMANAGED}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler@1ba74b31{/SQL/json,null,STOPPED,@Spark}\n",
      "23/10/03 12:21:08 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler@1ba74b31{/SQL/json,null,STARTING,@Spark}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting SymlinkAllowedResourceAliasChecker@6839e0cd{base=null,protected=null}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6884ms SymlinkAllowedResourceAliasChecker@6839e0cd{base=null,protected=null}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting ServletHandler@781ffb31{STOPPED}\n",
      "23/10/03 12:21:08 DEBUG ServletHandler: Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$1-59c0c9c0[EMBEDDED:null]\n",
      "23/10/03 12:21:08 DEBUG PathMappings: Added MappedResource[pathSpec=ServletPathSpec@9457dcb7{/},resource=org.apache.spark.ui.JettyUtils$$anon$1-59c0c9c0==org.apache.spark.ui.JettyUtils$$anon$1@8cfed7e2{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null,STOPPED}] to PathMappings[size=1]\n",
      "23/10/03 12:21:08 DEBUG ServletHandler: filterNameMap={org.apache.spark.ui.HttpSecurityFilter-c307655=org.apache.spark.ui.HttpSecurityFilter-c307655==org.apache.spark.ui.HttpSecurityFilter@c307655{inst=false,async=true,src=EMBEDDED:null}}\n",
      "23/10/03 12:21:08 DEBUG ServletHandler: pathFilters=[[/*]/[]/[REQUEST, ERROR, INCLUDE, FORWARD, ASYNC]=>org.apache.spark.ui.HttpSecurityFilter-c307655]\n",
      "23/10/03 12:21:08 DEBUG ServletHandler: servletFilterMap={}\n",
      "23/10/03 12:21:08 DEBUG ServletHandler: servletPathMap=PathMappings[size=1]\n",
      "23/10/03 12:21:08 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$1-59c0c9c0=org.apache.spark.ui.JettyUtils$$anon$1-59c0c9c0==org.apache.spark.ui.JettyUtils$$anon$1@8cfed7e2{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null,STOPPED}}\n",
      "23/10/03 12:21:08 DEBUG AbstractHandler: starting ServletHandler@781ffb31{STARTING}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6886ms ServletHandler@781ffb31{STARTED}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.HttpSecurityFilter-c307655==org.apache.spark.ui.HttpSecurityFilter@c307655{inst=false,async=true,src=EMBEDDED:null}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6887ms org.apache.spark.ui.HttpSecurityFilter-c307655==org.apache.spark.ui.HttpSecurityFilter@c307655{inst=false,async=true,src=EMBEDDED:null}\n",
      "23/10/03 12:21:08 DEBUG FilterHolder: Filter.init org.apache.spark.ui.HttpSecurityFilter@3dd86f49\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$1-59c0c9c0==org.apache.spark.ui.JettyUtils$$anon$1@8cfed7e2{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null,STOPPED}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6887ms org.apache.spark.ui.JettyUtils$$anon$1-59c0c9c0==org.apache.spark.ui.JettyUtils$$anon$1@8cfed7e2{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null,STARTED}\n",
      "23/10/03 12:21:08 DEBUG ServletHolder: Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$1-59c0c9c0\n",
      "23/10/03 12:21:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1ba74b31{/SQL/json,null,AVAILABLE,@Spark}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6888ms o.e.j.s.ServletContextHandler@1ba74b31{/SQL/json,null,AVAILABLE,@Spark}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting GzipHandler@2f070e0e{STOPPED,min=32,inflate=-1}\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: GzipHandler@2f070e0e{STARTING,min=32,inflate=-1} added {DeflaterPool@32d53479{STOPPED,size=0,capacity=UNLIMITED},AUTO}\n",
      "23/10/03 12:21:08 DEBUG AbstractHandler: starting GzipHandler@2f070e0e{STARTING,min=32,inflate=-1}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting DeflaterPool@32d53479{STOPPED,size=0,capacity=UNLIMITED}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6888ms DeflaterPool@32d53479{STARTED,size=0,capacity=UNLIMITED}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6889ms GzipHandler@2f070e0e{STARTED,min=32,inflate=-1}\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: o.e.j.s.ServletContextHandler@75181a91{/,null,STOPPED} added {SymlinkAllowedResourceAliasChecker@79fbd8d2{base=null,protected=null},MANAGED}\n",
      "23/10/03 12:21:08 DEBUG DecoratedObjectFactory: Adding Decorator: org.eclipse.jetty.util.DeprecationWarning@21df29de\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: o.e.j.s.ServletContextHandler@75181a91{/,null,STOPPED} added {ServletHandler@63866b62{STOPPED},MANAGED}\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: ServletHandler@63866b62{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$1-567f3911==org.apache.spark.ui.JettyUtils$$anon$1@a61a0824{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null,STOPPED},AUTO}\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: ServletHandler@63866b62{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$1-567f3911,POJO}\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: o.e.j.s.ServletContextHandler@71e54644{/,null,STOPPED} added {SymlinkAllowedResourceAliasChecker@29ac107f{base=null,protected=null},MANAGED}\n",
      "23/10/03 12:21:08 DEBUG DecoratedObjectFactory: Adding Decorator: org.eclipse.jetty.util.DeprecationWarning@7921b353\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: o.e.j.s.ServletContextHandler@71e54644{/,null,STOPPED} added {ServletHandler@5639bad7{STOPPED},MANAGED}\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: ServletHandler@5639bad7{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$1-d351132==org.apache.spark.ui.JettyUtils$$anon$1@b64db5d7{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null,STOPPED},AUTO}\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: ServletHandler@5639bad7{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$1-d351132,POJO}\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: ServletHandler@63866b62{STOPPED} added {org.apache.spark.ui.HttpSecurityFilter-4453d703==org.apache.spark.ui.HttpSecurityFilter@4453d703{inst=false,async=true,src=EMBEDDED:null},AUTO}\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: ServletHandler@63866b62{STOPPED} added {[/*]/[]/[REQUEST, ERROR, INCLUDE, FORWARD, ASYNC]=>org.apache.spark.ui.HttpSecurityFilter-4453d703,POJO}\n",
      "23/10/03 12:21:08 DEBUG ServletPathSpec: Creating ServletPathSpec[*.svgz] (group: SUFFIX_GLOB, prefix: \"null\", suffix: \"svgz\")\n",
      "23/10/03 12:21:08 DEBUG PathMappings: Added MappedResource[pathSpec=ServletPathSpec@9727ded2{*.svgz},resource=true] to PathMappings[size=1]\n",
      "23/10/03 12:21:08 DEBUG GzipHandler: GzipHandler@309c4ff1{STOPPED,min=32,inflate=-1} mime types IncludeExclude@2b68a437{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, image/apng, audio/midi, image/webp, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/avif, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: GzipHandler@309c4ff1{STOPPED,min=32,inflate=-1} added {o.e.j.s.ServletContextHandler@75181a91{/SQL/execution,null,STOPPED,@Spark},MANAGED}\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: ->[{GzipHandler@5d364122{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@380b3e1a{/,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: executors/heapHistogram/json->[{GzipHandler@3f897684{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@22d58ee0{/executors/heapHistogram/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: storage/rdd->[{GzipHandler@18d6f3bc{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@7f105e07{/storage/rdd,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: storage->[{GzipHandler@78091ed{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@513b76a9{/storage,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: storage/rdd/json->[{GzipHandler@6850ef6e{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@23e8c429{/storage/rdd/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: api->[{GzipHandler@1c1989b6{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@7264ad48{/api,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages/pool/json->[{GzipHandler@337dd29a{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@1466bd10{/stages/pool/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages/pool->[{GzipHandler@30f122e1{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@3656dfff{/stages/pool,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: jobs/json->[{GzipHandler@5f35800e{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@3d9e22a1{/jobs/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: static->[{GzipHandler@fb1e667{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@7cea766a{/static,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: executors/json->[{GzipHandler@52851c09{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@8aa2b98{/executors/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages/stage/json->[{GzipHandler@6b5ada92{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@298cc8d5{/stages/stage/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: executors/heapHistogram->[{GzipHandler@189c1207{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@38d1671{/executors/heapHistogram,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: executors/threadDump/json->[{GzipHandler@181e5dad{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@64cb19df{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: environment/json->[{GzipHandler@62017ff3{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@1e19efad{/environment/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: jobs/job/json->[{GzipHandler@5f32a76b{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@6900b20{/jobs/job/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: jobs->[{GzipHandler@27468aa0{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@32de47ea{/jobs,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages/json->[{GzipHandler@4ddf48d0{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@2e231eda{/stages/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages/stage->[{GzipHandler@24c8587f{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@77e698f{/stages/stage,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: storage/json->[{GzipHandler@6e0bba72{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@33529c71{/storage/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: SQL->[{GzipHandler@48ede2b3{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@34eca6ab{/SQL,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages/stage/kill->[{GzipHandler@7d0b2e99{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@1eb45647{/stages/stage/kill,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: jobs/job->[{GzipHandler@4a257020{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@4942aebe{/jobs/job,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: environment->[{GzipHandler@2cc8c2bb{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@c32bd42{/environment,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages->[{GzipHandler@1f23357a{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@772fb9d8{/stages,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: executors->[{GzipHandler@3b78afe1{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@f571950{/executors,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: SQL/json->[{GzipHandler@2f070e0e{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@1ba74b31{/SQL/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: jobs/job/kill->[{GzipHandler@5a3d7cde{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@2b0bd31d{/jobs/job/kill,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: metrics/json->[{GzipHandler@308aa038{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@238deca4{/metrics/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: SQL/execution->[{GzipHandler@309c4ff1{STOPPED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@75181a91{/SQL/execution,null,STOPPED,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: executors/threadDump->[{GzipHandler@3604dbb6{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@1ac0d892{/executors/threadDump,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: ContextHandlerCollection@48fd6d30{STARTED} added {GzipHandler@309c4ff1{STOPPED,min=32,inflate=-1},UNMANAGED}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler@75181a91{/SQL/execution,null,STOPPED,@Spark}\n",
      "23/10/03 12:21:08 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler@75181a91{/SQL/execution,null,STARTING,@Spark}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting SymlinkAllowedResourceAliasChecker@79fbd8d2{base=null,protected=null}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6898ms SymlinkAllowedResourceAliasChecker@79fbd8d2{base=null,protected=null}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting ServletHandler@63866b62{STOPPED}\n",
      "23/10/03 12:21:08 DEBUG ServletHandler: Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$1-567f3911[EMBEDDED:null]\n",
      "23/10/03 12:21:08 DEBUG PathMappings: Added MappedResource[pathSpec=ServletPathSpec@9457dcb7{/},resource=org.apache.spark.ui.JettyUtils$$anon$1-567f3911==org.apache.spark.ui.JettyUtils$$anon$1@a61a0824{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null,STOPPED}] to PathMappings[size=1]\n",
      "23/10/03 12:21:08 DEBUG ServletHandler: filterNameMap={org.apache.spark.ui.HttpSecurityFilter-4453d703=org.apache.spark.ui.HttpSecurityFilter-4453d703==org.apache.spark.ui.HttpSecurityFilter@4453d703{inst=false,async=true,src=EMBEDDED:null}}\n",
      "23/10/03 12:21:08 DEBUG ServletHandler: pathFilters=[[/*]/[]/[REQUEST, ERROR, INCLUDE, FORWARD, ASYNC]=>org.apache.spark.ui.HttpSecurityFilter-4453d703]\n",
      "23/10/03 12:21:08 DEBUG ServletHandler: servletFilterMap={}\n",
      "23/10/03 12:21:08 DEBUG ServletHandler: servletPathMap=PathMappings[size=1]\n",
      "23/10/03 12:21:08 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$1-567f3911=org.apache.spark.ui.JettyUtils$$anon$1-567f3911==org.apache.spark.ui.JettyUtils$$anon$1@a61a0824{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null,STOPPED}}\n",
      "23/10/03 12:21:08 DEBUG AbstractHandler: starting ServletHandler@63866b62{STARTING}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6899ms ServletHandler@63866b62{STARTED}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.HttpSecurityFilter-4453d703==org.apache.spark.ui.HttpSecurityFilter@4453d703{inst=false,async=true,src=EMBEDDED:null}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6899ms org.apache.spark.ui.HttpSecurityFilter-4453d703==org.apache.spark.ui.HttpSecurityFilter@4453d703{inst=false,async=true,src=EMBEDDED:null}\n",
      "23/10/03 12:21:08 DEBUG FilterHolder: Filter.init org.apache.spark.ui.HttpSecurityFilter@e719ce9\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$1-567f3911==org.apache.spark.ui.JettyUtils$$anon$1@a61a0824{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null,STOPPED}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6899ms org.apache.spark.ui.JettyUtils$$anon$1-567f3911==org.apache.spark.ui.JettyUtils$$anon$1@a61a0824{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null,STARTED}\n",
      "23/10/03 12:21:08 DEBUG ServletHolder: Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$1-567f3911\n",
      "23/10/03 12:21:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@75181a91{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6900ms o.e.j.s.ServletContextHandler@75181a91{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting GzipHandler@309c4ff1{STOPPED,min=32,inflate=-1}\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: GzipHandler@309c4ff1{STARTING,min=32,inflate=-1} added {DeflaterPool@5440a9eb{STOPPED,size=0,capacity=UNLIMITED},AUTO}\n",
      "23/10/03 12:21:08 DEBUG AbstractHandler: starting GzipHandler@309c4ff1{STARTING,min=32,inflate=-1}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting DeflaterPool@5440a9eb{STOPPED,size=0,capacity=UNLIMITED}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6900ms DeflaterPool@5440a9eb{STARTED,size=0,capacity=UNLIMITED}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6900ms GzipHandler@309c4ff1{STARTED,min=32,inflate=-1}\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: ServletHandler@5639bad7{STOPPED} added {org.apache.spark.ui.HttpSecurityFilter-6718755==org.apache.spark.ui.HttpSecurityFilter@6718755{inst=false,async=true,src=EMBEDDED:null},AUTO}\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: ServletHandler@5639bad7{STOPPED} added {[/*]/[]/[REQUEST, ERROR, INCLUDE, FORWARD, ASYNC]=>org.apache.spark.ui.HttpSecurityFilter-6718755,POJO}\n",
      "23/10/03 12:21:08 DEBUG ServletPathSpec: Creating ServletPathSpec[*.svgz] (group: SUFFIX_GLOB, prefix: \"null\", suffix: \"svgz\")\n",
      "23/10/03 12:21:08 DEBUG PathMappings: Added MappedResource[pathSpec=ServletPathSpec@9727ded2{*.svgz},resource=true] to PathMappings[size=1]\n",
      "23/10/03 12:21:08 DEBUG GzipHandler: GzipHandler@6856c2b0{STOPPED,min=32,inflate=-1} mime types IncludeExclude@6ac29d87{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, image/apng, audio/midi, image/webp, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/avif, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: GzipHandler@6856c2b0{STOPPED,min=32,inflate=-1} added {o.e.j.s.ServletContextHandler@71e54644{/SQL/execution/json,null,STOPPED,@Spark},MANAGED}\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: ->[{GzipHandler@5d364122{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@380b3e1a{/,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: executors/heapHistogram/json->[{GzipHandler@3f897684{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@22d58ee0{/executors/heapHistogram/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: storage/rdd->[{GzipHandler@18d6f3bc{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@7f105e07{/storage/rdd,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: storage->[{GzipHandler@78091ed{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@513b76a9{/storage,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: storage/rdd/json->[{GzipHandler@6850ef6e{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@23e8c429{/storage/rdd/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: SQL/execution/json->[{GzipHandler@6856c2b0{STOPPED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@71e54644{/SQL/execution/json,null,STOPPED,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: api->[{GzipHandler@1c1989b6{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@7264ad48{/api,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages/pool/json->[{GzipHandler@337dd29a{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@1466bd10{/stages/pool/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages/pool->[{GzipHandler@30f122e1{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@3656dfff{/stages/pool,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: jobs/json->[{GzipHandler@5f35800e{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@3d9e22a1{/jobs/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: static->[{GzipHandler@fb1e667{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@7cea766a{/static,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: executors/json->[{GzipHandler@52851c09{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@8aa2b98{/executors/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages/stage/json->[{GzipHandler@6b5ada92{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@298cc8d5{/stages/stage/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: executors/heapHistogram->[{GzipHandler@189c1207{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@38d1671{/executors/heapHistogram,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: executors/threadDump/json->[{GzipHandler@181e5dad{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@64cb19df{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: environment/json->[{GzipHandler@62017ff3{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@1e19efad{/environment/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: jobs/job/json->[{GzipHandler@5f32a76b{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@6900b20{/jobs/job/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: jobs->[{GzipHandler@27468aa0{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@32de47ea{/jobs,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages/json->[{GzipHandler@4ddf48d0{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@2e231eda{/stages/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages/stage->[{GzipHandler@24c8587f{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@77e698f{/stages/stage,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: storage/json->[{GzipHandler@6e0bba72{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@33529c71{/storage/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: SQL->[{GzipHandler@48ede2b3{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@34eca6ab{/SQL,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages/stage/kill->[{GzipHandler@7d0b2e99{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@1eb45647{/stages/stage/kill,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: jobs/job->[{GzipHandler@4a257020{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@4942aebe{/jobs/job,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: environment->[{GzipHandler@2cc8c2bb{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@c32bd42{/environment,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages->[{GzipHandler@1f23357a{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@772fb9d8{/stages,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: executors->[{GzipHandler@3b78afe1{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@f571950{/executors,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: SQL/json->[{GzipHandler@2f070e0e{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@1ba74b31{/SQL/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: jobs/job/kill->[{GzipHandler@5a3d7cde{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@2b0bd31d{/jobs/job/kill,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: metrics/json->[{GzipHandler@308aa038{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@238deca4{/metrics/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: SQL/execution->[{GzipHandler@309c4ff1{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@75181a91{/SQL/execution,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: executors/threadDump->[{GzipHandler@3604dbb6{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@1ac0d892{/executors/threadDump,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: ContextHandlerCollection@48fd6d30{STARTED} added {GzipHandler@6856c2b0{STOPPED,min=32,inflate=-1},UNMANAGED}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler@71e54644{/SQL/execution/json,null,STOPPED,@Spark}\n",
      "23/10/03 12:21:08 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler@71e54644{/SQL/execution/json,null,STARTING,@Spark}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting SymlinkAllowedResourceAliasChecker@29ac107f{base=null,protected=null}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6906ms SymlinkAllowedResourceAliasChecker@29ac107f{base=null,protected=null}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting ServletHandler@5639bad7{STOPPED}\n",
      "23/10/03 12:21:08 DEBUG ServletHandler: Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$1-d351132[EMBEDDED:null]\n",
      "23/10/03 12:21:08 DEBUG PathMappings: Added MappedResource[pathSpec=ServletPathSpec@9457dcb7{/},resource=org.apache.spark.ui.JettyUtils$$anon$1-d351132==org.apache.spark.ui.JettyUtils$$anon$1@b64db5d7{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null,STOPPED}] to PathMappings[size=1]\n",
      "23/10/03 12:21:08 DEBUG ServletHandler: filterNameMap={org.apache.spark.ui.HttpSecurityFilter-6718755=org.apache.spark.ui.HttpSecurityFilter-6718755==org.apache.spark.ui.HttpSecurityFilter@6718755{inst=false,async=true,src=EMBEDDED:null}}\n",
      "23/10/03 12:21:08 DEBUG ServletHandler: pathFilters=[[/*]/[]/[REQUEST, ERROR, INCLUDE, FORWARD, ASYNC]=>org.apache.spark.ui.HttpSecurityFilter-6718755]\n",
      "23/10/03 12:21:08 DEBUG ServletHandler: servletFilterMap={}\n",
      "23/10/03 12:21:08 DEBUG ServletHandler: servletPathMap=PathMappings[size=1]\n",
      "23/10/03 12:21:08 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$1-d351132=org.apache.spark.ui.JettyUtils$$anon$1-d351132==org.apache.spark.ui.JettyUtils$$anon$1@b64db5d7{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null,STOPPED}}\n",
      "23/10/03 12:21:08 DEBUG AbstractHandler: starting ServletHandler@5639bad7{STARTING}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6908ms ServletHandler@5639bad7{STARTED}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.HttpSecurityFilter-6718755==org.apache.spark.ui.HttpSecurityFilter@6718755{inst=false,async=true,src=EMBEDDED:null}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6908ms org.apache.spark.ui.HttpSecurityFilter-6718755==org.apache.spark.ui.HttpSecurityFilter@6718755{inst=false,async=true,src=EMBEDDED:null}\n",
      "23/10/03 12:21:08 DEBUG FilterHolder: Filter.init org.apache.spark.ui.HttpSecurityFilter@3b9c7ec5\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$1-d351132==org.apache.spark.ui.JettyUtils$$anon$1@b64db5d7{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null,STOPPED}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6908ms org.apache.spark.ui.JettyUtils$$anon$1-d351132==org.apache.spark.ui.JettyUtils$$anon$1@b64db5d7{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null,STARTED}\n",
      "23/10/03 12:21:08 DEBUG ServletHolder: Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$1-d351132\n",
      "23/10/03 12:21:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@71e54644{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6908ms o.e.j.s.ServletContextHandler@71e54644{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting GzipHandler@6856c2b0{STOPPED,min=32,inflate=-1}\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: GzipHandler@6856c2b0{STARTING,min=32,inflate=-1} added {DeflaterPool@31dd0dcc{STOPPED,size=0,capacity=UNLIMITED},AUTO}\n",
      "23/10/03 12:21:08 DEBUG AbstractHandler: starting GzipHandler@6856c2b0{STARTING,min=32,inflate=-1}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting DeflaterPool@31dd0dcc{STOPPED,size=0,capacity=UNLIMITED}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6909ms DeflaterPool@31dd0dcc{STARTED,size=0,capacity=UNLIMITED}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6909ms GzipHandler@6856c2b0{STARTED,min=32,inflate=-1}\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: o.e.j.s.ServletContextHandler@7f3745c5{/,null,STOPPED} added {SymlinkAllowedResourceAliasChecker@6ad45888{base=null,protected=null},MANAGED}\n",
      "23/10/03 12:21:08 DEBUG DecoratedObjectFactory: Adding Decorator: org.eclipse.jetty.util.DeprecationWarning@3639267e\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: o.e.j.s.ServletContextHandler@7f3745c5{/,null,STOPPED} added {ServletHandler@658dfa6f{STOPPED},MANAGED}\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: ServletHandler@658dfa6f{STOPPED} added {org.eclipse.jetty.servlet.DefaultServlet-20d18276==org.eclipse.jetty.servlet.DefaultServlet@e3528403{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null,STOPPED},AUTO}\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: ServletHandler@658dfa6f{STOPPED} added {[/]=>org.eclipse.jetty.servlet.DefaultServlet-20d18276,POJO}\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: ServletHandler@658dfa6f{STOPPED} added {org.apache.spark.ui.HttpSecurityFilter-68098efc==org.apache.spark.ui.HttpSecurityFilter@68098efc{inst=false,async=true,src=EMBEDDED:null},AUTO}\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: ServletHandler@658dfa6f{STOPPED} added {[/*]/[]/[REQUEST, ERROR, INCLUDE, FORWARD, ASYNC]=>org.apache.spark.ui.HttpSecurityFilter-68098efc,POJO}\n",
      "23/10/03 12:21:08 DEBUG ServletPathSpec: Creating ServletPathSpec[*.svgz] (group: SUFFIX_GLOB, prefix: \"null\", suffix: \"svgz\")\n",
      "23/10/03 12:21:08 DEBUG PathMappings: Added MappedResource[pathSpec=ServletPathSpec@9727ded2{*.svgz},resource=true] to PathMappings[size=1]\n",
      "23/10/03 12:21:08 DEBUG GzipHandler: GzipHandler@17fc4f55{STOPPED,min=32,inflate=-1} mime types IncludeExclude@1da89147{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, image/apng, audio/midi, image/webp, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/avif, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: GzipHandler@17fc4f55{STOPPED,min=32,inflate=-1} added {o.e.j.s.ServletContextHandler@7f3745c5{/static/sql,null,STOPPED,@Spark},MANAGED}\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: ->[{GzipHandler@5d364122{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@380b3e1a{/,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: executors/heapHistogram/json->[{GzipHandler@3f897684{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@22d58ee0{/executors/heapHistogram/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: storage/rdd->[{GzipHandler@18d6f3bc{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@7f105e07{/storage/rdd,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: storage->[{GzipHandler@78091ed{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@513b76a9{/storage,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: storage/rdd/json->[{GzipHandler@6850ef6e{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@23e8c429{/storage/rdd/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: SQL/execution/json->[{GzipHandler@6856c2b0{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@71e54644{/SQL/execution/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: api->[{GzipHandler@1c1989b6{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@7264ad48{/api,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages/pool/json->[{GzipHandler@337dd29a{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@1466bd10{/stages/pool/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages/pool->[{GzipHandler@30f122e1{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@3656dfff{/stages/pool,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: jobs/json->[{GzipHandler@5f35800e{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@3d9e22a1{/jobs/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: static->[{GzipHandler@fb1e667{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@7cea766a{/static,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: executors/json->[{GzipHandler@52851c09{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@8aa2b98{/executors/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages/stage/json->[{GzipHandler@6b5ada92{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@298cc8d5{/stages/stage/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: executors/heapHistogram->[{GzipHandler@189c1207{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@38d1671{/executors/heapHistogram,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: executors/threadDump/json->[{GzipHandler@181e5dad{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@64cb19df{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: environment/json->[{GzipHandler@62017ff3{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@1e19efad{/environment/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: jobs/job/json->[{GzipHandler@5f32a76b{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@6900b20{/jobs/job/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: jobs->[{GzipHandler@27468aa0{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@32de47ea{/jobs,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages/json->[{GzipHandler@4ddf48d0{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@2e231eda{/stages/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages/stage->[{GzipHandler@24c8587f{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@77e698f{/stages/stage,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: storage/json->[{GzipHandler@6e0bba72{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@33529c71{/storage/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: SQL->[{GzipHandler@48ede2b3{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@34eca6ab{/SQL,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: static/sql->[{GzipHandler@17fc4f55{STOPPED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@7f3745c5{/static/sql,null,STOPPED,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages/stage/kill->[{GzipHandler@7d0b2e99{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@1eb45647{/stages/stage/kill,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: jobs/job->[{GzipHandler@4a257020{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@4942aebe{/jobs/job,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: environment->[{GzipHandler@2cc8c2bb{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@c32bd42{/environment,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: stages->[{GzipHandler@1f23357a{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@772fb9d8{/stages,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: executors->[{GzipHandler@3b78afe1{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@f571950{/executors,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: SQL/json->[{GzipHandler@2f070e0e{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@1ba74b31{/SQL/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: jobs/job/kill->[{GzipHandler@5a3d7cde{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@2b0bd31d{/jobs/job/kill,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: metrics/json->[{GzipHandler@308aa038{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@238deca4{/metrics/json,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: SQL/execution->[{GzipHandler@309c4ff1{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@75181a91{/SQL/execution,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContextHandlerCollection: executors/threadDump->[{GzipHandler@3604dbb6{STARTED,min=32,inflate=-1},[o.e.j.s.ServletContextHandler@1ac0d892{/executors/threadDump,null,AVAILABLE,@Spark}]}]\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: ContextHandlerCollection@48fd6d30{STARTED} added {GzipHandler@17fc4f55{STOPPED,min=32,inflate=-1},UNMANAGED}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler@7f3745c5{/static/sql,null,STOPPED,@Spark}\n",
      "23/10/03 12:21:08 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler@7f3745c5{/static/sql,null,STARTING,@Spark}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting SymlinkAllowedResourceAliasChecker@6ad45888{base=null,protected=null}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6915ms SymlinkAllowedResourceAliasChecker@6ad45888{base=null,protected=null}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting ServletHandler@658dfa6f{STOPPED}\n",
      "23/10/03 12:21:08 DEBUG ServletHandler: Path=/[EMBEDDED:null] mapped to servlet=org.eclipse.jetty.servlet.DefaultServlet-20d18276[EMBEDDED:null]\n",
      "23/10/03 12:21:08 DEBUG PathMappings: Added MappedResource[pathSpec=ServletPathSpec@9457dcb7{/},resource=org.eclipse.jetty.servlet.DefaultServlet-20d18276==org.eclipse.jetty.servlet.DefaultServlet@e3528403{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null,STOPPED}] to PathMappings[size=1]\n",
      "23/10/03 12:21:08 DEBUG ServletHandler: filterNameMap={org.apache.spark.ui.HttpSecurityFilter-68098efc=org.apache.spark.ui.HttpSecurityFilter-68098efc==org.apache.spark.ui.HttpSecurityFilter@68098efc{inst=false,async=true,src=EMBEDDED:null}}\n",
      "23/10/03 12:21:08 DEBUG ServletHandler: pathFilters=[[/*]/[]/[REQUEST, ERROR, INCLUDE, FORWARD, ASYNC]=>org.apache.spark.ui.HttpSecurityFilter-68098efc]\n",
      "23/10/03 12:21:08 DEBUG ServletHandler: servletFilterMap={}\n",
      "23/10/03 12:21:08 DEBUG ServletHandler: servletPathMap=PathMappings[size=1]\n",
      "23/10/03 12:21:08 DEBUG ServletHandler: servletNameMap={org.eclipse.jetty.servlet.DefaultServlet-20d18276=org.eclipse.jetty.servlet.DefaultServlet-20d18276==org.eclipse.jetty.servlet.DefaultServlet@e3528403{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null,STOPPED}}\n",
      "23/10/03 12:21:08 DEBUG AbstractHandler: starting ServletHandler@658dfa6f{STARTING}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6916ms ServletHandler@658dfa6f{STARTED}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.HttpSecurityFilter-68098efc==org.apache.spark.ui.HttpSecurityFilter@68098efc{inst=false,async=true,src=EMBEDDED:null}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6916ms org.apache.spark.ui.HttpSecurityFilter-68098efc==org.apache.spark.ui.HttpSecurityFilter@68098efc{inst=false,async=true,src=EMBEDDED:null}\n",
      "23/10/03 12:21:08 DEBUG FilterHolder: Filter.init org.apache.spark.ui.HttpSecurityFilter@31c662b6\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.servlet.DefaultServlet-20d18276==org.eclipse.jetty.servlet.DefaultServlet@e3528403{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null,STOPPED}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6916ms org.eclipse.jetty.servlet.DefaultServlet-20d18276==org.eclipse.jetty.servlet.DefaultServlet@e3528403{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null,STARTED}\n",
      "23/10/03 12:21:08 DEBUG ServletHolder: Servlet.init null for org.eclipse.jetty.servlet.DefaultServlet-20d18276\n",
      "23/10/03 12:21:08 DEBUG FsUrlStreamHandlerFactory: Creating handler for protocol jar\n",
      "23/10/03 12:21:08 DEBUG FileSystem: Looking for FS supporting jar\n",
      "23/10/03 12:21:08 DEBUG FileSystem: looking for configuration option fs.jar.impl\n",
      "23/10/03 12:21:08 DEBUG FileSystem: Looking in service filesystems for implementation class\n",
      "23/10/03 12:21:08 DEBUG FsUrlStreamHandlerFactory: Unknown protocol jar, delegating to default implementation\n",
      "23/10/03 12:21:08 DEBUG DefaultServlet: resource base = jar:file:/home/as/.local/lib/python3.10/site-packages/pyspark/jars/spark-sql_2.12-3.5.0-SNAPSHOT.jar!/org/apache/spark/sql/execution/ui/static\n",
      "23/10/03 12:21:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7f3745c5{/static/sql,null,AVAILABLE,@Spark}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6917ms o.e.j.s.ServletContextHandler@7f3745c5{/static/sql,null,AVAILABLE,@Spark}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting GzipHandler@17fc4f55{STOPPED,min=32,inflate=-1}\n",
      "23/10/03 12:21:08 DEBUG ContainerLifeCycle: GzipHandler@17fc4f55{STARTING,min=32,inflate=-1} added {DeflaterPool@606f070d{STOPPED,size=0,capacity=UNLIMITED},AUTO}\n",
      "23/10/03 12:21:08 DEBUG AbstractHandler: starting GzipHandler@17fc4f55{STARTING,min=32,inflate=-1}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: starting DeflaterPool@606f070d{STOPPED,size=0,capacity=UNLIMITED}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6918ms DeflaterPool@606f070d{STARTED,size=0,capacity=UNLIMITED}\n",
      "23/10/03 12:21:08 DEBUG AbstractLifeCycle: STARTED @6918ms GzipHandler@17fc4f55{STARTED,min=32,inflate=-1}\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"app\") \\\n",
    "        .config(\"spark.driver.memory\", \"45g\") \\\n",
    "        .config(\"spark.executor.memory\", \"45g\") \\\n",
    "        .config(\"spark.jars\", \"postgresql-42.3.3.jar\") \\\n",
    "        .getOrCreate()\n",
    "#spark.sparkContext.setLogLevel(\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06d1e816",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"WARN\") # ALL, DEBUG, WARN,\n",
    "#spark.sparkContext.setLogLevel(\"ALL\") # ALL, DEBUG, WARN,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acbbca1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/03 12:21:14 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:14 DEBUG ConnectionProvider: Loaded built-in provider: org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider@db3cdfd\n",
      "23/10/03 12:21:14 DEBUG ConnectionProvider: Loaded built-in provider: org.apache.spark.sql.execution.datasources.jdbc.connection.DB2ConnectionProvider@35d17bfd\n",
      "23/10/03 12:21:14 DEBUG ConnectionProvider: Loaded built-in provider: org.apache.spark.sql.execution.datasources.jdbc.connection.MariaDBConnectionProvider@4f1db444\n",
      "23/10/03 12:21:14 DEBUG ConnectionProvider: Loaded built-in provider: org.apache.spark.sql.execution.datasources.jdbc.connection.MSSQLConnectionProvider@38a45114\n",
      "23/10/03 12:21:14 DEBUG ConnectionProvider: Loaded built-in provider: org.apache.spark.sql.execution.datasources.jdbc.connection.PostgresConnectionProvider@360bc040\n",
      "23/10/03 12:21:14 DEBUG ConnectionProvider: Loaded built-in provider: org.apache.spark.sql.execution.datasources.jdbc.connection.OracleConnectionProvider@44e1498d\n",
      "23/10/03 12:21:14 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:14 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:14 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:14 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:14 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:14 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:14 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:14 DEBUG BasicConnectionProvider: JDBC connection initiated with URL: jdbc:postgresql://localhost:5432/tpch and properties: {password=tpch, user=tpch}\n",
      "23/10/03 12:21:14 DEBUG FsUrlStreamHandlerFactory: Creating handler for protocol http\n",
      "23/10/03 12:21:14 DEBUG FsUrlStreamHandlerFactory: Unknown protocol http, delegating to default implementation\n",
      "23/10/03 12:21:14 DEBUG CatalystSqlParser: Parsing command: spark_grouping_id\n",
      "23/10/03 12:21:14 DEBUG CatalystSqlParser: Parsing command: varchar(2147483647)\n",
      "23/10/03 12:21:14 DEBUG CatalystSqlParser: Parsing command: varchar(2147483647)\n",
      "23/10/03 12:21:14 DEBUG CatalystSqlParser: Parsing command: varchar(2147483647)\n",
      "23/10/03 12:21:14 DEBUG CatalystSqlParser: Parsing command: varchar(2147483647)\n",
      "23/10/03 12:21:14 DEBUG CatalystSqlParser: Parsing command: varchar(2147483647)\n",
      "23/10/03 12:21:14 DEBUG CatalystSqlParser: Parsing command: varchar(2147483647)\n",
      "23/10/03 12:21:14 DEBUG CatalystSqlParser: Parsing command: varchar(2147483647)\n",
      "23/10/03 12:21:14 DEBUG CatalystSqlParser: Parsing command: varchar(2147483647)\n",
      "23/10/03 12:21:14 DEBUG CatalystSqlParser: Parsing command: varchar(2147483647)\n",
      "23/10/03 12:21:14 DEBUG CatalystSqlParser: Parsing command: varchar(3)\n",
      "23/10/03 12:21:14 DEBUG CatalystSqlParser: Parsing command: varchar(3)\n",
      "23/10/03 12:21:14 DEBUG CatalystSqlParser: Parsing command: varchar(2147483647)\n",
      "23/10/03 12:21:15 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator scan_input_0;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] scan_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 011 */\n",
      "/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 013 */     this.references = references;\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 017 */     partitionIndex = index;\n",
      "/* 018 */     this.inputs = inputs;\n",
      "/* 019 */     scan_input_0 = inputs[0];\n",
      "/* 020 */     scan_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(12, 384);\n",
      "/* 021 */\n",
      "/* 022 */   }\n",
      "/* 023 */\n",
      "/* 024 */   protected void processNext() throws java.io.IOException {\n",
      "/* 025 */     while ( scan_input_0.hasNext()) {\n",
      "/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_input_0.next();\n",
      "/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 028 */       boolean scan_isNull_0 = scan_row_0.isNullAt(0);\n",
      "/* 029 */       UTF8String scan_value_0 = scan_isNull_0 ?\n",
      "/* 030 */       null : (scan_row_0.getUTF8String(0));\n",
      "/* 031 */       boolean scan_isNull_1 = scan_row_0.isNullAt(1);\n",
      "/* 032 */       UTF8String scan_value_1 = scan_isNull_1 ?\n",
      "/* 033 */       null : (scan_row_0.getUTF8String(1));\n",
      "/* 034 */       boolean scan_isNull_2 = scan_row_0.isNullAt(2);\n",
      "/* 035 */       UTF8String scan_value_2 = scan_isNull_2 ?\n",
      "/* 036 */       null : (scan_row_0.getUTF8String(2));\n",
      "/* 037 */       boolean scan_isNull_3 = scan_row_0.isNullAt(3);\n",
      "/* 038 */       UTF8String scan_value_3 = scan_isNull_3 ?\n",
      "/* 039 */       null : (scan_row_0.getUTF8String(3));\n",
      "/* 040 */       boolean scan_isNull_4 = scan_row_0.isNullAt(4);\n",
      "/* 041 */       UTF8String scan_value_4 = scan_isNull_4 ?\n",
      "/* 042 */       null : (scan_row_0.getUTF8String(4));\n",
      "/* 043 */       boolean scan_isNull_5 = scan_row_0.isNullAt(5);\n",
      "/* 044 */       UTF8String scan_value_5 = scan_isNull_5 ?\n",
      "/* 045 */       null : (scan_row_0.getUTF8String(5));\n",
      "/* 046 */       boolean scan_isNull_6 = scan_row_0.isNullAt(6);\n",
      "/* 047 */       UTF8String scan_value_6 = scan_isNull_6 ?\n",
      "/* 048 */       null : (scan_row_0.getUTF8String(6));\n",
      "/* 049 */       boolean scan_isNull_7 = scan_row_0.isNullAt(7);\n",
      "/* 050 */       UTF8String scan_value_7 = scan_isNull_7 ?\n",
      "/* 051 */       null : (scan_row_0.getUTF8String(7));\n",
      "/* 052 */       boolean scan_isNull_8 = scan_row_0.isNullAt(8);\n",
      "/* 053 */       UTF8String scan_value_8 = scan_isNull_8 ?\n",
      "/* 054 */       null : (scan_row_0.getUTF8String(8));\n",
      "/* 055 */       boolean scan_isNull_9 = scan_row_0.isNullAt(9);\n",
      "/* 056 */       UTF8String scan_value_9 = scan_isNull_9 ?\n",
      "/* 057 */       null : (scan_row_0.getUTF8String(9));\n",
      "/* 058 */       boolean scan_isNull_10 = scan_row_0.isNullAt(10);\n",
      "/* 059 */       UTF8String scan_value_10 = scan_isNull_10 ?\n",
      "/* 060 */       null : (scan_row_0.getUTF8String(10));\n",
      "/* 061 */       boolean scan_isNull_11 = scan_row_0.isNullAt(11);\n",
      "/* 062 */       UTF8String scan_value_11 = scan_isNull_11 ?\n",
      "/* 063 */       null : (scan_row_0.getUTF8String(11));\n",
      "/* 064 */       scan_mutableStateArray_0[0].reset();\n",
      "/* 065 */\n",
      "/* 066 */       scan_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 067 */\n",
      "/* 068 */       if (scan_isNull_0) {\n",
      "/* 069 */         scan_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 070 */       } else {\n",
      "/* 071 */         scan_mutableStateArray_0[0].write(0, scan_value_0);\n",
      "/* 072 */       }\n",
      "/* 073 */\n",
      "/* 074 */       if (scan_isNull_1) {\n",
      "/* 075 */         scan_mutableStateArray_0[0].setNullAt(1);\n",
      "/* 076 */       } else {\n",
      "/* 077 */         scan_mutableStateArray_0[0].write(1, scan_value_1);\n",
      "/* 078 */       }\n",
      "/* 079 */\n",
      "/* 080 */       if (scan_isNull_2) {\n",
      "/* 081 */         scan_mutableStateArray_0[0].setNullAt(2);\n",
      "/* 082 */       } else {\n",
      "/* 083 */         scan_mutableStateArray_0[0].write(2, scan_value_2);\n",
      "/* 084 */       }\n",
      "/* 085 */\n",
      "/* 086 */       if (scan_isNull_3) {\n",
      "/* 087 */         scan_mutableStateArray_0[0].setNullAt(3);\n",
      "/* 088 */       } else {\n",
      "/* 089 */         scan_mutableStateArray_0[0].write(3, scan_value_3);\n",
      "/* 090 */       }\n",
      "/* 091 */\n",
      "/* 092 */       if (scan_isNull_4) {\n",
      "/* 093 */         scan_mutableStateArray_0[0].setNullAt(4);\n",
      "/* 094 */       } else {\n",
      "/* 095 */         scan_mutableStateArray_0[0].write(4, scan_value_4);\n",
      "/* 096 */       }\n",
      "/* 097 */\n",
      "/* 098 */       if (scan_isNull_5) {\n",
      "/* 099 */         scan_mutableStateArray_0[0].setNullAt(5);\n",
      "/* 100 */       } else {\n",
      "/* 101 */         scan_mutableStateArray_0[0].write(5, scan_value_5);\n",
      "/* 102 */       }\n",
      "/* 103 */\n",
      "/* 104 */       if (scan_isNull_6) {\n",
      "/* 105 */         scan_mutableStateArray_0[0].setNullAt(6);\n",
      "/* 106 */       } else {\n",
      "/* 107 */         scan_mutableStateArray_0[0].write(6, scan_value_6);\n",
      "/* 108 */       }\n",
      "/* 109 */\n",
      "/* 110 */       if (scan_isNull_7) {\n",
      "/* 111 */         scan_mutableStateArray_0[0].setNullAt(7);\n",
      "/* 112 */       } else {\n",
      "/* 113 */         scan_mutableStateArray_0[0].write(7, scan_value_7);\n",
      "/* 114 */       }\n",
      "/* 115 */\n",
      "/* 116 */       if (scan_isNull_8) {\n",
      "/* 117 */         scan_mutableStateArray_0[0].setNullAt(8);\n",
      "/* 118 */       } else {\n",
      "/* 119 */         scan_mutableStateArray_0[0].write(8, scan_value_8);\n",
      "/* 120 */       }\n",
      "/* 121 */\n",
      "/* 122 */       if (scan_isNull_9) {\n",
      "/* 123 */         scan_mutableStateArray_0[0].setNullAt(9);\n",
      "/* 124 */       } else {\n",
      "/* 125 */         scan_mutableStateArray_0[0].write(9, scan_value_9);\n",
      "/* 126 */       }\n",
      "/* 127 */\n",
      "/* 128 */       if (scan_isNull_10) {\n",
      "/* 129 */         scan_mutableStateArray_0[0].setNullAt(10);\n",
      "/* 130 */       } else {\n",
      "/* 131 */         scan_mutableStateArray_0[0].write(10, scan_value_10);\n",
      "/* 132 */       }\n",
      "/* 133 */\n",
      "/* 134 */       if (scan_isNull_11) {\n",
      "/* 135 */         scan_mutableStateArray_0[0].setNullAt(11);\n",
      "/* 136 */       } else {\n",
      "/* 137 */         scan_mutableStateArray_0[0].write(11, scan_value_11);\n",
      "/* 138 */       }\n",
      "/* 139 */       append((scan_mutableStateArray_0[0].getRow()));\n",
      "/* 140 */       if (shouldStop()) return;\n",
      "/* 141 */     }\n",
      "/* 142 */   }\n",
      "/* 143 */\n",
      "/* 144 */ }\n",
      "\n",
      "23/10/03 12:21:15 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator scan_input_0;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] scan_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 011 */\n",
      "/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 013 */     this.references = references;\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 017 */     partitionIndex = index;\n",
      "/* 018 */     this.inputs = inputs;\n",
      "/* 019 */     scan_input_0 = inputs[0];\n",
      "/* 020 */     scan_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(12, 384);\n",
      "/* 021 */\n",
      "/* 022 */   }\n",
      "/* 023 */\n",
      "/* 024 */   protected void processNext() throws java.io.IOException {\n",
      "/* 025 */     while ( scan_input_0.hasNext()) {\n",
      "/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_input_0.next();\n",
      "/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 028 */       boolean scan_isNull_0 = scan_row_0.isNullAt(0);\n",
      "/* 029 */       UTF8String scan_value_0 = scan_isNull_0 ?\n",
      "/* 030 */       null : (scan_row_0.getUTF8String(0));\n",
      "/* 031 */       boolean scan_isNull_1 = scan_row_0.isNullAt(1);\n",
      "/* 032 */       UTF8String scan_value_1 = scan_isNull_1 ?\n",
      "/* 033 */       null : (scan_row_0.getUTF8String(1));\n",
      "/* 034 */       boolean scan_isNull_2 = scan_row_0.isNullAt(2);\n",
      "/* 035 */       UTF8String scan_value_2 = scan_isNull_2 ?\n",
      "/* 036 */       null : (scan_row_0.getUTF8String(2));\n",
      "/* 037 */       boolean scan_isNull_3 = scan_row_0.isNullAt(3);\n",
      "/* 038 */       UTF8String scan_value_3 = scan_isNull_3 ?\n",
      "/* 039 */       null : (scan_row_0.getUTF8String(3));\n",
      "/* 040 */       boolean scan_isNull_4 = scan_row_0.isNullAt(4);\n",
      "/* 041 */       UTF8String scan_value_4 = scan_isNull_4 ?\n",
      "/* 042 */       null : (scan_row_0.getUTF8String(4));\n",
      "/* 043 */       boolean scan_isNull_5 = scan_row_0.isNullAt(5);\n",
      "/* 044 */       UTF8String scan_value_5 = scan_isNull_5 ?\n",
      "/* 045 */       null : (scan_row_0.getUTF8String(5));\n",
      "/* 046 */       boolean scan_isNull_6 = scan_row_0.isNullAt(6);\n",
      "/* 047 */       UTF8String scan_value_6 = scan_isNull_6 ?\n",
      "/* 048 */       null : (scan_row_0.getUTF8String(6));\n",
      "/* 049 */       boolean scan_isNull_7 = scan_row_0.isNullAt(7);\n",
      "/* 050 */       UTF8String scan_value_7 = scan_isNull_7 ?\n",
      "/* 051 */       null : (scan_row_0.getUTF8String(7));\n",
      "/* 052 */       boolean scan_isNull_8 = scan_row_0.isNullAt(8);\n",
      "/* 053 */       UTF8String scan_value_8 = scan_isNull_8 ?\n",
      "/* 054 */       null : (scan_row_0.getUTF8String(8));\n",
      "/* 055 */       boolean scan_isNull_9 = scan_row_0.isNullAt(9);\n",
      "/* 056 */       UTF8String scan_value_9 = scan_isNull_9 ?\n",
      "/* 057 */       null : (scan_row_0.getUTF8String(9));\n",
      "/* 058 */       boolean scan_isNull_10 = scan_row_0.isNullAt(10);\n",
      "/* 059 */       UTF8String scan_value_10 = scan_isNull_10 ?\n",
      "/* 060 */       null : (scan_row_0.getUTF8String(10));\n",
      "/* 061 */       boolean scan_isNull_11 = scan_row_0.isNullAt(11);\n",
      "/* 062 */       UTF8String scan_value_11 = scan_isNull_11 ?\n",
      "/* 063 */       null : (scan_row_0.getUTF8String(11));\n",
      "/* 064 */       scan_mutableStateArray_0[0].reset();\n",
      "/* 065 */\n",
      "/* 066 */       scan_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 067 */\n",
      "/* 068 */       if (scan_isNull_0) {\n",
      "/* 069 */         scan_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 070 */       } else {\n",
      "/* 071 */         scan_mutableStateArray_0[0].write(0, scan_value_0);\n",
      "/* 072 */       }\n",
      "/* 073 */\n",
      "/* 074 */       if (scan_isNull_1) {\n",
      "/* 075 */         scan_mutableStateArray_0[0].setNullAt(1);\n",
      "/* 076 */       } else {\n",
      "/* 077 */         scan_mutableStateArray_0[0].write(1, scan_value_1);\n",
      "/* 078 */       }\n",
      "/* 079 */\n",
      "/* 080 */       if (scan_isNull_2) {\n",
      "/* 081 */         scan_mutableStateArray_0[0].setNullAt(2);\n",
      "/* 082 */       } else {\n",
      "/* 083 */         scan_mutableStateArray_0[0].write(2, scan_value_2);\n",
      "/* 084 */       }\n",
      "/* 085 */\n",
      "/* 086 */       if (scan_isNull_3) {\n",
      "/* 087 */         scan_mutableStateArray_0[0].setNullAt(3);\n",
      "/* 088 */       } else {\n",
      "/* 089 */         scan_mutableStateArray_0[0].write(3, scan_value_3);\n",
      "/* 090 */       }\n",
      "/* 091 */\n",
      "/* 092 */       if (scan_isNull_4) {\n",
      "/* 093 */         scan_mutableStateArray_0[0].setNullAt(4);\n",
      "/* 094 */       } else {\n",
      "/* 095 */         scan_mutableStateArray_0[0].write(4, scan_value_4);\n",
      "/* 096 */       }\n",
      "/* 097 */\n",
      "/* 098 */       if (scan_isNull_5) {\n",
      "/* 099 */         scan_mutableStateArray_0[0].setNullAt(5);\n",
      "/* 100 */       } else {\n",
      "/* 101 */         scan_mutableStateArray_0[0].write(5, scan_value_5);\n",
      "/* 102 */       }\n",
      "/* 103 */\n",
      "/* 104 */       if (scan_isNull_6) {\n",
      "/* 105 */         scan_mutableStateArray_0[0].setNullAt(6);\n",
      "/* 106 */       } else {\n",
      "/* 107 */         scan_mutableStateArray_0[0].write(6, scan_value_6);\n",
      "/* 108 */       }\n",
      "/* 109 */\n",
      "/* 110 */       if (scan_isNull_7) {\n",
      "/* 111 */         scan_mutableStateArray_0[0].setNullAt(7);\n",
      "/* 112 */       } else {\n",
      "/* 113 */         scan_mutableStateArray_0[0].write(7, scan_value_7);\n",
      "/* 114 */       }\n",
      "/* 115 */\n",
      "/* 116 */       if (scan_isNull_8) {\n",
      "/* 117 */         scan_mutableStateArray_0[0].setNullAt(8);\n",
      "/* 118 */       } else {\n",
      "/* 119 */         scan_mutableStateArray_0[0].write(8, scan_value_8);\n",
      "/* 120 */       }\n",
      "/* 121 */\n",
      "/* 122 */       if (scan_isNull_9) {\n",
      "/* 123 */         scan_mutableStateArray_0[0].setNullAt(9);\n",
      "/* 124 */       } else {\n",
      "/* 125 */         scan_mutableStateArray_0[0].write(9, scan_value_9);\n",
      "/* 126 */       }\n",
      "/* 127 */\n",
      "/* 128 */       if (scan_isNull_10) {\n",
      "/* 129 */         scan_mutableStateArray_0[0].setNullAt(10);\n",
      "/* 130 */       } else {\n",
      "/* 131 */         scan_mutableStateArray_0[0].write(10, scan_value_10);\n",
      "/* 132 */       }\n",
      "/* 133 */\n",
      "/* 134 */       if (scan_isNull_11) {\n",
      "/* 135 */         scan_mutableStateArray_0[0].setNullAt(11);\n",
      "/* 136 */       } else {\n",
      "/* 137 */         scan_mutableStateArray_0[0].write(11, scan_value_11);\n",
      "/* 138 */       }\n",
      "/* 139 */       append((scan_mutableStateArray_0[0].getRow()));\n",
      "/* 140 */       if (shouldStop()) return;\n",
      "/* 141 */     }\n",
      "/* 142 */   }\n",
      "/* 143 */\n",
      "/* 144 */ }\n",
      "\n",
      "23/10/03 12:21:15 INFO CodeGenerator: Code generated in 102.49155 ms\n",
      "23/10/03 12:21:15 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "23/10/03 12:21:15 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "23/10/03 12:21:15 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$collect$2\n",
      "23/10/03 12:21:15 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$collect$2) is now cleaned +++\n",
      "23/10/03 12:21:15 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "23/10/03 12:21:15 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "23/10/03 12:21:15 INFO SparkContext: Starting job: toPandas at /tmp/ipykernel_3934097/2514603901.py:14\n",
      "23/10/03 12:21:15 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 2 took 0.000567 seconds\n",
      "23/10/03 12:21:15 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/03 12:21:15 INFO DAGScheduler: Got job 0 (toPandas at /tmp/ipykernel_3934097/2514603901.py:14) with 1 output partitions\n",
      "23/10/03 12:21:15 INFO DAGScheduler: Final stage: ResultStage 0 (toPandas at /tmp/ipykernel_3934097/2514603901.py:14)\n",
      "23/10/03 12:21:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/10/03 12:21:15 INFO DAGScheduler: Missing parents: List()\n",
      "23/10/03 12:21:15 DEBUG DAGScheduler: submitStage(ResultStage 0 (name=toPandas at /tmp/ipykernel_3934097/2514603901.py:14;jobs=0))\n",
      "23/10/03 12:21:15 DEBUG DAGScheduler: missing: List()\n",
      "23/10/03 12:21:15 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at toPandas at /tmp/ipykernel_3934097/2514603901.py:14), which has no missing parents\n",
      "23/10/03 12:21:15 DEBUG DAGScheduler: submitMissingTasks(ResultStage 0)\n",
      "23/10/03 12:21:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 14.7 KiB, free 26.8 GiB)\n",
      "23/10/03 12:21:15 DEBUG BlockManager: Put block broadcast_0 locally took 23 ms\n",
      "23/10/03 12:21:15 DEBUG BlockManager: Putting block broadcast_0 without replication took 24 ms\n",
      "23/10/03 12:21:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 26.8 GiB)\n",
      "23/10/03 12:21:15 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, asusbc-rtl8117.lan, 34663, None)\n",
      "23/10/03 12:21:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on asusbc-rtl8117.lan:34663 (size: 6.5 KiB, free: 26.8 GiB)\n",
      "23/10/03 12:21:15 DEBUG BlockManagerMaster: Updated info of block broadcast_0_piece0\n",
      "23/10/03 12:21:15 DEBUG BlockManager: Told master about block broadcast_0_piece0\n",
      "23/10/03 12:21:15 DEBUG BlockManager: Put block broadcast_0_piece0 locally took 5 ms\n",
      "23/10/03 12:21:15 DEBUG BlockManager: Putting block broadcast_0_piece0 without replication took 5 ms\n",
      "23/10/03 12:21:15 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580\n",
      "23/10/03 12:21:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at toPandas at /tmp/ipykernel_3934097/2514603901.py:14) (first 15 tasks are for partitions Vector(0))\n",
      "23/10/03 12:21:15 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "23/10/03 12:21:15 DEBUG TaskSetManager: Epoch for TaskSet 0.0: 0\n",
      "23/10/03 12:21:15 DEBUG TaskSetManager: Adding pending tasks took 1 ms\n",
      "23/10/03 12:21:15 DEBUG TaskSetManager: Valid locality levels for TaskSet 0.0: NO_PREF, ANY\n",
      "23/10/03 12:21:15 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0.0, runningTasks: 0\n",
      "23/10/03 12:21:15 DEBUG TaskSetManager: Valid locality levels for TaskSet 0.0: NO_PREF, ANY\n",
      "23/10/03 12:21:15 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (asusbc-rtl8117.lan, executor driver, partition 0, PROCESS_LOCAL, 7647 bytes) \n",
      "23/10/03 12:21:15 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "23/10/03 12:21:15 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "23/10/03 12:21:15 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 1\n",
      "23/10/03 12:21:15 DEBUG BlockManager: Getting local block broadcast_0\n",
      "23/10/03 12:21:15 DEBUG BlockManager: Level for block broadcast_0 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "23/10/03 12:21:15 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:15 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:15 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:15 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:15 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:15 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:15 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:15 DEBUG BasicConnectionProvider: JDBC connection initiated with URL: jdbc:postgresql://localhost:5432/tpch and properties: {password=tpch, user=tpch}\n",
      "23/10/03 12:21:15 INFO JDBCRDD: closed connection\n",
      "23/10/03 12:21:15 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 6198 bytes result sent to driver\n",
      "23/10/03 12:21:15 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 0\n",
      "23/10/03 12:21:15 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 126 ms on asusbc-rtl8117.lan (executor driver) (1/1)\n",
      "23/10/03 12:21:15 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "23/10/03 12:21:15 INFO DAGScheduler: ResultStage 0 (toPandas at /tmp/ipykernel_3934097/2514603901.py:14) finished in 0.223 s\n",
      "23/10/03 12:21:15 DEBUG DAGScheduler: After removal of stage 0, remaining stages = 0\n",
      "23/10/03 12:21:15 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/10/03 12:21:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "23/10/03 12:21:15 INFO DAGScheduler: Job 0 finished: toPandas at /tmp/ipykernel_3934097/2514603901.py:14, took 0.252591 s\n",
      "23/10/03 12:21:15 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:15 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:15 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:15 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:15 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:15 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:15 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:15 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:15 DEBUG BasicConnectionProvider: JDBC connection initiated with URL: jdbc:postgresql://localhost:5432/tpch and properties: {password=tpch, user=tpch}\n",
      "23/10/03 12:21:15 DEBUG CatalystSqlParser: Parsing command: varchar(55)\n",
      "23/10/03 12:21:15 DEBUG CatalystSqlParser: Parsing command: char(25)\n",
      "23/10/03 12:21:15 DEBUG CatalystSqlParser: Parsing command: char(10)\n",
      "23/10/03 12:21:15 DEBUG CatalystSqlParser: Parsing command: varchar(25)\n",
      "23/10/03 12:21:15 DEBUG CatalystSqlParser: Parsing command: char(10)\n",
      "23/10/03 12:21:15 DEBUG CatalystSqlParser: Parsing command: varchar(23)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(30)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 30\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 30\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(36)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 36\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 36\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(26)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 26\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 26\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(18)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 18\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 18\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(8)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 8\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 8\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(13)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 13\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 13\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(0)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning broadcast 0\n",
      "23/10/03 12:21:15 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 0\n",
      "23/10/03 12:21:15 DEBUG BlockManagerStorageEndpoint: removing broadcast 0\n",
      "23/10/03 12:21:15 DEBUG BlockManager: Removing broadcast 0\n",
      "23/10/03 12:21:15 DEBUG BlockManager: Removing block broadcast_0_piece0\n",
      "23/10/03 12:21:15 DEBUG MemoryStore: Block broadcast_0_piece0 of size 6690 dropped from memory (free 28802270488)\n",
      "23/10/03 12:21:15 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, asusbc-rtl8117.lan, 34663, None)\n",
      "23/10/03 12:21:15 INFO BlockManagerInfo: Removed broadcast_0_piece0 on asusbc-rtl8117.lan:34663 in memory (size: 6.5 KiB, free: 26.8 GiB)\n",
      "23/10/03 12:21:15 DEBUG BlockManagerMaster: Updated info of block broadcast_0_piece0\n",
      "23/10/03 12:21:15 DEBUG BlockManager: Told master about block broadcast_0_piece0\n",
      "23/10/03 12:21:15 DEBUG BlockManager: Removing block broadcast_0\n",
      "23/10/03 12:21:15 DEBUG MemoryStore: Block broadcast_0 of size 15080 dropped from memory (free 28802285568)\n",
      "23/10/03 12:21:15 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 0, response is 0\n",
      "23/10/03 12:21:15 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to asusbc-rtl8117.lan:44641\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned broadcast 0\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(31)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 31\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 31\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(19)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 19\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 19\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(34)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 34\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 34\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(5)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 5\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 5\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(3)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 3\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 3\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(16)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 16\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 16\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(20)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 20\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 20\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(37)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 37\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 37\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(4)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 4\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 4\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(7)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 7\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 7\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(9)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 9\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 9\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(27)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 27\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 27\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(23)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 23\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 23\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(32)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 32\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 32\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(11)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 11\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 11\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(28)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 28\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 28\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(33)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 33\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 33\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(29)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 29\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 29\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(22)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 22\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 22\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(10)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 10\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 10\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(17)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 17\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 17\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(35)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 35\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 35\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(6)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 6\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 6\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(15)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 15\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 15\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(14)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 14\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 14\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(21)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 21\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 21\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(25)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 25\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 25\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(12)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 12\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 12\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Got cleaning task CleanAccum(24)\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaning accumulator 24\n",
      "23/10/03 12:21:15 DEBUG ContextCleaner: Cleaned accumulator 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/03 12:21:15 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator scan_input_0;\n",
      "/* 010 */   private boolean project_resultIsNull_0;\n",
      "/* 011 */   private int project_argValue_0;\n",
      "/* 012 */   private boolean project_resultIsNull_1;\n",
      "/* 013 */   private int project_argValue_1;\n",
      "/* 014 */   private boolean project_resultIsNull_2;\n",
      "/* 015 */   private int project_argValue_2;\n",
      "/* 016 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] scan_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 017 */   private UTF8String[] project_mutableStateArray_0 = new UTF8String[3];\n",
      "/* 018 */\n",
      "/* 019 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 020 */     this.references = references;\n",
      "/* 021 */   }\n",
      "/* 022 */\n",
      "/* 023 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 024 */     partitionIndex = index;\n",
      "/* 025 */     this.inputs = inputs;\n",
      "/* 026 */     scan_input_0 = inputs[0];\n",
      "/* 027 */     scan_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 224);\n",
      "/* 028 */\n",
      "/* 029 */     scan_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 288);\n",
      "/* 030 */\n",
      "/* 031 */   }\n",
      "/* 032 */\n",
      "/* 033 */   protected void processNext() throws java.io.IOException {\n",
      "/* 034 */     while ( scan_input_0.hasNext()) {\n",
      "/* 035 */       InternalRow scan_row_0 = (InternalRow) scan_input_0.next();\n",
      "/* 036 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 037 */       // common sub-expressions\n",
      "/* 038 */\n",
      "/* 039 */       boolean scan_isNull_0 = scan_row_0.isNullAt(0);\n",
      "/* 040 */       int scan_value_0 = scan_isNull_0 ?\n",
      "/* 041 */       -1 : (scan_row_0.getInt(0));\n",
      "/* 042 */       UTF8String project_value_0;\n",
      "/* 043 */       if (scan_isNull_0) {\n",
      "/* 044 */         project_value_0 = UTF8String.fromString(\"NULL\");\n",
      "/* 045 */       } else {\n",
      "/* 046 */         project_value_0 = UTF8String.fromString(String.valueOf(scan_value_0));\n",
      "/* 047 */       }\n",
      "/* 048 */       boolean scan_isNull_1 = scan_row_0.isNullAt(1);\n",
      "/* 049 */       UTF8String scan_value_1 = scan_isNull_1 ?\n",
      "/* 050 */       null : (scan_row_0.getUTF8String(1));\n",
      "/* 051 */       UTF8String project_value_2;\n",
      "/* 052 */       if (scan_isNull_1) {\n",
      "/* 053 */         project_value_2 = UTF8String.fromString(\"NULL\");\n",
      "/* 054 */       } else {\n",
      "/* 055 */         project_value_2 = scan_value_1;\n",
      "/* 056 */       }\n",
      "/* 057 */       project_resultIsNull_0 = false;\n",
      "/* 058 */\n",
      "/* 059 */       if (!project_resultIsNull_0) {\n",
      "/* 060 */         boolean scan_isNull_2 = scan_row_0.isNullAt(2);\n",
      "/* 061 */         UTF8String scan_value_2 = scan_isNull_2 ?\n",
      "/* 062 */         null : (scan_row_0.getUTF8String(2));\n",
      "/* 063 */         project_resultIsNull_0 = scan_isNull_2;\n",
      "/* 064 */         project_mutableStateArray_0[0] = scan_value_2;\n",
      "/* 065 */       }\n",
      "/* 066 */\n",
      "/* 067 */       if (!project_resultIsNull_0) {\n",
      "/* 068 */         project_argValue_0 = 25;\n",
      "/* 069 */       }\n",
      "/* 070 */\n",
      "/* 071 */       boolean project_isNull_5 = project_resultIsNull_0;\n",
      "/* 072 */       UTF8String project_value_5 = null;\n",
      "/* 073 */       if (!project_resultIsNull_0) {\n",
      "/* 074 */         project_value_5 = org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils.readSidePadding(project_mutableStateArray_0[0], project_argValue_0);\n",
      "/* 075 */       }\n",
      "/* 076 */       UTF8String project_value_4;\n",
      "/* 077 */       if (project_isNull_5) {\n",
      "/* 078 */         project_value_4 = UTF8String.fromString(\"NULL\");\n",
      "/* 079 */       } else {\n",
      "/* 080 */         project_value_4 = project_value_5;\n",
      "/* 081 */       }\n",
      "/* 082 */       project_resultIsNull_1 = false;\n",
      "/* 083 */\n",
      "/* 084 */       if (!project_resultIsNull_1) {\n",
      "/* 085 */         boolean scan_isNull_3 = scan_row_0.isNullAt(3);\n",
      "/* 086 */         UTF8String scan_value_3 = scan_isNull_3 ?\n",
      "/* 087 */         null : (scan_row_0.getUTF8String(3));\n",
      "/* 088 */         project_resultIsNull_1 = scan_isNull_3;\n",
      "/* 089 */         project_mutableStateArray_0[1] = scan_value_3;\n",
      "/* 090 */       }\n",
      "/* 091 */\n",
      "/* 092 */       if (!project_resultIsNull_1) {\n",
      "/* 093 */         project_argValue_1 = 10;\n",
      "/* 094 */       }\n",
      "/* 095 */\n",
      "/* 096 */       boolean project_isNull_9 = project_resultIsNull_1;\n",
      "/* 097 */       UTF8String project_value_9 = null;\n",
      "/* 098 */       if (!project_resultIsNull_1) {\n",
      "/* 099 */         project_value_9 = org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils.readSidePadding(project_mutableStateArray_0[1], project_argValue_1);\n",
      "/* 100 */       }\n",
      "/* 101 */       UTF8String project_value_8;\n",
      "/* 102 */       if (project_isNull_9) {\n",
      "/* 103 */         project_value_8 = UTF8String.fromString(\"NULL\");\n",
      "/* 104 */       } else {\n",
      "/* 105 */         project_value_8 = project_value_9;\n",
      "/* 106 */       }\n",
      "/* 107 */       boolean scan_isNull_4 = scan_row_0.isNullAt(4);\n",
      "/* 108 */       UTF8String scan_value_4 = scan_isNull_4 ?\n",
      "/* 109 */       null : (scan_row_0.getUTF8String(4));\n",
      "/* 110 */       UTF8String project_value_12;\n",
      "/* 111 */       if (scan_isNull_4) {\n",
      "/* 112 */         project_value_12 = UTF8String.fromString(\"NULL\");\n",
      "/* 113 */       } else {\n",
      "/* 114 */         project_value_12 = scan_value_4;\n",
      "/* 115 */       }\n",
      "/* 116 */       boolean scan_isNull_5 = scan_row_0.isNullAt(5);\n",
      "/* 117 */       int scan_value_5 = scan_isNull_5 ?\n",
      "/* 118 */       -1 : (scan_row_0.getInt(5));\n",
      "/* 119 */       UTF8String project_value_14;\n",
      "/* 120 */       if (scan_isNull_5) {\n",
      "/* 121 */         project_value_14 = UTF8String.fromString(\"NULL\");\n",
      "/* 122 */       } else {\n",
      "/* 123 */         project_value_14 = UTF8String.fromString(String.valueOf(scan_value_5));\n",
      "/* 124 */       }\n",
      "/* 125 */       project_resultIsNull_2 = false;\n",
      "/* 126 */\n",
      "/* 127 */       if (!project_resultIsNull_2) {\n",
      "/* 128 */         boolean scan_isNull_6 = scan_row_0.isNullAt(6);\n",
      "/* 129 */         UTF8String scan_value_6 = scan_isNull_6 ?\n",
      "/* 130 */         null : (scan_row_0.getUTF8String(6));\n",
      "/* 131 */         project_resultIsNull_2 = scan_isNull_6;\n",
      "/* 132 */         project_mutableStateArray_0[2] = scan_value_6;\n",
      "/* 133 */       }\n",
      "/* 134 */\n",
      "/* 135 */       if (!project_resultIsNull_2) {\n",
      "/* 136 */         project_argValue_2 = 10;\n",
      "/* 137 */       }\n",
      "/* 138 */\n",
      "/* 139 */       boolean project_isNull_17 = project_resultIsNull_2;\n",
      "/* 140 */       UTF8String project_value_17 = null;\n",
      "/* 141 */       if (!project_resultIsNull_2) {\n",
      "/* 142 */         project_value_17 = org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils.readSidePadding(project_mutableStateArray_0[2], project_argValue_2);\n",
      "/* 143 */       }\n",
      "/* 144 */       UTF8String project_value_16;\n",
      "/* 145 */       if (project_isNull_17) {\n",
      "/* 146 */         project_value_16 = UTF8String.fromString(\"NULL\");\n",
      "/* 147 */       } else {\n",
      "/* 148 */         project_value_16 = project_value_17;\n",
      "/* 149 */       }\n",
      "/* 150 */       boolean scan_isNull_7 = scan_row_0.isNullAt(7);\n",
      "/* 151 */       Decimal scan_value_7 = scan_isNull_7 ?\n",
      "/* 152 */       null : (scan_row_0.getDecimal(7, 38, 18));\n",
      "/* 153 */       UTF8String project_value_20;\n",
      "/* 154 */       if (scan_isNull_7) {\n",
      "/* 155 */         project_value_20 = UTF8String.fromString(\"NULL\");\n",
      "/* 156 */       } else {\n",
      "/* 157 */         project_value_20 = UTF8String.fromString(scan_value_7.toPlainString());\n",
      "/* 158 */       }\n",
      "/* 159 */       boolean scan_isNull_8 = scan_row_0.isNullAt(8);\n",
      "/* 160 */       UTF8String scan_value_8 = scan_isNull_8 ?\n",
      "/* 161 */       null : (scan_row_0.getUTF8String(8));\n",
      "/* 162 */       UTF8String project_value_22;\n",
      "/* 163 */       if (scan_isNull_8) {\n",
      "/* 164 */         project_value_22 = UTF8String.fromString(\"NULL\");\n",
      "/* 165 */       } else {\n",
      "/* 166 */         project_value_22 = scan_value_8;\n",
      "/* 167 */       }\n",
      "/* 168 */       scan_mutableStateArray_0[1].reset();\n",
      "/* 169 */\n",
      "/* 170 */       scan_mutableStateArray_0[1].write(0, project_value_0);\n",
      "/* 171 */\n",
      "/* 172 */       scan_mutableStateArray_0[1].write(1, project_value_2);\n",
      "/* 173 */\n",
      "/* 174 */       scan_mutableStateArray_0[1].write(2, project_value_4);\n",
      "/* 175 */\n",
      "/* 176 */       scan_mutableStateArray_0[1].write(3, project_value_8);\n",
      "/* 177 */\n",
      "/* 178 */       scan_mutableStateArray_0[1].write(4, project_value_12);\n",
      "/* 179 */\n",
      "/* 180 */       scan_mutableStateArray_0[1].write(5, project_value_14);\n",
      "/* 181 */\n",
      "/* 182 */       scan_mutableStateArray_0[1].write(6, project_value_16);\n",
      "/* 183 */\n",
      "/* 184 */       scan_mutableStateArray_0[1].write(7, project_value_20);\n",
      "/* 185 */\n",
      "/* 186 */       scan_mutableStateArray_0[1].write(8, project_value_22);\n",
      "/* 187 */       append((scan_mutableStateArray_0[1].getRow()));\n",
      "/* 188 */       if (shouldStop()) return;\n",
      "/* 189 */     }\n",
      "/* 190 */   }\n",
      "/* 191 */\n",
      "/* 192 */ }\n",
      "\n",
      "23/10/03 12:21:15 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator scan_input_0;\n",
      "/* 010 */   private boolean project_resultIsNull_0;\n",
      "/* 011 */   private int project_argValue_0;\n",
      "/* 012 */   private boolean project_resultIsNull_1;\n",
      "/* 013 */   private int project_argValue_1;\n",
      "/* 014 */   private boolean project_resultIsNull_2;\n",
      "/* 015 */   private int project_argValue_2;\n",
      "/* 016 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] scan_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 017 */   private UTF8String[] project_mutableStateArray_0 = new UTF8String[3];\n",
      "/* 018 */\n",
      "/* 019 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 020 */     this.references = references;\n",
      "/* 021 */   }\n",
      "/* 022 */\n",
      "/* 023 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 024 */     partitionIndex = index;\n",
      "/* 025 */     this.inputs = inputs;\n",
      "/* 026 */     scan_input_0 = inputs[0];\n",
      "/* 027 */     scan_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 224);\n",
      "/* 028 */\n",
      "/* 029 */     scan_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 288);\n",
      "/* 030 */\n",
      "/* 031 */   }\n",
      "/* 032 */\n",
      "/* 033 */   protected void processNext() throws java.io.IOException {\n",
      "/* 034 */     while ( scan_input_0.hasNext()) {\n",
      "/* 035 */       InternalRow scan_row_0 = (InternalRow) scan_input_0.next();\n",
      "/* 036 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 037 */       // common sub-expressions\n",
      "/* 038 */\n",
      "/* 039 */       boolean scan_isNull_0 = scan_row_0.isNullAt(0);\n",
      "/* 040 */       int scan_value_0 = scan_isNull_0 ?\n",
      "/* 041 */       -1 : (scan_row_0.getInt(0));\n",
      "/* 042 */       UTF8String project_value_0;\n",
      "/* 043 */       if (scan_isNull_0) {\n",
      "/* 044 */         project_value_0 = UTF8String.fromString(\"NULL\");\n",
      "/* 045 */       } else {\n",
      "/* 046 */         project_value_0 = UTF8String.fromString(String.valueOf(scan_value_0));\n",
      "/* 047 */       }\n",
      "/* 048 */       boolean scan_isNull_1 = scan_row_0.isNullAt(1);\n",
      "/* 049 */       UTF8String scan_value_1 = scan_isNull_1 ?\n",
      "/* 050 */       null : (scan_row_0.getUTF8String(1));\n",
      "/* 051 */       UTF8String project_value_2;\n",
      "/* 052 */       if (scan_isNull_1) {\n",
      "/* 053 */         project_value_2 = UTF8String.fromString(\"NULL\");\n",
      "/* 054 */       } else {\n",
      "/* 055 */         project_value_2 = scan_value_1;\n",
      "/* 056 */       }\n",
      "/* 057 */       project_resultIsNull_0 = false;\n",
      "/* 058 */\n",
      "/* 059 */       if (!project_resultIsNull_0) {\n",
      "/* 060 */         boolean scan_isNull_2 = scan_row_0.isNullAt(2);\n",
      "/* 061 */         UTF8String scan_value_2 = scan_isNull_2 ?\n",
      "/* 062 */         null : (scan_row_0.getUTF8String(2));\n",
      "/* 063 */         project_resultIsNull_0 = scan_isNull_2;\n",
      "/* 064 */         project_mutableStateArray_0[0] = scan_value_2;\n",
      "/* 065 */       }\n",
      "/* 066 */\n",
      "/* 067 */       if (!project_resultIsNull_0) {\n",
      "/* 068 */         project_argValue_0 = 25;\n",
      "/* 069 */       }\n",
      "/* 070 */\n",
      "/* 071 */       boolean project_isNull_5 = project_resultIsNull_0;\n",
      "/* 072 */       UTF8String project_value_5 = null;\n",
      "/* 073 */       if (!project_resultIsNull_0) {\n",
      "/* 074 */         project_value_5 = org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils.readSidePadding(project_mutableStateArray_0[0], project_argValue_0);\n",
      "/* 075 */       }\n",
      "/* 076 */       UTF8String project_value_4;\n",
      "/* 077 */       if (project_isNull_5) {\n",
      "/* 078 */         project_value_4 = UTF8String.fromString(\"NULL\");\n",
      "/* 079 */       } else {\n",
      "/* 080 */         project_value_4 = project_value_5;\n",
      "/* 081 */       }\n",
      "/* 082 */       project_resultIsNull_1 = false;\n",
      "/* 083 */\n",
      "/* 084 */       if (!project_resultIsNull_1) {\n",
      "/* 085 */         boolean scan_isNull_3 = scan_row_0.isNullAt(3);\n",
      "/* 086 */         UTF8String scan_value_3 = scan_isNull_3 ?\n",
      "/* 087 */         null : (scan_row_0.getUTF8String(3));\n",
      "/* 088 */         project_resultIsNull_1 = scan_isNull_3;\n",
      "/* 089 */         project_mutableStateArray_0[1] = scan_value_3;\n",
      "/* 090 */       }\n",
      "/* 091 */\n",
      "/* 092 */       if (!project_resultIsNull_1) {\n",
      "/* 093 */         project_argValue_1 = 10;\n",
      "/* 094 */       }\n",
      "/* 095 */\n",
      "/* 096 */       boolean project_isNull_9 = project_resultIsNull_1;\n",
      "/* 097 */       UTF8String project_value_9 = null;\n",
      "/* 098 */       if (!project_resultIsNull_1) {\n",
      "/* 099 */         project_value_9 = org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils.readSidePadding(project_mutableStateArray_0[1], project_argValue_1);\n",
      "/* 100 */       }\n",
      "/* 101 */       UTF8String project_value_8;\n",
      "/* 102 */       if (project_isNull_9) {\n",
      "/* 103 */         project_value_8 = UTF8String.fromString(\"NULL\");\n",
      "/* 104 */       } else {\n",
      "/* 105 */         project_value_8 = project_value_9;\n",
      "/* 106 */       }\n",
      "/* 107 */       boolean scan_isNull_4 = scan_row_0.isNullAt(4);\n",
      "/* 108 */       UTF8String scan_value_4 = scan_isNull_4 ?\n",
      "/* 109 */       null : (scan_row_0.getUTF8String(4));\n",
      "/* 110 */       UTF8String project_value_12;\n",
      "/* 111 */       if (scan_isNull_4) {\n",
      "/* 112 */         project_value_12 = UTF8String.fromString(\"NULL\");\n",
      "/* 113 */       } else {\n",
      "/* 114 */         project_value_12 = scan_value_4;\n",
      "/* 115 */       }\n",
      "/* 116 */       boolean scan_isNull_5 = scan_row_0.isNullAt(5);\n",
      "/* 117 */       int scan_value_5 = scan_isNull_5 ?\n",
      "/* 118 */       -1 : (scan_row_0.getInt(5));\n",
      "/* 119 */       UTF8String project_value_14;\n",
      "/* 120 */       if (scan_isNull_5) {\n",
      "/* 121 */         project_value_14 = UTF8String.fromString(\"NULL\");\n",
      "/* 122 */       } else {\n",
      "/* 123 */         project_value_14 = UTF8String.fromString(String.valueOf(scan_value_5));\n",
      "/* 124 */       }\n",
      "/* 125 */       project_resultIsNull_2 = false;\n",
      "/* 126 */\n",
      "/* 127 */       if (!project_resultIsNull_2) {\n",
      "/* 128 */         boolean scan_isNull_6 = scan_row_0.isNullAt(6);\n",
      "/* 129 */         UTF8String scan_value_6 = scan_isNull_6 ?\n",
      "/* 130 */         null : (scan_row_0.getUTF8String(6));\n",
      "/* 131 */         project_resultIsNull_2 = scan_isNull_6;\n",
      "/* 132 */         project_mutableStateArray_0[2] = scan_value_6;\n",
      "/* 133 */       }\n",
      "/* 134 */\n",
      "/* 135 */       if (!project_resultIsNull_2) {\n",
      "/* 136 */         project_argValue_2 = 10;\n",
      "/* 137 */       }\n",
      "/* 138 */\n",
      "/* 139 */       boolean project_isNull_17 = project_resultIsNull_2;\n",
      "/* 140 */       UTF8String project_value_17 = null;\n",
      "/* 141 */       if (!project_resultIsNull_2) {\n",
      "/* 142 */         project_value_17 = org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils.readSidePadding(project_mutableStateArray_0[2], project_argValue_2);\n",
      "/* 143 */       }\n",
      "/* 144 */       UTF8String project_value_16;\n",
      "/* 145 */       if (project_isNull_17) {\n",
      "/* 146 */         project_value_16 = UTF8String.fromString(\"NULL\");\n",
      "/* 147 */       } else {\n",
      "/* 148 */         project_value_16 = project_value_17;\n",
      "/* 149 */       }\n",
      "/* 150 */       boolean scan_isNull_7 = scan_row_0.isNullAt(7);\n",
      "/* 151 */       Decimal scan_value_7 = scan_isNull_7 ?\n",
      "/* 152 */       null : (scan_row_0.getDecimal(7, 38, 18));\n",
      "/* 153 */       UTF8String project_value_20;\n",
      "/* 154 */       if (scan_isNull_7) {\n",
      "/* 155 */         project_value_20 = UTF8String.fromString(\"NULL\");\n",
      "/* 156 */       } else {\n",
      "/* 157 */         project_value_20 = UTF8String.fromString(scan_value_7.toPlainString());\n",
      "/* 158 */       }\n",
      "/* 159 */       boolean scan_isNull_8 = scan_row_0.isNullAt(8);\n",
      "/* 160 */       UTF8String scan_value_8 = scan_isNull_8 ?\n",
      "/* 161 */       null : (scan_row_0.getUTF8String(8));\n",
      "/* 162 */       UTF8String project_value_22;\n",
      "/* 163 */       if (scan_isNull_8) {\n",
      "/* 164 */         project_value_22 = UTF8String.fromString(\"NULL\");\n",
      "/* 165 */       } else {\n",
      "/* 166 */         project_value_22 = scan_value_8;\n",
      "/* 167 */       }\n",
      "/* 168 */       scan_mutableStateArray_0[1].reset();\n",
      "/* 169 */\n",
      "/* 170 */       scan_mutableStateArray_0[1].write(0, project_value_0);\n",
      "/* 171 */\n",
      "/* 172 */       scan_mutableStateArray_0[1].write(1, project_value_2);\n",
      "/* 173 */\n",
      "/* 174 */       scan_mutableStateArray_0[1].write(2, project_value_4);\n",
      "/* 175 */\n",
      "/* 176 */       scan_mutableStateArray_0[1].write(3, project_value_8);\n",
      "/* 177 */\n",
      "/* 178 */       scan_mutableStateArray_0[1].write(4, project_value_12);\n",
      "/* 179 */\n",
      "/* 180 */       scan_mutableStateArray_0[1].write(5, project_value_14);\n",
      "/* 181 */\n",
      "/* 182 */       scan_mutableStateArray_0[1].write(6, project_value_16);\n",
      "/* 183 */\n",
      "/* 184 */       scan_mutableStateArray_0[1].write(7, project_value_20);\n",
      "/* 185 */\n",
      "/* 186 */       scan_mutableStateArray_0[1].write(8, project_value_22);\n",
      "/* 187 */       append((scan_mutableStateArray_0[1].getRow()));\n",
      "/* 188 */       if (shouldStop()) return;\n",
      "/* 189 */     }\n",
      "/* 190 */   }\n",
      "/* 191 */\n",
      "/* 192 */ }\n",
      "\n",
      "23/10/03 12:21:15 INFO CodeGenerator: Code generated in 23.967089 ms\n",
      "23/10/03 12:21:15 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "23/10/03 12:21:15 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "23/10/03 12:21:15 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$executeTake$2\n",
      "23/10/03 12:21:15 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++\n",
      "23/10/03 12:21:15 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "23/10/03 12:21:15 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "23/10/03 12:21:15 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/10/03 12:21:15 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 5 took 0.000075 seconds\n",
      "23/10/03 12:21:15 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "23/10/03 12:21:15 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/10/03 12:21:15 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/10/03 12:21:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/10/03 12:21:15 INFO DAGScheduler: Missing parents: List()\n",
      "23/10/03 12:21:15 DEBUG DAGScheduler: submitStage(ResultStage 1 (name=showString at NativeMethodAccessorImpl.java:0;jobs=1))\n",
      "23/10/03 12:21:15 DEBUG DAGScheduler: missing: List()\n",
      "23/10/03 12:21:15 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/10/03 12:21:15 DEBUG DAGScheduler: submitMissingTasks(ResultStage 1)\n",
      "23/10/03 12:21:15 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 26.8 GiB)\n",
      "23/10/03 12:21:15 DEBUG BlockManager: Put block broadcast_1 locally took 1 ms\n",
      "23/10/03 12:21:15 DEBUG BlockManager: Putting block broadcast_1 without replication took 1 ms\n",
      "23/10/03 12:21:15 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 26.8 GiB)\n",
      "23/10/03 12:21:15 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, asusbc-rtl8117.lan, 34663, None)\n",
      "23/10/03 12:21:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on asusbc-rtl8117.lan:34663 (size: 7.2 KiB, free: 26.8 GiB)\n",
      "23/10/03 12:21:15 DEBUG BlockManagerMaster: Updated info of block broadcast_1_piece0\n",
      "23/10/03 12:21:15 DEBUG BlockManager: Told master about block broadcast_1_piece0\n",
      "23/10/03 12:21:15 DEBUG BlockManager: Put block broadcast_1_piece0 locally took 0 ms\n",
      "23/10/03 12:21:15 DEBUG BlockManager: Putting block broadcast_1_piece0 without replication took 0 ms\n",
      "23/10/03 12:21:15 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580\n",
      "23/10/03 12:21:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/10/03 12:21:15 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "23/10/03 12:21:15 DEBUG TaskSetManager: Epoch for TaskSet 1.0: 0\n",
      "23/10/03 12:21:15 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "23/10/03 12:21:15 DEBUG TaskSetManager: Valid locality levels for TaskSet 1.0: NO_PREF, ANY\n",
      "23/10/03 12:21:15 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_1.0, runningTasks: 0\n",
      "23/10/03 12:21:15 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (asusbc-rtl8117.lan, executor driver, partition 0, PROCESS_LOCAL, 7647 bytes) \n",
      "23/10/03 12:21:15 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "23/10/03 12:21:15 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "23/10/03 12:21:15 DEBUG ExecutorMetricsPoller: stageTCMP: (1, 0) -> 1\n",
      "23/10/03 12:21:15 DEBUG BlockManager: Getting local block broadcast_1\n",
      "23/10/03 12:21:15 DEBUG BlockManager: Level for block broadcast_1 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "23/10/03 12:21:15 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:15 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:15 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:15 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:15 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:15 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:15 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:15 DEBUG BasicConnectionProvider: JDBC connection initiated with URL: jdbc:postgresql://localhost:5432/tpch and properties: {password=tpch, user=tpch}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/03 12:21:17 INFO JDBCRDD: closed connection                   (0 + 1) / 1]\n",
      "23/10/03 12:21:17 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 3943 bytes result sent to driver\n",
      "23/10/03 12:21:17 DEBUG ExecutorMetricsPoller: stageTCMP: (1, 0) -> 0\n",
      "23/10/03 12:21:17 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1641 ms on asusbc-rtl8117.lan (executor driver) (1/1)\n",
      "23/10/03 12:21:17 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "23/10/03 12:21:17 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 1.650 s\n",
      "23/10/03 12:21:17 DEBUG DAGScheduler: After removal of stage 1, remaining stages = 0\n",
      "23/10/03 12:21:17 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/10/03 12:21:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "23/10/03 12:21:17 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 1.652977 s\n",
      "23/10/03 12:21:17 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, false].toString, input[1, string, false].toString, input[2, string, false].toString, input[3, string, false].toString, input[4, string, false].toString, input[5, string, false].toString, input[6, string, false].toString, input[7, string, false].toString, input[8, string, false].toString, StructField(toprettystring(p_partkey),StringType,false), StructField(toprettystring(p_name),StringType,false), StructField(toprettystring(p_mfgr),StringType,false), StructField(toprettystring(p_brand),StringType,false), StructField(toprettystring(p_type),StringType,false), StructField(toprettystring(p_size),StringType,false), StructField(toprettystring(p_container),StringType,false), StructField(toprettystring(p_retailprice),StringType,false), StructField(toprettystring(p_comment),StringType,false)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[9];\n",
      "/* 024 */     createExternalRow_0_0(i, values_0);\n",
      "/* 025 */     createExternalRow_0_1(i, values_0);\n",
      "/* 026 */     createExternalRow_0_2(i, values_0);\n",
      "/* 027 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 028 */     if (false) {\n",
      "/* 029 */       mutableRow.setNullAt(0);\n",
      "/* 030 */     } else {\n",
      "/* 031 */\n",
      "/* 032 */       mutableRow.update(0, value_0);\n",
      "/* 033 */     }\n",
      "/* 034 */\n",
      "/* 035 */     return mutableRow;\n",
      "/* 036 */   }\n",
      "/* 037 */\n",
      "/* 038 */\n",
      "/* 039 */   private void createExternalRow_0_2(InternalRow i, Object[] values_0) {\n",
      "/* 040 */\n",
      "/* 041 */     UTF8String value_14 = i.getUTF8String(6);\n",
      "/* 042 */     boolean isNull_13 = true;\n",
      "/* 043 */     java.lang.String value_13 = null;\n",
      "/* 044 */     isNull_13 = false;\n",
      "/* 045 */     if (!isNull_13) {\n",
      "/* 046 */\n",
      "/* 047 */       Object funcResult_6 = null;\n",
      "/* 048 */       funcResult_6 = value_14.toString();\n",
      "/* 049 */       value_13 = (java.lang.String) funcResult_6;\n",
      "/* 050 */\n",
      "/* 051 */     }\n",
      "/* 052 */     if (isNull_13) {\n",
      "/* 053 */       values_0[6] = null;\n",
      "/* 054 */     } else {\n",
      "/* 055 */       values_0[6] = value_13;\n",
      "/* 056 */     }\n",
      "/* 057 */\n",
      "/* 058 */     UTF8String value_16 = i.getUTF8String(7);\n",
      "/* 059 */     boolean isNull_15 = true;\n",
      "/* 060 */     java.lang.String value_15 = null;\n",
      "/* 061 */     isNull_15 = false;\n",
      "/* 062 */     if (!isNull_15) {\n",
      "/* 063 */\n",
      "/* 064 */       Object funcResult_7 = null;\n",
      "/* 065 */       funcResult_7 = value_16.toString();\n",
      "/* 066 */       value_15 = (java.lang.String) funcResult_7;\n",
      "/* 067 */\n",
      "/* 068 */     }\n",
      "/* 069 */     if (isNull_15) {\n",
      "/* 070 */       values_0[7] = null;\n",
      "/* 071 */     } else {\n",
      "/* 072 */       values_0[7] = value_15;\n",
      "/* 073 */     }\n",
      "/* 074 */\n",
      "/* 075 */     UTF8String value_18 = i.getUTF8String(8);\n",
      "/* 076 */     boolean isNull_17 = true;\n",
      "/* 077 */     java.lang.String value_17 = null;\n",
      "/* 078 */     isNull_17 = false;\n",
      "/* 079 */     if (!isNull_17) {\n",
      "/* 080 */\n",
      "/* 081 */       Object funcResult_8 = null;\n",
      "/* 082 */       funcResult_8 = value_18.toString();\n",
      "/* 083 */       value_17 = (java.lang.String) funcResult_8;\n",
      "/* 084 */\n",
      "/* 085 */     }\n",
      "/* 086 */     if (isNull_17) {\n",
      "/* 087 */       values_0[8] = null;\n",
      "/* 088 */     } else {\n",
      "/* 089 */       values_0[8] = value_17;\n",
      "/* 090 */     }\n",
      "/* 091 */\n",
      "/* 092 */   }\n",
      "/* 093 */\n",
      "/* 094 */\n",
      "/* 095 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {\n",
      "/* 096 */\n",
      "/* 097 */     UTF8String value_8 = i.getUTF8String(3);\n",
      "/* 098 */     boolean isNull_7 = true;\n",
      "/* 099 */     java.lang.String value_7 = null;\n",
      "/* 100 */     isNull_7 = false;\n",
      "/* 101 */     if (!isNull_7) {\n",
      "/* 102 */\n",
      "/* 103 */       Object funcResult_3 = null;\n",
      "/* 104 */       funcResult_3 = value_8.toString();\n",
      "/* 105 */       value_7 = (java.lang.String) funcResult_3;\n",
      "/* 106 */\n",
      "/* 107 */     }\n",
      "/* 108 */     if (isNull_7) {\n",
      "/* 109 */       values_0[3] = null;\n",
      "/* 110 */     } else {\n",
      "/* 111 */       values_0[3] = value_7;\n",
      "/* 112 */     }\n",
      "/* 113 */\n",
      "/* 114 */     UTF8String value_10 = i.getUTF8String(4);\n",
      "/* 115 */     boolean isNull_9 = true;\n",
      "/* 116 */     java.lang.String value_9 = null;\n",
      "/* 117 */     isNull_9 = false;\n",
      "/* 118 */     if (!isNull_9) {\n",
      "/* 119 */\n",
      "/* 120 */       Object funcResult_4 = null;\n",
      "/* 121 */       funcResult_4 = value_10.toString();\n",
      "/* 122 */       value_9 = (java.lang.String) funcResult_4;\n",
      "/* 123 */\n",
      "/* 124 */     }\n",
      "/* 125 */     if (isNull_9) {\n",
      "/* 126 */       values_0[4] = null;\n",
      "/* 127 */     } else {\n",
      "/* 128 */       values_0[4] = value_9;\n",
      "/* 129 */     }\n",
      "/* 130 */\n",
      "/* 131 */     UTF8String value_12 = i.getUTF8String(5);\n",
      "/* 132 */     boolean isNull_11 = true;\n",
      "/* 133 */     java.lang.String value_11 = null;\n",
      "/* 134 */     isNull_11 = false;\n",
      "/* 135 */     if (!isNull_11) {\n",
      "/* 136 */\n",
      "/* 137 */       Object funcResult_5 = null;\n",
      "/* 138 */       funcResult_5 = value_12.toString();\n",
      "/* 139 */       value_11 = (java.lang.String) funcResult_5;\n",
      "/* 140 */\n",
      "/* 141 */     }\n",
      "/* 142 */     if (isNull_11) {\n",
      "/* 143 */       values_0[5] = null;\n",
      "/* 144 */     } else {\n",
      "/* 145 */       values_0[5] = value_11;\n",
      "/* 146 */     }\n",
      "/* 147 */\n",
      "/* 148 */   }\n",
      "/* 149 */\n",
      "/* 150 */\n",
      "/* 151 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {\n",
      "/* 152 */\n",
      "/* 153 */     UTF8String value_2 = i.getUTF8String(0);\n",
      "/* 154 */     boolean isNull_1 = true;\n",
      "/* 155 */     java.lang.String value_1 = null;\n",
      "/* 156 */     isNull_1 = false;\n",
      "/* 157 */     if (!isNull_1) {\n",
      "/* 158 */\n",
      "/* 159 */       Object funcResult_0 = null;\n",
      "/* 160 */       funcResult_0 = value_2.toString();\n",
      "/* 161 */       value_1 = (java.lang.String) funcResult_0;\n",
      "/* 162 */\n",
      "/* 163 */     }\n",
      "/* 164 */     if (isNull_1) {\n",
      "/* 165 */       values_0[0] = null;\n",
      "/* 166 */     } else {\n",
      "/* 167 */       values_0[0] = value_1;\n",
      "/* 168 */     }\n",
      "/* 169 */\n",
      "/* 170 */     UTF8String value_4 = i.getUTF8String(1);\n",
      "/* 171 */     boolean isNull_3 = true;\n",
      "/* 172 */     java.lang.String value_3 = null;\n",
      "/* 173 */     isNull_3 = false;\n",
      "/* 174 */     if (!isNull_3) {\n",
      "/* 175 */\n",
      "/* 176 */       Object funcResult_1 = null;\n",
      "/* 177 */       funcResult_1 = value_4.toString();\n",
      "/* 178 */       value_3 = (java.lang.String) funcResult_1;\n",
      "/* 179 */\n",
      "/* 180 */     }\n",
      "/* 181 */     if (isNull_3) {\n",
      "/* 182 */       values_0[1] = null;\n",
      "/* 183 */     } else {\n",
      "/* 184 */       values_0[1] = value_3;\n",
      "/* 185 */     }\n",
      "/* 186 */\n",
      "/* 187 */     UTF8String value_6 = i.getUTF8String(2);\n",
      "/* 188 */     boolean isNull_5 = true;\n",
      "/* 189 */     java.lang.String value_5 = null;\n",
      "/* 190 */     isNull_5 = false;\n",
      "/* 191 */     if (!isNull_5) {\n",
      "/* 192 */\n",
      "/* 193 */       Object funcResult_2 = null;\n",
      "/* 194 */       funcResult_2 = value_6.toString();\n",
      "/* 195 */       value_5 = (java.lang.String) funcResult_2;\n",
      "/* 196 */\n",
      "/* 197 */     }\n",
      "/* 198 */     if (isNull_5) {\n",
      "/* 199 */       values_0[2] = null;\n",
      "/* 200 */     } else {\n",
      "/* 201 */       values_0[2] = value_5;\n",
      "/* 202 */     }\n",
      "/* 203 */\n",
      "/* 204 */   }\n",
      "/* 205 */\n",
      "/* 206 */ }\n",
      "\n",
      "23/10/03 12:21:17 DEBUG CodeGenerator: \n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[9];\n",
      "/* 024 */     createExternalRow_0_0(i, values_0);\n",
      "/* 025 */     createExternalRow_0_1(i, values_0);\n",
      "/* 026 */     createExternalRow_0_2(i, values_0);\n",
      "/* 027 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 028 */     if (false) {\n",
      "/* 029 */       mutableRow.setNullAt(0);\n",
      "/* 030 */     } else {\n",
      "/* 031 */\n",
      "/* 032 */       mutableRow.update(0, value_0);\n",
      "/* 033 */     }\n",
      "/* 034 */\n",
      "/* 035 */     return mutableRow;\n",
      "/* 036 */   }\n",
      "/* 037 */\n",
      "/* 038 */\n",
      "/* 039 */   private void createExternalRow_0_2(InternalRow i, Object[] values_0) {\n",
      "/* 040 */\n",
      "/* 041 */     UTF8String value_14 = i.getUTF8String(6);\n",
      "/* 042 */     boolean isNull_13 = true;\n",
      "/* 043 */     java.lang.String value_13 = null;\n",
      "/* 044 */     isNull_13 = false;\n",
      "/* 045 */     if (!isNull_13) {\n",
      "/* 046 */\n",
      "/* 047 */       Object funcResult_6 = null;\n",
      "/* 048 */       funcResult_6 = value_14.toString();\n",
      "/* 049 */       value_13 = (java.lang.String) funcResult_6;\n",
      "/* 050 */\n",
      "/* 051 */     }\n",
      "/* 052 */     if (isNull_13) {\n",
      "/* 053 */       values_0[6] = null;\n",
      "/* 054 */     } else {\n",
      "/* 055 */       values_0[6] = value_13;\n",
      "/* 056 */     }\n",
      "/* 057 */\n",
      "/* 058 */     UTF8String value_16 = i.getUTF8String(7);\n",
      "/* 059 */     boolean isNull_15 = true;\n",
      "/* 060 */     java.lang.String value_15 = null;\n",
      "/* 061 */     isNull_15 = false;\n",
      "/* 062 */     if (!isNull_15) {\n",
      "/* 063 */\n",
      "/* 064 */       Object funcResult_7 = null;\n",
      "/* 065 */       funcResult_7 = value_16.toString();\n",
      "/* 066 */       value_15 = (java.lang.String) funcResult_7;\n",
      "/* 067 */\n",
      "/* 068 */     }\n",
      "/* 069 */     if (isNull_15) {\n",
      "/* 070 */       values_0[7] = null;\n",
      "/* 071 */     } else {\n",
      "/* 072 */       values_0[7] = value_15;\n",
      "/* 073 */     }\n",
      "/* 074 */\n",
      "/* 075 */     UTF8String value_18 = i.getUTF8String(8);\n",
      "/* 076 */     boolean isNull_17 = true;\n",
      "/* 077 */     java.lang.String value_17 = null;\n",
      "/* 078 */     isNull_17 = false;\n",
      "/* 079 */     if (!isNull_17) {\n",
      "/* 080 */\n",
      "/* 081 */       Object funcResult_8 = null;\n",
      "/* 082 */       funcResult_8 = value_18.toString();\n",
      "/* 083 */       value_17 = (java.lang.String) funcResult_8;\n",
      "/* 084 */\n",
      "/* 085 */     }\n",
      "/* 086 */     if (isNull_17) {\n",
      "/* 087 */       values_0[8] = null;\n",
      "/* 088 */     } else {\n",
      "/* 089 */       values_0[8] = value_17;\n",
      "/* 090 */     }\n",
      "/* 091 */\n",
      "/* 092 */   }\n",
      "/* 093 */\n",
      "/* 094 */\n",
      "/* 095 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {\n",
      "/* 096 */\n",
      "/* 097 */     UTF8String value_8 = i.getUTF8String(3);\n",
      "/* 098 */     boolean isNull_7 = true;\n",
      "/* 099 */     java.lang.String value_7 = null;\n",
      "/* 100 */     isNull_7 = false;\n",
      "/* 101 */     if (!isNull_7) {\n",
      "/* 102 */\n",
      "/* 103 */       Object funcResult_3 = null;\n",
      "/* 104 */       funcResult_3 = value_8.toString();\n",
      "/* 105 */       value_7 = (java.lang.String) funcResult_3;\n",
      "/* 106 */\n",
      "/* 107 */     }\n",
      "/* 108 */     if (isNull_7) {\n",
      "/* 109 */       values_0[3] = null;\n",
      "/* 110 */     } else {\n",
      "/* 111 */       values_0[3] = value_7;\n",
      "/* 112 */     }\n",
      "/* 113 */\n",
      "/* 114 */     UTF8String value_10 = i.getUTF8String(4);\n",
      "/* 115 */     boolean isNull_9 = true;\n",
      "/* 116 */     java.lang.String value_9 = null;\n",
      "/* 117 */     isNull_9 = false;\n",
      "/* 118 */     if (!isNull_9) {\n",
      "/* 119 */\n",
      "/* 120 */       Object funcResult_4 = null;\n",
      "/* 121 */       funcResult_4 = value_10.toString();\n",
      "/* 122 */       value_9 = (java.lang.String) funcResult_4;\n",
      "/* 123 */\n",
      "/* 124 */     }\n",
      "/* 125 */     if (isNull_9) {\n",
      "/* 126 */       values_0[4] = null;\n",
      "/* 127 */     } else {\n",
      "/* 128 */       values_0[4] = value_9;\n",
      "/* 129 */     }\n",
      "/* 130 */\n",
      "/* 131 */     UTF8String value_12 = i.getUTF8String(5);\n",
      "/* 132 */     boolean isNull_11 = true;\n",
      "/* 133 */     java.lang.String value_11 = null;\n",
      "/* 134 */     isNull_11 = false;\n",
      "/* 135 */     if (!isNull_11) {\n",
      "/* 136 */\n",
      "/* 137 */       Object funcResult_5 = null;\n",
      "/* 138 */       funcResult_5 = value_12.toString();\n",
      "/* 139 */       value_11 = (java.lang.String) funcResult_5;\n",
      "/* 140 */\n",
      "/* 141 */     }\n",
      "/* 142 */     if (isNull_11) {\n",
      "/* 143 */       values_0[5] = null;\n",
      "/* 144 */     } else {\n",
      "/* 145 */       values_0[5] = value_11;\n",
      "/* 146 */     }\n",
      "/* 147 */\n",
      "/* 148 */   }\n",
      "/* 149 */\n",
      "/* 150 */\n",
      "/* 151 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {\n",
      "/* 152 */\n",
      "/* 153 */     UTF8String value_2 = i.getUTF8String(0);\n",
      "/* 154 */     boolean isNull_1 = true;\n",
      "/* 155 */     java.lang.String value_1 = null;\n",
      "/* 156 */     isNull_1 = false;\n",
      "/* 157 */     if (!isNull_1) {\n",
      "/* 158 */\n",
      "/* 159 */       Object funcResult_0 = null;\n",
      "/* 160 */       funcResult_0 = value_2.toString();\n",
      "/* 161 */       value_1 = (java.lang.String) funcResult_0;\n",
      "/* 162 */\n",
      "/* 163 */     }\n",
      "/* 164 */     if (isNull_1) {\n",
      "/* 165 */       values_0[0] = null;\n",
      "/* 166 */     } else {\n",
      "/* 167 */       values_0[0] = value_1;\n",
      "/* 168 */     }\n",
      "/* 169 */\n",
      "/* 170 */     UTF8String value_4 = i.getUTF8String(1);\n",
      "/* 171 */     boolean isNull_3 = true;\n",
      "/* 172 */     java.lang.String value_3 = null;\n",
      "/* 173 */     isNull_3 = false;\n",
      "/* 174 */     if (!isNull_3) {\n",
      "/* 175 */\n",
      "/* 176 */       Object funcResult_1 = null;\n",
      "/* 177 */       funcResult_1 = value_4.toString();\n",
      "/* 178 */       value_3 = (java.lang.String) funcResult_1;\n",
      "/* 179 */\n",
      "/* 180 */     }\n",
      "/* 181 */     if (isNull_3) {\n",
      "/* 182 */       values_0[1] = null;\n",
      "/* 183 */     } else {\n",
      "/* 184 */       values_0[1] = value_3;\n",
      "/* 185 */     }\n",
      "/* 186 */\n",
      "/* 187 */     UTF8String value_6 = i.getUTF8String(2);\n",
      "/* 188 */     boolean isNull_5 = true;\n",
      "/* 189 */     java.lang.String value_5 = null;\n",
      "/* 190 */     isNull_5 = false;\n",
      "/* 191 */     if (!isNull_5) {\n",
      "/* 192 */\n",
      "/* 193 */       Object funcResult_2 = null;\n",
      "/* 194 */       funcResult_2 = value_6.toString();\n",
      "/* 195 */       value_5 = (java.lang.String) funcResult_2;\n",
      "/* 196 */\n",
      "/* 197 */     }\n",
      "/* 198 */     if (isNull_5) {\n",
      "/* 199 */       values_0[2] = null;\n",
      "/* 200 */     } else {\n",
      "/* 201 */       values_0[2] = value_5;\n",
      "/* 202 */     }\n",
      "/* 203 */\n",
      "/* 204 */   }\n",
      "/* 205 */\n",
      "/* 206 */ }\n",
      "\n",
      "23/10/03 12:21:17 INFO CodeGenerator: Code generated in 13.983269 ms\n",
      "23/10/03 12:21:17 DEBUG SparkSqlParser: Parsing command: part\n",
      "23/10/03 12:21:18 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:18 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:18 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:18 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:18 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:18 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:18 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:18 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:18 DEBUG BasicConnectionProvider: JDBC connection initiated with URL: jdbc:postgresql://localhost:5432/tpch and properties: {password=tpch, user=tpch}\n",
      "23/10/03 12:21:18 DEBUG CatalystSqlParser: Parsing command: char(25)\n",
      "23/10/03 12:21:18 DEBUG CatalystSqlParser: Parsing command: varchar(40)\n",
      "23/10/03 12:21:18 DEBUG CatalystSqlParser: Parsing command: char(15)\n",
      "23/10/03 12:21:18 DEBUG CatalystSqlParser: Parsing command: varchar(101)\n",
      "23/10/03 12:21:18 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator scan_input_0;\n",
      "/* 010 */   private boolean project_resultIsNull_0;\n",
      "/* 011 */   private int project_argValue_0;\n",
      "/* 012 */   private boolean project_resultIsNull_1;\n",
      "/* 013 */   private int project_argValue_1;\n",
      "/* 014 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] scan_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 015 */   private UTF8String[] project_mutableStateArray_0 = new UTF8String[2];\n",
      "/* 016 */\n",
      "/* 017 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 018 */     this.references = references;\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 022 */     partitionIndex = index;\n",
      "/* 023 */     this.inputs = inputs;\n",
      "/* 024 */     scan_input_0 = inputs[0];\n",
      "/* 025 */     scan_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(7, 160);\n",
      "/* 026 */\n",
      "/* 027 */     scan_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(7, 224);\n",
      "/* 028 */\n",
      "/* 029 */   }\n",
      "/* 030 */\n",
      "/* 031 */   protected void processNext() throws java.io.IOException {\n",
      "/* 032 */     while ( scan_input_0.hasNext()) {\n",
      "/* 033 */       InternalRow scan_row_0 = (InternalRow) scan_input_0.next();\n",
      "/* 034 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 035 */       // common sub-expressions\n",
      "/* 036 */\n",
      "/* 037 */       boolean scan_isNull_0 = scan_row_0.isNullAt(0);\n",
      "/* 038 */       int scan_value_0 = scan_isNull_0 ?\n",
      "/* 039 */       -1 : (scan_row_0.getInt(0));\n",
      "/* 040 */       UTF8String project_value_0;\n",
      "/* 041 */       if (scan_isNull_0) {\n",
      "/* 042 */         project_value_0 = UTF8String.fromString(\"NULL\");\n",
      "/* 043 */       } else {\n",
      "/* 044 */         project_value_0 = UTF8String.fromString(String.valueOf(scan_value_0));\n",
      "/* 045 */       }\n",
      "/* 046 */       project_resultIsNull_0 = false;\n",
      "/* 047 */\n",
      "/* 048 */       if (!project_resultIsNull_0) {\n",
      "/* 049 */         boolean scan_isNull_1 = scan_row_0.isNullAt(1);\n",
      "/* 050 */         UTF8String scan_value_1 = scan_isNull_1 ?\n",
      "/* 051 */         null : (scan_row_0.getUTF8String(1));\n",
      "/* 052 */         project_resultIsNull_0 = scan_isNull_1;\n",
      "/* 053 */         project_mutableStateArray_0[0] = scan_value_1;\n",
      "/* 054 */       }\n",
      "/* 055 */\n",
      "/* 056 */       if (!project_resultIsNull_0) {\n",
      "/* 057 */         project_argValue_0 = 25;\n",
      "/* 058 */       }\n",
      "/* 059 */\n",
      "/* 060 */       boolean project_isNull_3 = project_resultIsNull_0;\n",
      "/* 061 */       UTF8String project_value_3 = null;\n",
      "/* 062 */       if (!project_resultIsNull_0) {\n",
      "/* 063 */         project_value_3 = org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils.readSidePadding(project_mutableStateArray_0[0], project_argValue_0);\n",
      "/* 064 */       }\n",
      "/* 065 */       UTF8String project_value_2;\n",
      "/* 066 */       if (project_isNull_3) {\n",
      "/* 067 */         project_value_2 = UTF8String.fromString(\"NULL\");\n",
      "/* 068 */       } else {\n",
      "/* 069 */         project_value_2 = project_value_3;\n",
      "/* 070 */       }\n",
      "/* 071 */       boolean scan_isNull_2 = scan_row_0.isNullAt(2);\n",
      "/* 072 */       UTF8String scan_value_2 = scan_isNull_2 ?\n",
      "/* 073 */       null : (scan_row_0.getUTF8String(2));\n",
      "/* 074 */       UTF8String project_value_6;\n",
      "/* 075 */       if (scan_isNull_2) {\n",
      "/* 076 */         project_value_6 = UTF8String.fromString(\"NULL\");\n",
      "/* 077 */       } else {\n",
      "/* 078 */         project_value_6 = scan_value_2;\n",
      "/* 079 */       }\n",
      "/* 080 */       boolean scan_isNull_3 = scan_row_0.isNullAt(3);\n",
      "/* 081 */       long scan_value_3 = scan_isNull_3 ?\n",
      "/* 082 */       -1L : (scan_row_0.getLong(3));\n",
      "/* 083 */       UTF8String project_value_8;\n",
      "/* 084 */       if (scan_isNull_3) {\n",
      "/* 085 */         project_value_8 = UTF8String.fromString(\"NULL\");\n",
      "/* 086 */       } else {\n",
      "/* 087 */         project_value_8 = UTF8String.fromString(String.valueOf(scan_value_3));\n",
      "/* 088 */       }\n",
      "/* 089 */       project_resultIsNull_1 = false;\n",
      "/* 090 */\n",
      "/* 091 */       if (!project_resultIsNull_1) {\n",
      "/* 092 */         boolean scan_isNull_4 = scan_row_0.isNullAt(4);\n",
      "/* 093 */         UTF8String scan_value_4 = scan_isNull_4 ?\n",
      "/* 094 */         null : (scan_row_0.getUTF8String(4));\n",
      "/* 095 */         project_resultIsNull_1 = scan_isNull_4;\n",
      "/* 096 */         project_mutableStateArray_0[1] = scan_value_4;\n",
      "/* 097 */       }\n",
      "/* 098 */\n",
      "/* 099 */       if (!project_resultIsNull_1) {\n",
      "/* 100 */         project_argValue_1 = 15;\n",
      "/* 101 */       }\n",
      "/* 102 */\n",
      "/* 103 */       boolean project_isNull_11 = project_resultIsNull_1;\n",
      "/* 104 */       UTF8String project_value_11 = null;\n",
      "/* 105 */       if (!project_resultIsNull_1) {\n",
      "/* 106 */         project_value_11 = org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils.readSidePadding(project_mutableStateArray_0[1], project_argValue_1);\n",
      "/* 107 */       }\n",
      "/* 108 */       UTF8String project_value_10;\n",
      "/* 109 */       if (project_isNull_11) {\n",
      "/* 110 */         project_value_10 = UTF8String.fromString(\"NULL\");\n",
      "/* 111 */       } else {\n",
      "/* 112 */         project_value_10 = project_value_11;\n",
      "/* 113 */       }\n",
      "/* 114 */       boolean scan_isNull_5 = scan_row_0.isNullAt(5);\n",
      "/* 115 */       Decimal scan_value_5 = scan_isNull_5 ?\n",
      "/* 116 */       null : (scan_row_0.getDecimal(5, 38, 18));\n",
      "/* 117 */       UTF8String project_value_14;\n",
      "/* 118 */       if (scan_isNull_5) {\n",
      "/* 119 */         project_value_14 = UTF8String.fromString(\"NULL\");\n",
      "/* 120 */       } else {\n",
      "/* 121 */         project_value_14 = UTF8String.fromString(scan_value_5.toPlainString());\n",
      "/* 122 */       }\n",
      "/* 123 */       boolean scan_isNull_6 = scan_row_0.isNullAt(6);\n",
      "/* 124 */       UTF8String scan_value_6 = scan_isNull_6 ?\n",
      "/* 125 */       null : (scan_row_0.getUTF8String(6));\n",
      "/* 126 */       UTF8String project_value_16;\n",
      "/* 127 */       if (scan_isNull_6) {\n",
      "/* 128 */         project_value_16 = UTF8String.fromString(\"NULL\");\n",
      "/* 129 */       } else {\n",
      "/* 130 */         project_value_16 = scan_value_6;\n",
      "/* 131 */       }\n",
      "/* 132 */       scan_mutableStateArray_0[1].reset();\n",
      "/* 133 */\n",
      "/* 134 */       scan_mutableStateArray_0[1].write(0, project_value_0);\n",
      "/* 135 */\n",
      "/* 136 */       scan_mutableStateArray_0[1].write(1, project_value_2);\n",
      "/* 137 */\n",
      "/* 138 */       scan_mutableStateArray_0[1].write(2, project_value_6);\n",
      "/* 139 */\n",
      "/* 140 */       scan_mutableStateArray_0[1].write(3, project_value_8);\n",
      "/* 141 */\n",
      "/* 142 */       scan_mutableStateArray_0[1].write(4, project_value_10);\n",
      "/* 143 */\n",
      "/* 144 */       scan_mutableStateArray_0[1].write(5, project_value_14);\n",
      "/* 145 */\n",
      "/* 146 */       scan_mutableStateArray_0[1].write(6, project_value_16);\n",
      "/* 147 */       append((scan_mutableStateArray_0[1].getRow()));\n",
      "/* 148 */       if (shouldStop()) return;\n",
      "/* 149 */     }\n",
      "/* 150 */   }\n",
      "/* 151 */\n",
      "/* 152 */ }\n",
      "\n",
      "23/10/03 12:21:18 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator scan_input_0;\n",
      "/* 010 */   private boolean project_resultIsNull_0;\n",
      "/* 011 */   private int project_argValue_0;\n",
      "/* 012 */   private boolean project_resultIsNull_1;\n",
      "/* 013 */   private int project_argValue_1;\n",
      "/* 014 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] scan_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 015 */   private UTF8String[] project_mutableStateArray_0 = new UTF8String[2];\n",
      "/* 016 */\n",
      "/* 017 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 018 */     this.references = references;\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 022 */     partitionIndex = index;\n",
      "/* 023 */     this.inputs = inputs;\n",
      "/* 024 */     scan_input_0 = inputs[0];\n",
      "/* 025 */     scan_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(7, 160);\n",
      "/* 026 */\n",
      "/* 027 */     scan_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(7, 224);\n",
      "/* 028 */\n",
      "/* 029 */   }\n",
      "/* 030 */\n",
      "/* 031 */   protected void processNext() throws java.io.IOException {\n",
      "/* 032 */     while ( scan_input_0.hasNext()) {\n",
      "/* 033 */       InternalRow scan_row_0 = (InternalRow) scan_input_0.next();\n",
      "/* 034 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 035 */       // common sub-expressions\n",
      "/* 036 */\n",
      "/* 037 */       boolean scan_isNull_0 = scan_row_0.isNullAt(0);\n",
      "/* 038 */       int scan_value_0 = scan_isNull_0 ?\n",
      "/* 039 */       -1 : (scan_row_0.getInt(0));\n",
      "/* 040 */       UTF8String project_value_0;\n",
      "/* 041 */       if (scan_isNull_0) {\n",
      "/* 042 */         project_value_0 = UTF8String.fromString(\"NULL\");\n",
      "/* 043 */       } else {\n",
      "/* 044 */         project_value_0 = UTF8String.fromString(String.valueOf(scan_value_0));\n",
      "/* 045 */       }\n",
      "/* 046 */       project_resultIsNull_0 = false;\n",
      "/* 047 */\n",
      "/* 048 */       if (!project_resultIsNull_0) {\n",
      "/* 049 */         boolean scan_isNull_1 = scan_row_0.isNullAt(1);\n",
      "/* 050 */         UTF8String scan_value_1 = scan_isNull_1 ?\n",
      "/* 051 */         null : (scan_row_0.getUTF8String(1));\n",
      "/* 052 */         project_resultIsNull_0 = scan_isNull_1;\n",
      "/* 053 */         project_mutableStateArray_0[0] = scan_value_1;\n",
      "/* 054 */       }\n",
      "/* 055 */\n",
      "/* 056 */       if (!project_resultIsNull_0) {\n",
      "/* 057 */         project_argValue_0 = 25;\n",
      "/* 058 */       }\n",
      "/* 059 */\n",
      "/* 060 */       boolean project_isNull_3 = project_resultIsNull_0;\n",
      "/* 061 */       UTF8String project_value_3 = null;\n",
      "/* 062 */       if (!project_resultIsNull_0) {\n",
      "/* 063 */         project_value_3 = org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils.readSidePadding(project_mutableStateArray_0[0], project_argValue_0);\n",
      "/* 064 */       }\n",
      "/* 065 */       UTF8String project_value_2;\n",
      "/* 066 */       if (project_isNull_3) {\n",
      "/* 067 */         project_value_2 = UTF8String.fromString(\"NULL\");\n",
      "/* 068 */       } else {\n",
      "/* 069 */         project_value_2 = project_value_3;\n",
      "/* 070 */       }\n",
      "/* 071 */       boolean scan_isNull_2 = scan_row_0.isNullAt(2);\n",
      "/* 072 */       UTF8String scan_value_2 = scan_isNull_2 ?\n",
      "/* 073 */       null : (scan_row_0.getUTF8String(2));\n",
      "/* 074 */       UTF8String project_value_6;\n",
      "/* 075 */       if (scan_isNull_2) {\n",
      "/* 076 */         project_value_6 = UTF8String.fromString(\"NULL\");\n",
      "/* 077 */       } else {\n",
      "/* 078 */         project_value_6 = scan_value_2;\n",
      "/* 079 */       }\n",
      "/* 080 */       boolean scan_isNull_3 = scan_row_0.isNullAt(3);\n",
      "/* 081 */       long scan_value_3 = scan_isNull_3 ?\n",
      "/* 082 */       -1L : (scan_row_0.getLong(3));\n",
      "/* 083 */       UTF8String project_value_8;\n",
      "/* 084 */       if (scan_isNull_3) {\n",
      "/* 085 */         project_value_8 = UTF8String.fromString(\"NULL\");\n",
      "/* 086 */       } else {\n",
      "/* 087 */         project_value_8 = UTF8String.fromString(String.valueOf(scan_value_3));\n",
      "/* 088 */       }\n",
      "/* 089 */       project_resultIsNull_1 = false;\n",
      "/* 090 */\n",
      "/* 091 */       if (!project_resultIsNull_1) {\n",
      "/* 092 */         boolean scan_isNull_4 = scan_row_0.isNullAt(4);\n",
      "/* 093 */         UTF8String scan_value_4 = scan_isNull_4 ?\n",
      "/* 094 */         null : (scan_row_0.getUTF8String(4));\n",
      "/* 095 */         project_resultIsNull_1 = scan_isNull_4;\n",
      "/* 096 */         project_mutableStateArray_0[1] = scan_value_4;\n",
      "/* 097 */       }\n",
      "/* 098 */\n",
      "/* 099 */       if (!project_resultIsNull_1) {\n",
      "/* 100 */         project_argValue_1 = 15;\n",
      "/* 101 */       }\n",
      "/* 102 */\n",
      "/* 103 */       boolean project_isNull_11 = project_resultIsNull_1;\n",
      "/* 104 */       UTF8String project_value_11 = null;\n",
      "/* 105 */       if (!project_resultIsNull_1) {\n",
      "/* 106 */         project_value_11 = org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils.readSidePadding(project_mutableStateArray_0[1], project_argValue_1);\n",
      "/* 107 */       }\n",
      "/* 108 */       UTF8String project_value_10;\n",
      "/* 109 */       if (project_isNull_11) {\n",
      "/* 110 */         project_value_10 = UTF8String.fromString(\"NULL\");\n",
      "/* 111 */       } else {\n",
      "/* 112 */         project_value_10 = project_value_11;\n",
      "/* 113 */       }\n",
      "/* 114 */       boolean scan_isNull_5 = scan_row_0.isNullAt(5);\n",
      "/* 115 */       Decimal scan_value_5 = scan_isNull_5 ?\n",
      "/* 116 */       null : (scan_row_0.getDecimal(5, 38, 18));\n",
      "/* 117 */       UTF8String project_value_14;\n",
      "/* 118 */       if (scan_isNull_5) {\n",
      "/* 119 */         project_value_14 = UTF8String.fromString(\"NULL\");\n",
      "/* 120 */       } else {\n",
      "/* 121 */         project_value_14 = UTF8String.fromString(scan_value_5.toPlainString());\n",
      "/* 122 */       }\n",
      "/* 123 */       boolean scan_isNull_6 = scan_row_0.isNullAt(6);\n",
      "/* 124 */       UTF8String scan_value_6 = scan_isNull_6 ?\n",
      "/* 125 */       null : (scan_row_0.getUTF8String(6));\n",
      "/* 126 */       UTF8String project_value_16;\n",
      "/* 127 */       if (scan_isNull_6) {\n",
      "/* 128 */         project_value_16 = UTF8String.fromString(\"NULL\");\n",
      "/* 129 */       } else {\n",
      "/* 130 */         project_value_16 = scan_value_6;\n",
      "/* 131 */       }\n",
      "/* 132 */       scan_mutableStateArray_0[1].reset();\n",
      "/* 133 */\n",
      "/* 134 */       scan_mutableStateArray_0[1].write(0, project_value_0);\n",
      "/* 135 */\n",
      "/* 136 */       scan_mutableStateArray_0[1].write(1, project_value_2);\n",
      "/* 137 */\n",
      "/* 138 */       scan_mutableStateArray_0[1].write(2, project_value_6);\n",
      "/* 139 */\n",
      "/* 140 */       scan_mutableStateArray_0[1].write(3, project_value_8);\n",
      "/* 141 */\n",
      "/* 142 */       scan_mutableStateArray_0[1].write(4, project_value_10);\n",
      "/* 143 */\n",
      "/* 144 */       scan_mutableStateArray_0[1].write(5, project_value_14);\n",
      "/* 145 */\n",
      "/* 146 */       scan_mutableStateArray_0[1].write(6, project_value_16);\n",
      "/* 147 */       append((scan_mutableStateArray_0[1].getRow()));\n",
      "/* 148 */       if (shouldStop()) return;\n",
      "/* 149 */     }\n",
      "/* 150 */   }\n",
      "/* 151 */\n",
      "/* 152 */ }\n",
      "\n",
      "23/10/03 12:21:18 INFO CodeGenerator: Code generated in 13.468723 ms\n",
      "23/10/03 12:21:18 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "23/10/03 12:21:18 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "23/10/03 12:21:18 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$executeTake$2\n",
      "23/10/03 12:21:18 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++\n",
      "23/10/03 12:21:18 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "23/10/03 12:21:18 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "23/10/03 12:21:18 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/10/03 12:21:18 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 8 took 0.000061 seconds\n",
      "23/10/03 12:21:18 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "23/10/03 12:21:18 INFO DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/10/03 12:21:18 INFO DAGScheduler: Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/10/03 12:21:18 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/10/03 12:21:18 INFO DAGScheduler: Missing parents: List()\n",
      "23/10/03 12:21:18 DEBUG DAGScheduler: submitStage(ResultStage 2 (name=showString at NativeMethodAccessorImpl.java:0;jobs=2))\n",
      "23/10/03 12:21:18 DEBUG DAGScheduler: missing: List()\n",
      "23/10/03 12:21:18 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[8] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/10/03 12:21:18 DEBUG DAGScheduler: submitMissingTasks(ResultStage 2)\n",
      "23/10/03 12:21:18 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 14.7 KiB, free 26.8 GiB)\n",
      "23/10/03 12:21:18 DEBUG BlockManager: Put block broadcast_2 locally took 0 ms\n",
      "23/10/03 12:21:18 DEBUG BlockManager: Putting block broadcast_2 without replication took 1 ms\n",
      "23/10/03 12:21:18 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 26.8 GiB)\n",
      "23/10/03 12:21:18 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, asusbc-rtl8117.lan, 34663, None)\n",
      "23/10/03 12:21:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on asusbc-rtl8117.lan:34663 (size: 7.0 KiB, free: 26.8 GiB)\n",
      "23/10/03 12:21:18 DEBUG BlockManagerMaster: Updated info of block broadcast_2_piece0\n",
      "23/10/03 12:21:18 DEBUG BlockManager: Told master about block broadcast_2_piece0\n",
      "23/10/03 12:21:18 DEBUG BlockManager: Put block broadcast_2_piece0 locally took 0 ms\n",
      "23/10/03 12:21:18 DEBUG BlockManager: Putting block broadcast_2_piece0 without replication took 0 ms\n",
      "23/10/03 12:21:18 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580\n",
      "23/10/03 12:21:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/10/03 12:21:18 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "23/10/03 12:21:18 DEBUG TaskSetManager: Epoch for TaskSet 2.0: 0\n",
      "23/10/03 12:21:18 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "23/10/03 12:21:18 DEBUG TaskSetManager: Valid locality levels for TaskSet 2.0: NO_PREF, ANY\n",
      "23/10/03 12:21:18 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_2.0, runningTasks: 0\n",
      "23/10/03 12:21:18 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (asusbc-rtl8117.lan, executor driver, partition 0, PROCESS_LOCAL, 7647 bytes) \n",
      "23/10/03 12:21:18 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "23/10/03 12:21:18 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
      "23/10/03 12:21:18 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 1\n",
      "23/10/03 12:21:18 DEBUG BlockManager: Getting local block broadcast_2\n",
      "23/10/03 12:21:18 DEBUG BlockManager: Level for block broadcast_2 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "23/10/03 12:21:18 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:18 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:18 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:18 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:18 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:18 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:18 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:18 DEBUG BasicConnectionProvider: JDBC connection initiated with URL: jdbc:postgresql://localhost:5432/tpch and properties: {password=tpch, user=tpch}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+----------+--------------------+------+-----------+--------------------+--------------------+\n",
      "|p_partkey|              p_name|              p_mfgr|   p_brand|              p_type|p_size|p_container|       p_retailprice|           p_comment|\n",
      "+---------+--------------------+--------------------+----------+--------------------+------+-----------+--------------------+--------------------+\n",
      "|  2199961|forest rose blanc...|Manufacturer#5   ...|Brand#52  |ECONOMY POLISHED ...|    33| JUMBO CAN |2060.860000000000...|          nstruction|\n",
      "|  2199962|peach lavender th...|Manufacturer#4   ...|Brand#45  |MEDIUM BURNISHED ...|    22| LG PACK   |2061.860000000000...|ending ideas cajo...|\n",
      "|  2199963|pink dark azure o...|Manufacturer#1   ...|Brand#15  |MEDIUM PLATED NICKEL|    24| LG BAG    |2062.860000000000...|              regula|\n",
      "|  2199964|smoke blanched gr...|Manufacturer#1   ...|Brand#11  |SMALL BURNISHED N...|    12| WRAP DRUM |2063.860000000000...|     old accounts ar|\n",
      "|  2199965|sienna hot thistl...|Manufacturer#5   ...|Brand#55  |ECONOMY ANODIZED ...|     8| WRAP JAR  |2064.860000000000...|s accounts about ...|\n",
      "|  2199966|slate floral plum...|Manufacturer#1   ...|Brand#14  |  LARGE POLISHED TIN|     3| LG PACK   |2065.860000000000...|       s sleep final|\n",
      "|  2199967|coral drab powder...|Manufacturer#2   ...|Brand#24  |ECONOMY POLISHED ...|    41| LG CASE   |2066.860000000000...|     ctions during t|\n",
      "|  2199968|cornflower honeyd...|Manufacturer#2   ...|Brand#23  |SMALL POLISHED STEEL|     2| LG BAG    |2067.860000000000...|carefully ironic pac|\n",
      "|  2199969|puff ivory lemon ...|Manufacturer#2   ...|Brand#23  |MEDIUM ANODIZED N...|     7| WRAP CASE |2068.860000000000...|         d instructi|\n",
      "|  2199970|drab coral froste...|Manufacturer#4   ...|Brand#44  | PROMO BRUSHED STEEL|    37| LG PACK   |2069.870000000000...|         wake. blith|\n",
      "|  2199971|pale dodger wheat...|Manufacturer#1   ...|Brand#15  | SMALL BURNISHED TIN|    44| MED DRUM  |2070.870000000000...|riously unusual a...|\n",
      "|  2199972|sandy lawn orchid...|Manufacturer#1   ...|Brand#14  |STANDARD BURNISHE...|    49| LG DRUM   |2071.870000000000...|            rbits af|\n",
      "|  2199973|powder plum blanc...|Manufacturer#2   ...|Brand#25  |   LARGE BRUSHED TIN|    34| JUMBO JAR |2072.870000000000...|             sly spe|\n",
      "|  2199974|chartreuse gainsb...|Manufacturer#2   ...|Brand#23  |ECONOMY BRUSHED C...|    44| JUMBO CAN |2073.870000000000...|    carefully specia|\n",
      "|  2199975|brown plum orange...|Manufacturer#2   ...|Brand#25  | PROMO BRUSHED STEEL|    12| SM PACK   |2074.870000000000...|               somet|\n",
      "|  2199976|rosy white drab v...|Manufacturer#2   ...|Brand#23  |PROMO ANODIZED CO...|    31| LG CASE   |2075.870000000000...|            ze about|\n",
      "|  2199977|sandy red pale wh...|Manufacturer#2   ...|Brand#24  |MEDIUM BURNISHED TIN|    46| SM DRUM   |2076.870000000000...|       s. regular, u|\n",
      "|  2199978|royal sandy mediu...|Manufacturer#3   ...|Brand#35  |ECONOMY ANODIZED ...|    10| JUMBO CAN |2077.870000000000...|           foxes cou|\n",
      "|  2199979|forest linen rosy...|Manufacturer#1   ...|Brand#11  | SMALL PLATED NICKEL|    49| WRAP DRUM |2078.870000000000...|usual deposits bo...|\n",
      "|  2199980|peru blush almond...|Manufacturer#2   ...|Brand#25  |  LARGE ANODIZED TIN|    40| MED PACK  |2079.880000000000...|               fluff|\n",
      "+---------+--------------------+--------------------+----------+--------------------+------+-----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "supplier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/03 12:21:18 INFO JDBCRDD: closed connection\n",
      "23/10/03 12:21:18 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 4371 bytes result sent to driver\n",
      "23/10/03 12:21:18 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 0\n",
      "23/10/03 12:21:18 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 125 ms on asusbc-rtl8117.lan (executor driver) (1/1)\n",
      "23/10/03 12:21:18 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "23/10/03 12:21:18 INFO DAGScheduler: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 0.131 s\n",
      "23/10/03 12:21:18 DEBUG DAGScheduler: After removal of stage 2, remaining stages = 0\n",
      "23/10/03 12:21:18 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/10/03 12:21:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "23/10/03 12:21:18 INFO DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.134930 s\n",
      "23/10/03 12:21:18 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, false].toString, input[1, string, false].toString, input[2, string, false].toString, input[3, string, false].toString, input[4, string, false].toString, input[5, string, false].toString, input[6, string, false].toString, StructField(toprettystring(s_suppkey),StringType,false), StructField(toprettystring(s_name),StringType,false), StructField(toprettystring(s_address),StringType,false), StructField(toprettystring(s_nationkey),StringType,false), StructField(toprettystring(s_phone),StringType,false), StructField(toprettystring(s_acctbal),StringType,false), StructField(toprettystring(s_comment),StringType,false)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[7];\n",
      "/* 024 */     createExternalRow_0_0(i, values_0);\n",
      "/* 025 */     createExternalRow_0_1(i, values_0);\n",
      "/* 026 */     createExternalRow_0_2(i, values_0);\n",
      "/* 027 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 028 */     if (false) {\n",
      "/* 029 */       mutableRow.setNullAt(0);\n",
      "/* 030 */     } else {\n",
      "/* 031 */\n",
      "/* 032 */       mutableRow.update(0, value_0);\n",
      "/* 033 */     }\n",
      "/* 034 */\n",
      "/* 035 */     return mutableRow;\n",
      "/* 036 */   }\n",
      "/* 037 */\n",
      "/* 038 */\n",
      "/* 039 */   private void createExternalRow_0_2(InternalRow i, Object[] values_0) {\n",
      "/* 040 */\n",
      "/* 041 */     UTF8String value_14 = i.getUTF8String(6);\n",
      "/* 042 */     boolean isNull_13 = true;\n",
      "/* 043 */     java.lang.String value_13 = null;\n",
      "/* 044 */     isNull_13 = false;\n",
      "/* 045 */     if (!isNull_13) {\n",
      "/* 046 */\n",
      "/* 047 */       Object funcResult_6 = null;\n",
      "/* 048 */       funcResult_6 = value_14.toString();\n",
      "/* 049 */       value_13 = (java.lang.String) funcResult_6;\n",
      "/* 050 */\n",
      "/* 051 */     }\n",
      "/* 052 */     if (isNull_13) {\n",
      "/* 053 */       values_0[6] = null;\n",
      "/* 054 */     } else {\n",
      "/* 055 */       values_0[6] = value_13;\n",
      "/* 056 */     }\n",
      "/* 057 */\n",
      "/* 058 */   }\n",
      "/* 059 */\n",
      "/* 060 */\n",
      "/* 061 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {\n",
      "/* 062 */\n",
      "/* 063 */     UTF8String value_8 = i.getUTF8String(3);\n",
      "/* 064 */     boolean isNull_7 = true;\n",
      "/* 065 */     java.lang.String value_7 = null;\n",
      "/* 066 */     isNull_7 = false;\n",
      "/* 067 */     if (!isNull_7) {\n",
      "/* 068 */\n",
      "/* 069 */       Object funcResult_3 = null;\n",
      "/* 070 */       funcResult_3 = value_8.toString();\n",
      "/* 071 */       value_7 = (java.lang.String) funcResult_3;\n",
      "/* 072 */\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_7) {\n",
      "/* 075 */       values_0[3] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[3] = value_7;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     UTF8String value_10 = i.getUTF8String(4);\n",
      "/* 081 */     boolean isNull_9 = true;\n",
      "/* 082 */     java.lang.String value_9 = null;\n",
      "/* 083 */     isNull_9 = false;\n",
      "/* 084 */     if (!isNull_9) {\n",
      "/* 085 */\n",
      "/* 086 */       Object funcResult_4 = null;\n",
      "/* 087 */       funcResult_4 = value_10.toString();\n",
      "/* 088 */       value_9 = (java.lang.String) funcResult_4;\n",
      "/* 089 */\n",
      "/* 090 */     }\n",
      "/* 091 */     if (isNull_9) {\n",
      "/* 092 */       values_0[4] = null;\n",
      "/* 093 */     } else {\n",
      "/* 094 */       values_0[4] = value_9;\n",
      "/* 095 */     }\n",
      "/* 096 */\n",
      "/* 097 */     UTF8String value_12 = i.getUTF8String(5);\n",
      "/* 098 */     boolean isNull_11 = true;\n",
      "/* 099 */     java.lang.String value_11 = null;\n",
      "/* 100 */     isNull_11 = false;\n",
      "/* 101 */     if (!isNull_11) {\n",
      "/* 102 */\n",
      "/* 103 */       Object funcResult_5 = null;\n",
      "/* 104 */       funcResult_5 = value_12.toString();\n",
      "/* 105 */       value_11 = (java.lang.String) funcResult_5;\n",
      "/* 106 */\n",
      "/* 107 */     }\n",
      "/* 108 */     if (isNull_11) {\n",
      "/* 109 */       values_0[5] = null;\n",
      "/* 110 */     } else {\n",
      "/* 111 */       values_0[5] = value_11;\n",
      "/* 112 */     }\n",
      "/* 113 */\n",
      "/* 114 */   }\n",
      "/* 115 */\n",
      "/* 116 */\n",
      "/* 117 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {\n",
      "/* 118 */\n",
      "/* 119 */     UTF8String value_2 = i.getUTF8String(0);\n",
      "/* 120 */     boolean isNull_1 = true;\n",
      "/* 121 */     java.lang.String value_1 = null;\n",
      "/* 122 */     isNull_1 = false;\n",
      "/* 123 */     if (!isNull_1) {\n",
      "/* 124 */\n",
      "/* 125 */       Object funcResult_0 = null;\n",
      "/* 126 */       funcResult_0 = value_2.toString();\n",
      "/* 127 */       value_1 = (java.lang.String) funcResult_0;\n",
      "/* 128 */\n",
      "/* 129 */     }\n",
      "/* 130 */     if (isNull_1) {\n",
      "/* 131 */       values_0[0] = null;\n",
      "/* 132 */     } else {\n",
      "/* 133 */       values_0[0] = value_1;\n",
      "/* 134 */     }\n",
      "/* 135 */\n",
      "/* 136 */     UTF8String value_4 = i.getUTF8String(1);\n",
      "/* 137 */     boolean isNull_3 = true;\n",
      "/* 138 */     java.lang.String value_3 = null;\n",
      "/* 139 */     isNull_3 = false;\n",
      "/* 140 */     if (!isNull_3) {\n",
      "/* 141 */\n",
      "/* 142 */       Object funcResult_1 = null;\n",
      "/* 143 */       funcResult_1 = value_4.toString();\n",
      "/* 144 */       value_3 = (java.lang.String) funcResult_1;\n",
      "/* 145 */\n",
      "/* 146 */     }\n",
      "/* 147 */     if (isNull_3) {\n",
      "/* 148 */       values_0[1] = null;\n",
      "/* 149 */     } else {\n",
      "/* 150 */       values_0[1] = value_3;\n",
      "/* 151 */     }\n",
      "/* 152 */\n",
      "/* 153 */     UTF8String value_6 = i.getUTF8String(2);\n",
      "/* 154 */     boolean isNull_5 = true;\n",
      "/* 155 */     java.lang.String value_5 = null;\n",
      "/* 156 */     isNull_5 = false;\n",
      "/* 157 */     if (!isNull_5) {\n",
      "/* 158 */\n",
      "/* 159 */       Object funcResult_2 = null;\n",
      "/* 160 */       funcResult_2 = value_6.toString();\n",
      "/* 161 */       value_5 = (java.lang.String) funcResult_2;\n",
      "/* 162 */\n",
      "/* 163 */     }\n",
      "/* 164 */     if (isNull_5) {\n",
      "/* 165 */       values_0[2] = null;\n",
      "/* 166 */     } else {\n",
      "/* 167 */       values_0[2] = value_5;\n",
      "/* 168 */     }\n",
      "/* 169 */\n",
      "/* 170 */   }\n",
      "/* 171 */\n",
      "/* 172 */ }\n",
      "\n",
      "23/10/03 12:21:18 DEBUG CodeGenerator: \n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[7];\n",
      "/* 024 */     createExternalRow_0_0(i, values_0);\n",
      "/* 025 */     createExternalRow_0_1(i, values_0);\n",
      "/* 026 */     createExternalRow_0_2(i, values_0);\n",
      "/* 027 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 028 */     if (false) {\n",
      "/* 029 */       mutableRow.setNullAt(0);\n",
      "/* 030 */     } else {\n",
      "/* 031 */\n",
      "/* 032 */       mutableRow.update(0, value_0);\n",
      "/* 033 */     }\n",
      "/* 034 */\n",
      "/* 035 */     return mutableRow;\n",
      "/* 036 */   }\n",
      "/* 037 */\n",
      "/* 038 */\n",
      "/* 039 */   private void createExternalRow_0_2(InternalRow i, Object[] values_0) {\n",
      "/* 040 */\n",
      "/* 041 */     UTF8String value_14 = i.getUTF8String(6);\n",
      "/* 042 */     boolean isNull_13 = true;\n",
      "/* 043 */     java.lang.String value_13 = null;\n",
      "/* 044 */     isNull_13 = false;\n",
      "/* 045 */     if (!isNull_13) {\n",
      "/* 046 */\n",
      "/* 047 */       Object funcResult_6 = null;\n",
      "/* 048 */       funcResult_6 = value_14.toString();\n",
      "/* 049 */       value_13 = (java.lang.String) funcResult_6;\n",
      "/* 050 */\n",
      "/* 051 */     }\n",
      "/* 052 */     if (isNull_13) {\n",
      "/* 053 */       values_0[6] = null;\n",
      "/* 054 */     } else {\n",
      "/* 055 */       values_0[6] = value_13;\n",
      "/* 056 */     }\n",
      "/* 057 */\n",
      "/* 058 */   }\n",
      "/* 059 */\n",
      "/* 060 */\n",
      "/* 061 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {\n",
      "/* 062 */\n",
      "/* 063 */     UTF8String value_8 = i.getUTF8String(3);\n",
      "/* 064 */     boolean isNull_7 = true;\n",
      "/* 065 */     java.lang.String value_7 = null;\n",
      "/* 066 */     isNull_7 = false;\n",
      "/* 067 */     if (!isNull_7) {\n",
      "/* 068 */\n",
      "/* 069 */       Object funcResult_3 = null;\n",
      "/* 070 */       funcResult_3 = value_8.toString();\n",
      "/* 071 */       value_7 = (java.lang.String) funcResult_3;\n",
      "/* 072 */\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_7) {\n",
      "/* 075 */       values_0[3] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[3] = value_7;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     UTF8String value_10 = i.getUTF8String(4);\n",
      "/* 081 */     boolean isNull_9 = true;\n",
      "/* 082 */     java.lang.String value_9 = null;\n",
      "/* 083 */     isNull_9 = false;\n",
      "/* 084 */     if (!isNull_9) {\n",
      "/* 085 */\n",
      "/* 086 */       Object funcResult_4 = null;\n",
      "/* 087 */       funcResult_4 = value_10.toString();\n",
      "/* 088 */       value_9 = (java.lang.String) funcResult_4;\n",
      "/* 089 */\n",
      "/* 090 */     }\n",
      "/* 091 */     if (isNull_9) {\n",
      "/* 092 */       values_0[4] = null;\n",
      "/* 093 */     } else {\n",
      "/* 094 */       values_0[4] = value_9;\n",
      "/* 095 */     }\n",
      "/* 096 */\n",
      "/* 097 */     UTF8String value_12 = i.getUTF8String(5);\n",
      "/* 098 */     boolean isNull_11 = true;\n",
      "/* 099 */     java.lang.String value_11 = null;\n",
      "/* 100 */     isNull_11 = false;\n",
      "/* 101 */     if (!isNull_11) {\n",
      "/* 102 */\n",
      "/* 103 */       Object funcResult_5 = null;\n",
      "/* 104 */       funcResult_5 = value_12.toString();\n",
      "/* 105 */       value_11 = (java.lang.String) funcResult_5;\n",
      "/* 106 */\n",
      "/* 107 */     }\n",
      "/* 108 */     if (isNull_11) {\n",
      "/* 109 */       values_0[5] = null;\n",
      "/* 110 */     } else {\n",
      "/* 111 */       values_0[5] = value_11;\n",
      "/* 112 */     }\n",
      "/* 113 */\n",
      "/* 114 */   }\n",
      "/* 115 */\n",
      "/* 116 */\n",
      "/* 117 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {\n",
      "/* 118 */\n",
      "/* 119 */     UTF8String value_2 = i.getUTF8String(0);\n",
      "/* 120 */     boolean isNull_1 = true;\n",
      "/* 121 */     java.lang.String value_1 = null;\n",
      "/* 122 */     isNull_1 = false;\n",
      "/* 123 */     if (!isNull_1) {\n",
      "/* 124 */\n",
      "/* 125 */       Object funcResult_0 = null;\n",
      "/* 126 */       funcResult_0 = value_2.toString();\n",
      "/* 127 */       value_1 = (java.lang.String) funcResult_0;\n",
      "/* 128 */\n",
      "/* 129 */     }\n",
      "/* 130 */     if (isNull_1) {\n",
      "/* 131 */       values_0[0] = null;\n",
      "/* 132 */     } else {\n",
      "/* 133 */       values_0[0] = value_1;\n",
      "/* 134 */     }\n",
      "/* 135 */\n",
      "/* 136 */     UTF8String value_4 = i.getUTF8String(1);\n",
      "/* 137 */     boolean isNull_3 = true;\n",
      "/* 138 */     java.lang.String value_3 = null;\n",
      "/* 139 */     isNull_3 = false;\n",
      "/* 140 */     if (!isNull_3) {\n",
      "/* 141 */\n",
      "/* 142 */       Object funcResult_1 = null;\n",
      "/* 143 */       funcResult_1 = value_4.toString();\n",
      "/* 144 */       value_3 = (java.lang.String) funcResult_1;\n",
      "/* 145 */\n",
      "/* 146 */     }\n",
      "/* 147 */     if (isNull_3) {\n",
      "/* 148 */       values_0[1] = null;\n",
      "/* 149 */     } else {\n",
      "/* 150 */       values_0[1] = value_3;\n",
      "/* 151 */     }\n",
      "/* 152 */\n",
      "/* 153 */     UTF8String value_6 = i.getUTF8String(2);\n",
      "/* 154 */     boolean isNull_5 = true;\n",
      "/* 155 */     java.lang.String value_5 = null;\n",
      "/* 156 */     isNull_5 = false;\n",
      "/* 157 */     if (!isNull_5) {\n",
      "/* 158 */\n",
      "/* 159 */       Object funcResult_2 = null;\n",
      "/* 160 */       funcResult_2 = value_6.toString();\n",
      "/* 161 */       value_5 = (java.lang.String) funcResult_2;\n",
      "/* 162 */\n",
      "/* 163 */     }\n",
      "/* 164 */     if (isNull_5) {\n",
      "/* 165 */       values_0[2] = null;\n",
      "/* 166 */     } else {\n",
      "/* 167 */       values_0[2] = value_5;\n",
      "/* 168 */     }\n",
      "/* 169 */\n",
      "/* 170 */   }\n",
      "/* 171 */\n",
      "/* 172 */ }\n",
      "\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(47)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 47\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 47\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(38)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 38\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 38\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(80)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 80\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 80\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(44)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 44\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 44\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(86)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 86\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 86\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(51)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 51\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 51\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(67)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 67\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 67\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(63)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 63\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 63\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(42)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 42\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 42\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(72)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 72\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 72\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(61)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 61\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 61\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(40)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 40\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 40\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(56)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 56\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 56\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(84)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 84\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 84\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(41)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 41\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 41\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(55)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 55\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 55\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(85)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 85\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 85\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(53)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 53\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 53\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(68)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 68\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 68\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(90)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 90\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 90\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(69)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 69\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 69\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(58)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 58\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 58\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(65)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 65\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 65\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(64)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 64\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 64\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(88)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 88\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 88\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(91)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 91\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 91\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(89)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 89\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 89\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(48)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 48\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 48\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(76)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 76\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 76\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(92)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 92\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 92\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(74)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 74\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 74\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(60)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 60\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 60\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(62)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 62\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 62\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(54)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 54\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 54\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(49)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 49\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 49\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(71)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 71\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 71\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(46)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 46\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 46\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(66)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 66\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 66\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(45)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 45\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 45\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(73)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 73\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 73\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(43)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 43\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 43\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(81)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 81\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 81\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(75)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 75\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 75\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(79)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 79\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 79\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(57)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 57\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 57\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(82)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 82\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 82\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(52)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 52\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 52\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(39)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 39\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 39\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(87)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 87\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 87\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(94)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 94\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 94\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(50)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 50\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 50\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(78)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 78\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 78\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(70)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 70\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 70\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(1)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning broadcast 1\n",
      "23/10/03 12:21:18 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 1\n",
      "23/10/03 12:21:18 DEBUG BlockManagerStorageEndpoint: removing broadcast 1\n",
      "23/10/03 12:21:18 DEBUG BlockManager: Removing broadcast 1\n",
      "23/10/03 12:21:18 DEBUG BlockManager: Removing block broadcast_1\n",
      "23/10/03 12:21:18 DEBUG MemoryStore: Block broadcast_1 of size 16360 dropped from memory (free 28802255977)\n",
      "23/10/03 12:21:18 DEBUG BlockManager: Removing block broadcast_1_piece0\n",
      "23/10/03 12:21:18 DEBUG MemoryStore: Block broadcast_1_piece0 of size 7408 dropped from memory (free 28802263385)\n",
      "23/10/03 12:21:18 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, asusbc-rtl8117.lan, 34663, None)\n",
      "23/10/03 12:21:18 INFO BlockManagerInfo: Removed broadcast_1_piece0 on asusbc-rtl8117.lan:34663 in memory (size: 7.2 KiB, free: 26.8 GiB)\n",
      "23/10/03 12:21:18 DEBUG BlockManagerMaster: Updated info of block broadcast_1_piece0\n",
      "23/10/03 12:21:18 DEBUG BlockManager: Told master about block broadcast_1_piece0\n",
      "23/10/03 12:21:18 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 1, response is 0\n",
      "23/10/03 12:21:18 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to asusbc-rtl8117.lan:44641\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned broadcast 1\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(77)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 77\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 77\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(83)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 83\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 83\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(59)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 59\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 59\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Got cleaning task CleanAccum(93)\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaning accumulator 93\n",
      "23/10/03 12:21:18 DEBUG ContextCleaner: Cleaned accumulator 93\n",
      "23/10/03 12:21:18 INFO CodeGenerator: Code generated in 15.353741 ms\n",
      "23/10/03 12:21:18 DEBUG SparkSqlParser: Parsing command: supplier\n",
      "23/10/03 12:21:18 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:18 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:18 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:18 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:18 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:18 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:18 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:18 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:18 DEBUG BasicConnectionProvider: JDBC connection initiated with URL: jdbc:postgresql://localhost:5432/tpch and properties: {password=tpch, user=tpch}\n",
      "23/10/03 12:21:18 DEBUG CatalystSqlParser: Parsing command: varchar(199)\n",
      "23/10/03 12:21:18 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator scan_input_0;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] scan_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 011 */\n",
      "/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 013 */     this.references = references;\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 017 */     partitionIndex = index;\n",
      "/* 018 */     this.inputs = inputs;\n",
      "/* 019 */     scan_input_0 = inputs[0];\n",
      "/* 020 */     scan_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(5, 64);\n",
      "/* 021 */     scan_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(5, 160);\n",
      "/* 022 */\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   protected void processNext() throws java.io.IOException {\n",
      "/* 026 */     while ( scan_input_0.hasNext()) {\n",
      "/* 027 */       InternalRow scan_row_0 = (InternalRow) scan_input_0.next();\n",
      "/* 028 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 029 */       // common sub-expressions\n",
      "/* 030 */\n",
      "/* 031 */       boolean scan_isNull_0 = scan_row_0.isNullAt(0);\n",
      "/* 032 */       long scan_value_0 = scan_isNull_0 ?\n",
      "/* 033 */       -1L : (scan_row_0.getLong(0));\n",
      "/* 034 */       UTF8String project_value_0;\n",
      "/* 035 */       if (scan_isNull_0) {\n",
      "/* 036 */         project_value_0 = UTF8String.fromString(\"NULL\");\n",
      "/* 037 */       } else {\n",
      "/* 038 */         project_value_0 = UTF8String.fromString(String.valueOf(scan_value_0));\n",
      "/* 039 */       }\n",
      "/* 040 */       boolean scan_isNull_1 = scan_row_0.isNullAt(1);\n",
      "/* 041 */       long scan_value_1 = scan_isNull_1 ?\n",
      "/* 042 */       -1L : (scan_row_0.getLong(1));\n",
      "/* 043 */       UTF8String project_value_2;\n",
      "/* 044 */       if (scan_isNull_1) {\n",
      "/* 045 */         project_value_2 = UTF8String.fromString(\"NULL\");\n",
      "/* 046 */       } else {\n",
      "/* 047 */         project_value_2 = UTF8String.fromString(String.valueOf(scan_value_1));\n",
      "/* 048 */       }\n",
      "/* 049 */       boolean scan_isNull_2 = scan_row_0.isNullAt(2);\n",
      "/* 050 */       int scan_value_2 = scan_isNull_2 ?\n",
      "/* 051 */       -1 : (scan_row_0.getInt(2));\n",
      "/* 052 */       UTF8String project_value_4;\n",
      "/* 053 */       if (scan_isNull_2) {\n",
      "/* 054 */         project_value_4 = UTF8String.fromString(\"NULL\");\n",
      "/* 055 */       } else {\n",
      "/* 056 */         project_value_4 = UTF8String.fromString(String.valueOf(scan_value_2));\n",
      "/* 057 */       }\n",
      "/* 058 */       boolean scan_isNull_3 = scan_row_0.isNullAt(3);\n",
      "/* 059 */       Decimal scan_value_3 = scan_isNull_3 ?\n",
      "/* 060 */       null : (scan_row_0.getDecimal(3, 38, 18));\n",
      "/* 061 */       UTF8String project_value_6;\n",
      "/* 062 */       if (scan_isNull_3) {\n",
      "/* 063 */         project_value_6 = UTF8String.fromString(\"NULL\");\n",
      "/* 064 */       } else {\n",
      "/* 065 */         project_value_6 = UTF8String.fromString(scan_value_3.toPlainString());\n",
      "/* 066 */       }\n",
      "/* 067 */       boolean scan_isNull_4 = scan_row_0.isNullAt(4);\n",
      "/* 068 */       UTF8String scan_value_4 = scan_isNull_4 ?\n",
      "/* 069 */       null : (scan_row_0.getUTF8String(4));\n",
      "/* 070 */       UTF8String project_value_8;\n",
      "/* 071 */       if (scan_isNull_4) {\n",
      "/* 072 */         project_value_8 = UTF8String.fromString(\"NULL\");\n",
      "/* 073 */       } else {\n",
      "/* 074 */         project_value_8 = scan_value_4;\n",
      "/* 075 */       }\n",
      "/* 076 */       scan_mutableStateArray_0[1].reset();\n",
      "/* 077 */\n",
      "/* 078 */       scan_mutableStateArray_0[1].write(0, project_value_0);\n",
      "/* 079 */\n",
      "/* 080 */       scan_mutableStateArray_0[1].write(1, project_value_2);\n",
      "/* 081 */\n",
      "/* 082 */       scan_mutableStateArray_0[1].write(2, project_value_4);\n",
      "/* 083 */\n",
      "/* 084 */       scan_mutableStateArray_0[1].write(3, project_value_6);\n",
      "/* 085 */\n",
      "/* 086 */       scan_mutableStateArray_0[1].write(4, project_value_8);\n",
      "/* 087 */       append((scan_mutableStateArray_0[1].getRow()));\n",
      "/* 088 */       if (shouldStop()) return;\n",
      "/* 089 */     }\n",
      "/* 090 */   }\n",
      "/* 091 */\n",
      "/* 092 */ }\n",
      "\n",
      "23/10/03 12:21:18 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator scan_input_0;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] scan_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 011 */\n",
      "/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 013 */     this.references = references;\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 017 */     partitionIndex = index;\n",
      "/* 018 */     this.inputs = inputs;\n",
      "/* 019 */     scan_input_0 = inputs[0];\n",
      "/* 020 */     scan_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(5, 64);\n",
      "/* 021 */     scan_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(5, 160);\n",
      "/* 022 */\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   protected void processNext() throws java.io.IOException {\n",
      "/* 026 */     while ( scan_input_0.hasNext()) {\n",
      "/* 027 */       InternalRow scan_row_0 = (InternalRow) scan_input_0.next();\n",
      "/* 028 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 029 */       // common sub-expressions\n",
      "/* 030 */\n",
      "/* 031 */       boolean scan_isNull_0 = scan_row_0.isNullAt(0);\n",
      "/* 032 */       long scan_value_0 = scan_isNull_0 ?\n",
      "/* 033 */       -1L : (scan_row_0.getLong(0));\n",
      "/* 034 */       UTF8String project_value_0;\n",
      "/* 035 */       if (scan_isNull_0) {\n",
      "/* 036 */         project_value_0 = UTF8String.fromString(\"NULL\");\n",
      "/* 037 */       } else {\n",
      "/* 038 */         project_value_0 = UTF8String.fromString(String.valueOf(scan_value_0));\n",
      "/* 039 */       }\n",
      "/* 040 */       boolean scan_isNull_1 = scan_row_0.isNullAt(1);\n",
      "/* 041 */       long scan_value_1 = scan_isNull_1 ?\n",
      "/* 042 */       -1L : (scan_row_0.getLong(1));\n",
      "/* 043 */       UTF8String project_value_2;\n",
      "/* 044 */       if (scan_isNull_1) {\n",
      "/* 045 */         project_value_2 = UTF8String.fromString(\"NULL\");\n",
      "/* 046 */       } else {\n",
      "/* 047 */         project_value_2 = UTF8String.fromString(String.valueOf(scan_value_1));\n",
      "/* 048 */       }\n",
      "/* 049 */       boolean scan_isNull_2 = scan_row_0.isNullAt(2);\n",
      "/* 050 */       int scan_value_2 = scan_isNull_2 ?\n",
      "/* 051 */       -1 : (scan_row_0.getInt(2));\n",
      "/* 052 */       UTF8String project_value_4;\n",
      "/* 053 */       if (scan_isNull_2) {\n",
      "/* 054 */         project_value_4 = UTF8String.fromString(\"NULL\");\n",
      "/* 055 */       } else {\n",
      "/* 056 */         project_value_4 = UTF8String.fromString(String.valueOf(scan_value_2));\n",
      "/* 057 */       }\n",
      "/* 058 */       boolean scan_isNull_3 = scan_row_0.isNullAt(3);\n",
      "/* 059 */       Decimal scan_value_3 = scan_isNull_3 ?\n",
      "/* 060 */       null : (scan_row_0.getDecimal(3, 38, 18));\n",
      "/* 061 */       UTF8String project_value_6;\n",
      "/* 062 */       if (scan_isNull_3) {\n",
      "/* 063 */         project_value_6 = UTF8String.fromString(\"NULL\");\n",
      "/* 064 */       } else {\n",
      "/* 065 */         project_value_6 = UTF8String.fromString(scan_value_3.toPlainString());\n",
      "/* 066 */       }\n",
      "/* 067 */       boolean scan_isNull_4 = scan_row_0.isNullAt(4);\n",
      "/* 068 */       UTF8String scan_value_4 = scan_isNull_4 ?\n",
      "/* 069 */       null : (scan_row_0.getUTF8String(4));\n",
      "/* 070 */       UTF8String project_value_8;\n",
      "/* 071 */       if (scan_isNull_4) {\n",
      "/* 072 */         project_value_8 = UTF8String.fromString(\"NULL\");\n",
      "/* 073 */       } else {\n",
      "/* 074 */         project_value_8 = scan_value_4;\n",
      "/* 075 */       }\n",
      "/* 076 */       scan_mutableStateArray_0[1].reset();\n",
      "/* 077 */\n",
      "/* 078 */       scan_mutableStateArray_0[1].write(0, project_value_0);\n",
      "/* 079 */\n",
      "/* 080 */       scan_mutableStateArray_0[1].write(1, project_value_2);\n",
      "/* 081 */\n",
      "/* 082 */       scan_mutableStateArray_0[1].write(2, project_value_4);\n",
      "/* 083 */\n",
      "/* 084 */       scan_mutableStateArray_0[1].write(3, project_value_6);\n",
      "/* 085 */\n",
      "/* 086 */       scan_mutableStateArray_0[1].write(4, project_value_8);\n",
      "/* 087 */       append((scan_mutableStateArray_0[1].getRow()));\n",
      "/* 088 */       if (shouldStop()) return;\n",
      "/* 089 */     }\n",
      "/* 090 */   }\n",
      "/* 091 */\n",
      "/* 092 */ }\n",
      "\n",
      "23/10/03 12:21:18 INFO CodeGenerator: Code generated in 7.796735 ms\n",
      "23/10/03 12:21:18 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "23/10/03 12:21:18 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "23/10/03 12:21:18 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$executeTake$2\n",
      "23/10/03 12:21:18 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++\n",
      "23/10/03 12:21:18 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "23/10/03 12:21:18 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "23/10/03 12:21:18 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/10/03 12:21:18 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 11 took 0.000047 seconds\n",
      "23/10/03 12:21:18 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "23/10/03 12:21:18 INFO DAGScheduler: Got job 3 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/10/03 12:21:18 INFO DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/10/03 12:21:18 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/10/03 12:21:18 INFO DAGScheduler: Missing parents: List()\n",
      "23/10/03 12:21:18 DEBUG DAGScheduler: submitStage(ResultStage 3 (name=showString at NativeMethodAccessorImpl.java:0;jobs=3))\n",
      "23/10/03 12:21:18 DEBUG DAGScheduler: missing: List()\n",
      "23/10/03 12:21:18 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/10/03 12:21:18 DEBUG DAGScheduler: submitMissingTasks(ResultStage 3)\n",
      "23/10/03 12:21:18 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.8 KiB, free 26.8 GiB)\n",
      "23/10/03 12:21:18 DEBUG BlockManager: Put block broadcast_3 locally took 0 ms\n",
      "23/10/03 12:21:18 DEBUG BlockManager: Putting block broadcast_3 without replication took 0 ms\n",
      "23/10/03 12:21:18 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 26.8 GiB)\n",
      "23/10/03 12:21:18 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, asusbc-rtl8117.lan, 34663, None)\n",
      "23/10/03 12:21:18 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on asusbc-rtl8117.lan:34663 (size: 6.3 KiB, free: 26.8 GiB)\n",
      "23/10/03 12:21:18 DEBUG BlockManagerMaster: Updated info of block broadcast_3_piece0\n",
      "23/10/03 12:21:18 DEBUG BlockManager: Told master about block broadcast_3_piece0\n",
      "23/10/03 12:21:18 DEBUG BlockManager: Put block broadcast_3_piece0 locally took 0 ms\n",
      "23/10/03 12:21:18 DEBUG BlockManager: Putting block broadcast_3_piece0 without replication took 0 ms\n",
      "23/10/03 12:21:18 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580\n",
      "23/10/03 12:21:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/10/03 12:21:18 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "23/10/03 12:21:18 DEBUG TaskSetManager: Epoch for TaskSet 3.0: 0\n",
      "23/10/03 12:21:18 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "23/10/03 12:21:18 DEBUG TaskSetManager: Valid locality levels for TaskSet 3.0: NO_PREF, ANY\n",
      "23/10/03 12:21:18 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_3.0, runningTasks: 0\n",
      "23/10/03 12:21:18 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (asusbc-rtl8117.lan, executor driver, partition 0, PROCESS_LOCAL, 7647 bytes) \n",
      "23/10/03 12:21:18 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "23/10/03 12:21:18 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
      "23/10/03 12:21:18 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 1\n",
      "23/10/03 12:21:18 DEBUG BlockManager: Getting local block broadcast_3\n",
      "23/10/03 12:21:18 DEBUG BlockManager: Level for block broadcast_3 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "23/10/03 12:21:18 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:18 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:18 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:18 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:18 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:18 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:18 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:18 DEBUG BasicConnectionProvider: JDBC connection initiated with URL: jdbc:postgresql://localhost:5432/tpch and properties: {password=tpch, user=tpch}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+-----------+---------------+--------------------+--------------------+\n",
      "|s_suppkey|              s_name|           s_address|s_nationkey|        s_phone|           s_acctbal|           s_comment|\n",
      "+---------+--------------------+--------------------+-----------+---------------+--------------------+--------------------+\n",
      "|        1|Supplier#00000000...| N kD4on9OM Ipw3,...|         17|27-918-335-1736|5755.940000000000...|each slyly above ...|\n",
      "|        2|Supplier#00000000...|89eJ5ksX3ImxJQBvx...|          5|15-679-861-2259|4032.680000000000...| slyly bold instr...|\n",
      "|        3|Supplier#00000000...|q1,G3Pj6OjIuUYfUo...|          1|11-383-516-1199|4192.400000000000...|blithely silent r...|\n",
      "|        4|Supplier#00000000...|Bk7ah4CK8SYQTepEm...|         15|25-843-787-7479|4641.080000000000...|riously even requ...|\n",
      "|        5|Supplier#00000000...|   Gcdm2rJRzl5qlTVzc|         11|21-151-690-3663|-283.840000000000...|. slyly regular p...|\n",
      "|        6|Supplier#00000000...|        tQxuVm7s7CnK|         14|24-696-997-4969|1365.790000000000...|final accounts. r...|\n",
      "|        7|Supplier#00000000...|s,4TicNGB4uO6PaSq...|         23|33-990-965-2201|6820.350000000000...|s unwind silently...|\n",
      "|        8|Supplier#00000000...|9Sq4bBH2FQEmaFOoc...|         17|27-498-742-3860|7627.850000000000...|al pinto beans. a...|\n",
      "|        9|Supplier#00000000...|1KhUgZegwM3ua7dsY...|         10|20-403-398-8662|5302.370000000000...|s. unusual, even ...|\n",
      "|       10|Supplier#00000001...|  Saygah3gYWMp72i PY|         24|34-852-489-8585|3891.910000000000...|ing waters. regul...|\n",
      "|       11|Supplier#00000001...|    JfwTs,LZrV, M,9C|         18|28-613-996-1505|3393.080000000000...|y ironic packages...|\n",
      "|       12|Supplier#00000001...|         aLIW  q0HYd|          8|18-179-925-7181|1432.690000000000...|al packages nag a...|\n",
      "|       13|Supplier#00000001...|HK71HQyWoqRWOX8GI...|          3|13-727-620-7813|9107.220000000000...|requests engage r...|\n",
      "|       14|Supplier#00000001...|     EXsnO5pTNj4iZRm|         15|25-656-247-5058|9189.820000000000...|l accounts boost....|\n",
      "|       15|Supplier#00000001...|olXVbNBfVzRqgokr1...|          8|18-453-357-6394|308.5600000000000...| across the furio...|\n",
      "|       16|Supplier#00000001...|YjP5C55zHDXL7LalK...|         22|32-822-502-4215|2972.260000000000...|ously express ide...|\n",
      "|       17|Supplier#00000001...|c2d,ESHRSkK3WYnxp...|         19|29-601-884-9219|1687.810000000000...|eep against the f...|\n",
      "|       18|Supplier#00000001...|    PGGVE5PWAMwKDZw |         16|26-729-551-1115|7040.820000000000...|accounts snooze s...|\n",
      "|       19|Supplier#00000001...|edZT3es,nBFD8lBXT...|         24|34-278-310-2731|6150.380000000000...|refully final fox...|\n",
      "|       20|Supplier#00000002...|iybAE,RmTymrZVYaF...|          3|13-715-945-6730|530.8200000000000...|n, ironic ideas w...|\n",
      "+---------+--------------------+--------------------+-----------+---------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "partsupp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:>                                                          (0 + 1) / 1]\r",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(144)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 144\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 144\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(133)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 133\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 133\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(99)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 99\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 99\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(104)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 104\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 104\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(97)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 97\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 97\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(107)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 107\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 107\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(95)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 95\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 95\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(121)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 121\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 121\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(143)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 143\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 143\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(129)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 129\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 129\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(150)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 150\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 150\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(118)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 118\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 118\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(112)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 112\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 112\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(145)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 145\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 145\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(105)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 105\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 105\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(138)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 138\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 138\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(119)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 119\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 119\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(148)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 148\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 148\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(128)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 128\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 128\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(137)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 137\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 137\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(151)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 151\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 151\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(147)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 147\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 147\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(122)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 122\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 122\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(126)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 126\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 126\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(132)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 132\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 132\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(98)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 98\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 98\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(100)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 100\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 100\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(102)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 102\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 102\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(131)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 131\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 131\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(130)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 130\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 130\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(103)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 103\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 103\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(123)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 123\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 123\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(134)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 134\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 134\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(101)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 101\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 101\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(136)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 136\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 136\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(115)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 115\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 115\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(124)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 124\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 124\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(141)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 141\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 141\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(146)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 146\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 146\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(125)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 125\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 125\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(113)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 113\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 113\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(2)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning broadcast 2\n",
      "23/10/03 12:21:19 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 2\n",
      "23/10/03 12:21:19 DEBUG BlockManagerStorageEndpoint: removing broadcast 2\n",
      "23/10/03 12:21:19 DEBUG BlockManager: Removing broadcast 2\n",
      "23/10/03 12:21:19 DEBUG BlockManager: Removing block broadcast_2_piece0\n",
      "23/10/03 12:21:19 DEBUG MemoryStore: Block broadcast_2_piece0 of size 7135 dropped from memory (free 28802251009)\n",
      "23/10/03 12:21:19 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, asusbc-rtl8117.lan, 34663, None)\n",
      "23/10/03 12:21:19 INFO BlockManagerInfo: Removed broadcast_2_piece0 on asusbc-rtl8117.lan:34663 in memory (size: 7.0 KiB, free: 26.8 GiB)\n",
      "23/10/03 12:21:19 DEBUG BlockManagerMaster: Updated info of block broadcast_2_piece0\n",
      "23/10/03 12:21:19 DEBUG BlockManager: Told master about block broadcast_2_piece0\n",
      "23/10/03 12:21:19 DEBUG BlockManager: Removing block broadcast_2\n",
      "23/10/03 12:21:19 DEBUG MemoryStore: Block broadcast_2 of size 15048 dropped from memory (free 28802266057)\n",
      "23/10/03 12:21:19 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 2, response is 0\n",
      "23/10/03 12:21:19 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to asusbc-rtl8117.lan:44641\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned broadcast 2\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(149)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 149\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 149\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(120)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 120\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 120\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(117)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 117\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 117\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(108)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 108\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 108\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(111)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 111\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 111\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(142)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 142\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 142\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(135)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 135\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 135\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(109)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 109\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 109\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(106)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 106\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 106\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(127)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 127\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 127\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(116)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 116\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 116\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(140)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 140\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 140\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(139)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 139\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 139\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(96)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 96\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 96\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(110)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 110\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 110\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Got cleaning task CleanAccum(114)\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaning accumulator 114\n",
      "23/10/03 12:21:19 DEBUG ContextCleaner: Cleaned accumulator 114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/03 12:21:21 DEBUG ExecutorMetricsPoller: removing (1, 0) from stageTCMP\n",
      "23/10/03 12:21:21 DEBUG ExecutorMetricsPoller: removing (2, 0) from stageTCMP\n",
      "23/10/03 12:21:21 DEBUG ExecutorMetricsPoller: removing (0, 0) from stageTCMP\n",
      "23/10/03 12:21:23 INFO JDBCRDD: closed connection\n",
      "23/10/03 12:21:23 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3764 bytes result sent to driver\n",
      "23/10/03 12:21:23 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 0\n",
      "23/10/03 12:21:23 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 4719 ms on asusbc-rtl8117.lan (executor driver) (1/1)\n",
      "23/10/03 12:21:23 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "23/10/03 12:21:23 INFO DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 4.725 s\n",
      "23/10/03 12:21:23 DEBUG DAGScheduler: After removal of stage 3, remaining stages = 0\n",
      "23/10/03 12:21:23 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/10/03 12:21:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "23/10/03 12:21:23 INFO DAGScheduler: Job 3 finished: showString at NativeMethodAccessorImpl.java:0, took 4.727843 s\n",
      "23/10/03 12:21:23 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, false].toString, input[1, string, false].toString, input[2, string, false].toString, input[3, string, false].toString, input[4, string, false].toString, StructField(toprettystring(ps_partkey),StringType,false), StructField(toprettystring(ps_suppkey),StringType,false), StructField(toprettystring(ps_availqty),StringType,false), StructField(toprettystring(ps_supplycost),StringType,false), StructField(toprettystring(ps_comment),StringType,false)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[5];\n",
      "/* 024 */     createExternalRow_0_0(i, values_0);\n",
      "/* 025 */     createExternalRow_0_1(i, values_0);\n",
      "/* 026 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 027 */     if (false) {\n",
      "/* 028 */       mutableRow.setNullAt(0);\n",
      "/* 029 */     } else {\n",
      "/* 030 */\n",
      "/* 031 */       mutableRow.update(0, value_0);\n",
      "/* 032 */     }\n",
      "/* 033 */\n",
      "/* 034 */     return mutableRow;\n",
      "/* 035 */   }\n",
      "/* 036 */\n",
      "/* 037 */\n",
      "/* 038 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {\n",
      "/* 039 */\n",
      "/* 040 */     UTF8String value_8 = i.getUTF8String(3);\n",
      "/* 041 */     boolean isNull_7 = true;\n",
      "/* 042 */     java.lang.String value_7 = null;\n",
      "/* 043 */     isNull_7 = false;\n",
      "/* 044 */     if (!isNull_7) {\n",
      "/* 045 */\n",
      "/* 046 */       Object funcResult_3 = null;\n",
      "/* 047 */       funcResult_3 = value_8.toString();\n",
      "/* 048 */       value_7 = (java.lang.String) funcResult_3;\n",
      "/* 049 */\n",
      "/* 050 */     }\n",
      "/* 051 */     if (isNull_7) {\n",
      "/* 052 */       values_0[3] = null;\n",
      "/* 053 */     } else {\n",
      "/* 054 */       values_0[3] = value_7;\n",
      "/* 055 */     }\n",
      "/* 056 */\n",
      "/* 057 */     UTF8String value_10 = i.getUTF8String(4);\n",
      "/* 058 */     boolean isNull_9 = true;\n",
      "/* 059 */     java.lang.String value_9 = null;\n",
      "/* 060 */     isNull_9 = false;\n",
      "/* 061 */     if (!isNull_9) {\n",
      "/* 062 */\n",
      "/* 063 */       Object funcResult_4 = null;\n",
      "/* 064 */       funcResult_4 = value_10.toString();\n",
      "/* 065 */       value_9 = (java.lang.String) funcResult_4;\n",
      "/* 066 */\n",
      "/* 067 */     }\n",
      "/* 068 */     if (isNull_9) {\n",
      "/* 069 */       values_0[4] = null;\n",
      "/* 070 */     } else {\n",
      "/* 071 */       values_0[4] = value_9;\n",
      "/* 072 */     }\n",
      "/* 073 */\n",
      "/* 074 */   }\n",
      "/* 075 */\n",
      "/* 076 */\n",
      "/* 077 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {\n",
      "/* 078 */\n",
      "/* 079 */     UTF8String value_2 = i.getUTF8String(0);\n",
      "/* 080 */     boolean isNull_1 = true;\n",
      "/* 081 */     java.lang.String value_1 = null;\n",
      "/* 082 */     isNull_1 = false;\n",
      "/* 083 */     if (!isNull_1) {\n",
      "/* 084 */\n",
      "/* 085 */       Object funcResult_0 = null;\n",
      "/* 086 */       funcResult_0 = value_2.toString();\n",
      "/* 087 */       value_1 = (java.lang.String) funcResult_0;\n",
      "/* 088 */\n",
      "/* 089 */     }\n",
      "/* 090 */     if (isNull_1) {\n",
      "/* 091 */       values_0[0] = null;\n",
      "/* 092 */     } else {\n",
      "/* 093 */       values_0[0] = value_1;\n",
      "/* 094 */     }\n",
      "/* 095 */\n",
      "/* 096 */     UTF8String value_4 = i.getUTF8String(1);\n",
      "/* 097 */     boolean isNull_3 = true;\n",
      "/* 098 */     java.lang.String value_3 = null;\n",
      "/* 099 */     isNull_3 = false;\n",
      "/* 100 */     if (!isNull_3) {\n",
      "/* 101 */\n",
      "/* 102 */       Object funcResult_1 = null;\n",
      "/* 103 */       funcResult_1 = value_4.toString();\n",
      "/* 104 */       value_3 = (java.lang.String) funcResult_1;\n",
      "/* 105 */\n",
      "/* 106 */     }\n",
      "/* 107 */     if (isNull_3) {\n",
      "/* 108 */       values_0[1] = null;\n",
      "/* 109 */     } else {\n",
      "/* 110 */       values_0[1] = value_3;\n",
      "/* 111 */     }\n",
      "/* 112 */\n",
      "/* 113 */     UTF8String value_6 = i.getUTF8String(2);\n",
      "/* 114 */     boolean isNull_5 = true;\n",
      "/* 115 */     java.lang.String value_5 = null;\n",
      "/* 116 */     isNull_5 = false;\n",
      "/* 117 */     if (!isNull_5) {\n",
      "/* 118 */\n",
      "/* 119 */       Object funcResult_2 = null;\n",
      "/* 120 */       funcResult_2 = value_6.toString();\n",
      "/* 121 */       value_5 = (java.lang.String) funcResult_2;\n",
      "/* 122 */\n",
      "/* 123 */     }\n",
      "/* 124 */     if (isNull_5) {\n",
      "/* 125 */       values_0[2] = null;\n",
      "/* 126 */     } else {\n",
      "/* 127 */       values_0[2] = value_5;\n",
      "/* 128 */     }\n",
      "/* 129 */\n",
      "/* 130 */   }\n",
      "/* 131 */\n",
      "/* 132 */ }\n",
      "\n",
      "23/10/03 12:21:23 DEBUG CodeGenerator: \n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[5];\n",
      "/* 024 */     createExternalRow_0_0(i, values_0);\n",
      "/* 025 */     createExternalRow_0_1(i, values_0);\n",
      "/* 026 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 027 */     if (false) {\n",
      "/* 028 */       mutableRow.setNullAt(0);\n",
      "/* 029 */     } else {\n",
      "/* 030 */\n",
      "/* 031 */       mutableRow.update(0, value_0);\n",
      "/* 032 */     }\n",
      "/* 033 */\n",
      "/* 034 */     return mutableRow;\n",
      "/* 035 */   }\n",
      "/* 036 */\n",
      "/* 037 */\n",
      "/* 038 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {\n",
      "/* 039 */\n",
      "/* 040 */     UTF8String value_8 = i.getUTF8String(3);\n",
      "/* 041 */     boolean isNull_7 = true;\n",
      "/* 042 */     java.lang.String value_7 = null;\n",
      "/* 043 */     isNull_7 = false;\n",
      "/* 044 */     if (!isNull_7) {\n",
      "/* 045 */\n",
      "/* 046 */       Object funcResult_3 = null;\n",
      "/* 047 */       funcResult_3 = value_8.toString();\n",
      "/* 048 */       value_7 = (java.lang.String) funcResult_3;\n",
      "/* 049 */\n",
      "/* 050 */     }\n",
      "/* 051 */     if (isNull_7) {\n",
      "/* 052 */       values_0[3] = null;\n",
      "/* 053 */     } else {\n",
      "/* 054 */       values_0[3] = value_7;\n",
      "/* 055 */     }\n",
      "/* 056 */\n",
      "/* 057 */     UTF8String value_10 = i.getUTF8String(4);\n",
      "/* 058 */     boolean isNull_9 = true;\n",
      "/* 059 */     java.lang.String value_9 = null;\n",
      "/* 060 */     isNull_9 = false;\n",
      "/* 061 */     if (!isNull_9) {\n",
      "/* 062 */\n",
      "/* 063 */       Object funcResult_4 = null;\n",
      "/* 064 */       funcResult_4 = value_10.toString();\n",
      "/* 065 */       value_9 = (java.lang.String) funcResult_4;\n",
      "/* 066 */\n",
      "/* 067 */     }\n",
      "/* 068 */     if (isNull_9) {\n",
      "/* 069 */       values_0[4] = null;\n",
      "/* 070 */     } else {\n",
      "/* 071 */       values_0[4] = value_9;\n",
      "/* 072 */     }\n",
      "/* 073 */\n",
      "/* 074 */   }\n",
      "/* 075 */\n",
      "/* 076 */\n",
      "/* 077 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {\n",
      "/* 078 */\n",
      "/* 079 */     UTF8String value_2 = i.getUTF8String(0);\n",
      "/* 080 */     boolean isNull_1 = true;\n",
      "/* 081 */     java.lang.String value_1 = null;\n",
      "/* 082 */     isNull_1 = false;\n",
      "/* 083 */     if (!isNull_1) {\n",
      "/* 084 */\n",
      "/* 085 */       Object funcResult_0 = null;\n",
      "/* 086 */       funcResult_0 = value_2.toString();\n",
      "/* 087 */       value_1 = (java.lang.String) funcResult_0;\n",
      "/* 088 */\n",
      "/* 089 */     }\n",
      "/* 090 */     if (isNull_1) {\n",
      "/* 091 */       values_0[0] = null;\n",
      "/* 092 */     } else {\n",
      "/* 093 */       values_0[0] = value_1;\n",
      "/* 094 */     }\n",
      "/* 095 */\n",
      "/* 096 */     UTF8String value_4 = i.getUTF8String(1);\n",
      "/* 097 */     boolean isNull_3 = true;\n",
      "/* 098 */     java.lang.String value_3 = null;\n",
      "/* 099 */     isNull_3 = false;\n",
      "/* 100 */     if (!isNull_3) {\n",
      "/* 101 */\n",
      "/* 102 */       Object funcResult_1 = null;\n",
      "/* 103 */       funcResult_1 = value_4.toString();\n",
      "/* 104 */       value_3 = (java.lang.String) funcResult_1;\n",
      "/* 105 */\n",
      "/* 106 */     }\n",
      "/* 107 */     if (isNull_3) {\n",
      "/* 108 */       values_0[1] = null;\n",
      "/* 109 */     } else {\n",
      "/* 110 */       values_0[1] = value_3;\n",
      "/* 111 */     }\n",
      "/* 112 */\n",
      "/* 113 */     UTF8String value_6 = i.getUTF8String(2);\n",
      "/* 114 */     boolean isNull_5 = true;\n",
      "/* 115 */     java.lang.String value_5 = null;\n",
      "/* 116 */     isNull_5 = false;\n",
      "/* 117 */     if (!isNull_5) {\n",
      "/* 118 */\n",
      "/* 119 */       Object funcResult_2 = null;\n",
      "/* 120 */       funcResult_2 = value_6.toString();\n",
      "/* 121 */       value_5 = (java.lang.String) funcResult_2;\n",
      "/* 122 */\n",
      "/* 123 */     }\n",
      "/* 124 */     if (isNull_5) {\n",
      "/* 125 */       values_0[2] = null;\n",
      "/* 126 */     } else {\n",
      "/* 127 */       values_0[2] = value_5;\n",
      "/* 128 */     }\n",
      "/* 129 */\n",
      "/* 130 */   }\n",
      "/* 131 */\n",
      "/* 132 */ }\n",
      "\n",
      "23/10/03 12:21:23 INFO CodeGenerator: Code generated in 7.593424 ms\n",
      "23/10/03 12:21:23 DEBUG SparkSqlParser: Parsing command: partsupp\n",
      "23/10/03 12:21:23 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:23 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:23 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:23 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:23 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:23 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:23 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:23 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:23 DEBUG BasicConnectionProvider: JDBC connection initiated with URL: jdbc:postgresql://localhost:5432/tpch and properties: {password=tpch, user=tpch}\n",
      "23/10/03 12:21:23 DEBUG CatalystSqlParser: Parsing command: varchar(25)\n",
      "23/10/03 12:21:23 DEBUG CatalystSqlParser: Parsing command: varchar(40)\n",
      "23/10/03 12:21:23 DEBUG CatalystSqlParser: Parsing command: char(15)\n",
      "23/10/03 12:21:23 DEBUG CatalystSqlParser: Parsing command: char(10)\n",
      "23/10/03 12:21:23 DEBUG CatalystSqlParser: Parsing command: varchar(117)\n",
      "23/10/03 12:21:23 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator scan_input_0;\n",
      "/* 010 */   private boolean project_resultIsNull_0;\n",
      "/* 011 */   private int project_argValue_0;\n",
      "/* 012 */   private boolean project_resultIsNull_1;\n",
      "/* 013 */   private int project_argValue_1;\n",
      "/* 014 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] scan_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 015 */   private UTF8String[] project_mutableStateArray_0 = new UTF8String[2];\n",
      "/* 016 */\n",
      "/* 017 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 018 */     this.references = references;\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 022 */     partitionIndex = index;\n",
      "/* 023 */     this.inputs = inputs;\n",
      "/* 024 */     scan_input_0 = inputs[0];\n",
      "/* 025 */     scan_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(8, 192);\n",
      "/* 026 */\n",
      "/* 027 */     scan_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(8, 256);\n",
      "/* 028 */\n",
      "/* 029 */   }\n",
      "/* 030 */\n",
      "/* 031 */   protected void processNext() throws java.io.IOException {\n",
      "/* 032 */     while ( scan_input_0.hasNext()) {\n",
      "/* 033 */       InternalRow scan_row_0 = (InternalRow) scan_input_0.next();\n",
      "/* 034 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 035 */       // common sub-expressions\n",
      "/* 036 */\n",
      "/* 037 */       boolean scan_isNull_0 = scan_row_0.isNullAt(0);\n",
      "/* 038 */       int scan_value_0 = scan_isNull_0 ?\n",
      "/* 039 */       -1 : (scan_row_0.getInt(0));\n",
      "/* 040 */       UTF8String project_value_0;\n",
      "/* 041 */       if (scan_isNull_0) {\n",
      "/* 042 */         project_value_0 = UTF8String.fromString(\"NULL\");\n",
      "/* 043 */       } else {\n",
      "/* 044 */         project_value_0 = UTF8String.fromString(String.valueOf(scan_value_0));\n",
      "/* 045 */       }\n",
      "/* 046 */       boolean scan_isNull_1 = scan_row_0.isNullAt(1);\n",
      "/* 047 */       UTF8String scan_value_1 = scan_isNull_1 ?\n",
      "/* 048 */       null : (scan_row_0.getUTF8String(1));\n",
      "/* 049 */       UTF8String project_value_2;\n",
      "/* 050 */       if (scan_isNull_1) {\n",
      "/* 051 */         project_value_2 = UTF8String.fromString(\"NULL\");\n",
      "/* 052 */       } else {\n",
      "/* 053 */         project_value_2 = scan_value_1;\n",
      "/* 054 */       }\n",
      "/* 055 */       boolean scan_isNull_2 = scan_row_0.isNullAt(2);\n",
      "/* 056 */       UTF8String scan_value_2 = scan_isNull_2 ?\n",
      "/* 057 */       null : (scan_row_0.getUTF8String(2));\n",
      "/* 058 */       UTF8String project_value_4;\n",
      "/* 059 */       if (scan_isNull_2) {\n",
      "/* 060 */         project_value_4 = UTF8String.fromString(\"NULL\");\n",
      "/* 061 */       } else {\n",
      "/* 062 */         project_value_4 = scan_value_2;\n",
      "/* 063 */       }\n",
      "/* 064 */       boolean scan_isNull_3 = scan_row_0.isNullAt(3);\n",
      "/* 065 */       long scan_value_3 = scan_isNull_3 ?\n",
      "/* 066 */       -1L : (scan_row_0.getLong(3));\n",
      "/* 067 */       UTF8String project_value_6;\n",
      "/* 068 */       if (scan_isNull_3) {\n",
      "/* 069 */         project_value_6 = UTF8String.fromString(\"NULL\");\n",
      "/* 070 */       } else {\n",
      "/* 071 */         project_value_6 = UTF8String.fromString(String.valueOf(scan_value_3));\n",
      "/* 072 */       }\n",
      "/* 073 */       project_resultIsNull_0 = false;\n",
      "/* 074 */\n",
      "/* 075 */       if (!project_resultIsNull_0) {\n",
      "/* 076 */         boolean scan_isNull_4 = scan_row_0.isNullAt(4);\n",
      "/* 077 */         UTF8String scan_value_4 = scan_isNull_4 ?\n",
      "/* 078 */         null : (scan_row_0.getUTF8String(4));\n",
      "/* 079 */         project_resultIsNull_0 = scan_isNull_4;\n",
      "/* 080 */         project_mutableStateArray_0[0] = scan_value_4;\n",
      "/* 081 */       }\n",
      "/* 082 */\n",
      "/* 083 */       if (!project_resultIsNull_0) {\n",
      "/* 084 */         project_argValue_0 = 15;\n",
      "/* 085 */       }\n",
      "/* 086 */\n",
      "/* 087 */       boolean project_isNull_9 = project_resultIsNull_0;\n",
      "/* 088 */       UTF8String project_value_9 = null;\n",
      "/* 089 */       if (!project_resultIsNull_0) {\n",
      "/* 090 */         project_value_9 = org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils.readSidePadding(project_mutableStateArray_0[0], project_argValue_0);\n",
      "/* 091 */       }\n",
      "/* 092 */       UTF8String project_value_8;\n",
      "/* 093 */       if (project_isNull_9) {\n",
      "/* 094 */         project_value_8 = UTF8String.fromString(\"NULL\");\n",
      "/* 095 */       } else {\n",
      "/* 096 */         project_value_8 = project_value_9;\n",
      "/* 097 */       }\n",
      "/* 098 */       boolean scan_isNull_5 = scan_row_0.isNullAt(5);\n",
      "/* 099 */       Decimal scan_value_5 = scan_isNull_5 ?\n",
      "/* 100 */       null : (scan_row_0.getDecimal(5, 38, 18));\n",
      "/* 101 */       UTF8String project_value_12;\n",
      "/* 102 */       if (scan_isNull_5) {\n",
      "/* 103 */         project_value_12 = UTF8String.fromString(\"NULL\");\n",
      "/* 104 */       } else {\n",
      "/* 105 */         project_value_12 = UTF8String.fromString(scan_value_5.toPlainString());\n",
      "/* 106 */       }\n",
      "/* 107 */       project_resultIsNull_1 = false;\n",
      "/* 108 */\n",
      "/* 109 */       if (!project_resultIsNull_1) {\n",
      "/* 110 */         boolean scan_isNull_6 = scan_row_0.isNullAt(6);\n",
      "/* 111 */         UTF8String scan_value_6 = scan_isNull_6 ?\n",
      "/* 112 */         null : (scan_row_0.getUTF8String(6));\n",
      "/* 113 */         project_resultIsNull_1 = scan_isNull_6;\n",
      "/* 114 */         project_mutableStateArray_0[1] = scan_value_6;\n",
      "/* 115 */       }\n",
      "/* 116 */\n",
      "/* 117 */       if (!project_resultIsNull_1) {\n",
      "/* 118 */         project_argValue_1 = 10;\n",
      "/* 119 */       }\n",
      "/* 120 */\n",
      "/* 121 */       boolean project_isNull_15 = project_resultIsNull_1;\n",
      "/* 122 */       UTF8String project_value_15 = null;\n",
      "/* 123 */       if (!project_resultIsNull_1) {\n",
      "/* 124 */         project_value_15 = org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils.readSidePadding(project_mutableStateArray_0[1], project_argValue_1);\n",
      "/* 125 */       }\n",
      "/* 126 */       UTF8String project_value_14;\n",
      "/* 127 */       if (project_isNull_15) {\n",
      "/* 128 */         project_value_14 = UTF8String.fromString(\"NULL\");\n",
      "/* 129 */       } else {\n",
      "/* 130 */         project_value_14 = project_value_15;\n",
      "/* 131 */       }\n",
      "/* 132 */       boolean scan_isNull_7 = scan_row_0.isNullAt(7);\n",
      "/* 133 */       UTF8String scan_value_7 = scan_isNull_7 ?\n",
      "/* 134 */       null : (scan_row_0.getUTF8String(7));\n",
      "/* 135 */       UTF8String project_value_18;\n",
      "/* 136 */       if (scan_isNull_7) {\n",
      "/* 137 */         project_value_18 = UTF8String.fromString(\"NULL\");\n",
      "/* 138 */       } else {\n",
      "/* 139 */         project_value_18 = scan_value_7;\n",
      "/* 140 */       }\n",
      "/* 141 */       scan_mutableStateArray_0[1].reset();\n",
      "/* 142 */\n",
      "/* 143 */       scan_mutableStateArray_0[1].write(0, project_value_0);\n",
      "/* 144 */\n",
      "/* 145 */       scan_mutableStateArray_0[1].write(1, project_value_2);\n",
      "/* 146 */\n",
      "/* 147 */       scan_mutableStateArray_0[1].write(2, project_value_4);\n",
      "/* 148 */\n",
      "/* 149 */       scan_mutableStateArray_0[1].write(3, project_value_6);\n",
      "/* 150 */\n",
      "/* 151 */       scan_mutableStateArray_0[1].write(4, project_value_8);\n",
      "/* 152 */\n",
      "/* 153 */       scan_mutableStateArray_0[1].write(5, project_value_12);\n",
      "/* 154 */\n",
      "/* 155 */       scan_mutableStateArray_0[1].write(6, project_value_14);\n",
      "/* 156 */\n",
      "/* 157 */       scan_mutableStateArray_0[1].write(7, project_value_18);\n",
      "/* 158 */       append((scan_mutableStateArray_0[1].getRow()));\n",
      "/* 159 */       if (shouldStop()) return;\n",
      "/* 160 */     }\n",
      "/* 161 */   }\n",
      "/* 162 */\n",
      "/* 163 */ }\n",
      "\n",
      "23/10/03 12:21:23 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator scan_input_0;\n",
      "/* 010 */   private boolean project_resultIsNull_0;\n",
      "/* 011 */   private int project_argValue_0;\n",
      "/* 012 */   private boolean project_resultIsNull_1;\n",
      "/* 013 */   private int project_argValue_1;\n",
      "/* 014 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] scan_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 015 */   private UTF8String[] project_mutableStateArray_0 = new UTF8String[2];\n",
      "/* 016 */\n",
      "/* 017 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 018 */     this.references = references;\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 022 */     partitionIndex = index;\n",
      "/* 023 */     this.inputs = inputs;\n",
      "/* 024 */     scan_input_0 = inputs[0];\n",
      "/* 025 */     scan_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(8, 192);\n",
      "/* 026 */\n",
      "/* 027 */     scan_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(8, 256);\n",
      "/* 028 */\n",
      "/* 029 */   }\n",
      "/* 030 */\n",
      "/* 031 */   protected void processNext() throws java.io.IOException {\n",
      "/* 032 */     while ( scan_input_0.hasNext()) {\n",
      "/* 033 */       InternalRow scan_row_0 = (InternalRow) scan_input_0.next();\n",
      "/* 034 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 035 */       // common sub-expressions\n",
      "/* 036 */\n",
      "/* 037 */       boolean scan_isNull_0 = scan_row_0.isNullAt(0);\n",
      "/* 038 */       int scan_value_0 = scan_isNull_0 ?\n",
      "/* 039 */       -1 : (scan_row_0.getInt(0));\n",
      "/* 040 */       UTF8String project_value_0;\n",
      "/* 041 */       if (scan_isNull_0) {\n",
      "/* 042 */         project_value_0 = UTF8String.fromString(\"NULL\");\n",
      "/* 043 */       } else {\n",
      "/* 044 */         project_value_0 = UTF8String.fromString(String.valueOf(scan_value_0));\n",
      "/* 045 */       }\n",
      "/* 046 */       boolean scan_isNull_1 = scan_row_0.isNullAt(1);\n",
      "/* 047 */       UTF8String scan_value_1 = scan_isNull_1 ?\n",
      "/* 048 */       null : (scan_row_0.getUTF8String(1));\n",
      "/* 049 */       UTF8String project_value_2;\n",
      "/* 050 */       if (scan_isNull_1) {\n",
      "/* 051 */         project_value_2 = UTF8String.fromString(\"NULL\");\n",
      "/* 052 */       } else {\n",
      "/* 053 */         project_value_2 = scan_value_1;\n",
      "/* 054 */       }\n",
      "/* 055 */       boolean scan_isNull_2 = scan_row_0.isNullAt(2);\n",
      "/* 056 */       UTF8String scan_value_2 = scan_isNull_2 ?\n",
      "/* 057 */       null : (scan_row_0.getUTF8String(2));\n",
      "/* 058 */       UTF8String project_value_4;\n",
      "/* 059 */       if (scan_isNull_2) {\n",
      "/* 060 */         project_value_4 = UTF8String.fromString(\"NULL\");\n",
      "/* 061 */       } else {\n",
      "/* 062 */         project_value_4 = scan_value_2;\n",
      "/* 063 */       }\n",
      "/* 064 */       boolean scan_isNull_3 = scan_row_0.isNullAt(3);\n",
      "/* 065 */       long scan_value_3 = scan_isNull_3 ?\n",
      "/* 066 */       -1L : (scan_row_0.getLong(3));\n",
      "/* 067 */       UTF8String project_value_6;\n",
      "/* 068 */       if (scan_isNull_3) {\n",
      "/* 069 */         project_value_6 = UTF8String.fromString(\"NULL\");\n",
      "/* 070 */       } else {\n",
      "/* 071 */         project_value_6 = UTF8String.fromString(String.valueOf(scan_value_3));\n",
      "/* 072 */       }\n",
      "/* 073 */       project_resultIsNull_0 = false;\n",
      "/* 074 */\n",
      "/* 075 */       if (!project_resultIsNull_0) {\n",
      "/* 076 */         boolean scan_isNull_4 = scan_row_0.isNullAt(4);\n",
      "/* 077 */         UTF8String scan_value_4 = scan_isNull_4 ?\n",
      "/* 078 */         null : (scan_row_0.getUTF8String(4));\n",
      "/* 079 */         project_resultIsNull_0 = scan_isNull_4;\n",
      "/* 080 */         project_mutableStateArray_0[0] = scan_value_4;\n",
      "/* 081 */       }\n",
      "/* 082 */\n",
      "/* 083 */       if (!project_resultIsNull_0) {\n",
      "/* 084 */         project_argValue_0 = 15;\n",
      "/* 085 */       }\n",
      "/* 086 */\n",
      "/* 087 */       boolean project_isNull_9 = project_resultIsNull_0;\n",
      "/* 088 */       UTF8String project_value_9 = null;\n",
      "/* 089 */       if (!project_resultIsNull_0) {\n",
      "/* 090 */         project_value_9 = org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils.readSidePadding(project_mutableStateArray_0[0], project_argValue_0);\n",
      "/* 091 */       }\n",
      "/* 092 */       UTF8String project_value_8;\n",
      "/* 093 */       if (project_isNull_9) {\n",
      "/* 094 */         project_value_8 = UTF8String.fromString(\"NULL\");\n",
      "/* 095 */       } else {\n",
      "/* 096 */         project_value_8 = project_value_9;\n",
      "/* 097 */       }\n",
      "/* 098 */       boolean scan_isNull_5 = scan_row_0.isNullAt(5);\n",
      "/* 099 */       Decimal scan_value_5 = scan_isNull_5 ?\n",
      "/* 100 */       null : (scan_row_0.getDecimal(5, 38, 18));\n",
      "/* 101 */       UTF8String project_value_12;\n",
      "/* 102 */       if (scan_isNull_5) {\n",
      "/* 103 */         project_value_12 = UTF8String.fromString(\"NULL\");\n",
      "/* 104 */       } else {\n",
      "/* 105 */         project_value_12 = UTF8String.fromString(scan_value_5.toPlainString());\n",
      "/* 106 */       }\n",
      "/* 107 */       project_resultIsNull_1 = false;\n",
      "/* 108 */\n",
      "/* 109 */       if (!project_resultIsNull_1) {\n",
      "/* 110 */         boolean scan_isNull_6 = scan_row_0.isNullAt(6);\n",
      "/* 111 */         UTF8String scan_value_6 = scan_isNull_6 ?\n",
      "/* 112 */         null : (scan_row_0.getUTF8String(6));\n",
      "/* 113 */         project_resultIsNull_1 = scan_isNull_6;\n",
      "/* 114 */         project_mutableStateArray_0[1] = scan_value_6;\n",
      "/* 115 */       }\n",
      "/* 116 */\n",
      "/* 117 */       if (!project_resultIsNull_1) {\n",
      "/* 118 */         project_argValue_1 = 10;\n",
      "/* 119 */       }\n",
      "/* 120 */\n",
      "/* 121 */       boolean project_isNull_15 = project_resultIsNull_1;\n",
      "/* 122 */       UTF8String project_value_15 = null;\n",
      "/* 123 */       if (!project_resultIsNull_1) {\n",
      "/* 124 */         project_value_15 = org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils.readSidePadding(project_mutableStateArray_0[1], project_argValue_1);\n",
      "/* 125 */       }\n",
      "/* 126 */       UTF8String project_value_14;\n",
      "/* 127 */       if (project_isNull_15) {\n",
      "/* 128 */         project_value_14 = UTF8String.fromString(\"NULL\");\n",
      "/* 129 */       } else {\n",
      "/* 130 */         project_value_14 = project_value_15;\n",
      "/* 131 */       }\n",
      "/* 132 */       boolean scan_isNull_7 = scan_row_0.isNullAt(7);\n",
      "/* 133 */       UTF8String scan_value_7 = scan_isNull_7 ?\n",
      "/* 134 */       null : (scan_row_0.getUTF8String(7));\n",
      "/* 135 */       UTF8String project_value_18;\n",
      "/* 136 */       if (scan_isNull_7) {\n",
      "/* 137 */         project_value_18 = UTF8String.fromString(\"NULL\");\n",
      "/* 138 */       } else {\n",
      "/* 139 */         project_value_18 = scan_value_7;\n",
      "/* 140 */       }\n",
      "/* 141 */       scan_mutableStateArray_0[1].reset();\n",
      "/* 142 */\n",
      "/* 143 */       scan_mutableStateArray_0[1].write(0, project_value_0);\n",
      "/* 144 */\n",
      "/* 145 */       scan_mutableStateArray_0[1].write(1, project_value_2);\n",
      "/* 146 */\n",
      "/* 147 */       scan_mutableStateArray_0[1].write(2, project_value_4);\n",
      "/* 148 */\n",
      "/* 149 */       scan_mutableStateArray_0[1].write(3, project_value_6);\n",
      "/* 150 */\n",
      "/* 151 */       scan_mutableStateArray_0[1].write(4, project_value_8);\n",
      "/* 152 */\n",
      "/* 153 */       scan_mutableStateArray_0[1].write(5, project_value_12);\n",
      "/* 154 */\n",
      "/* 155 */       scan_mutableStateArray_0[1].write(6, project_value_14);\n",
      "/* 156 */\n",
      "/* 157 */       scan_mutableStateArray_0[1].write(7, project_value_18);\n",
      "/* 158 */       append((scan_mutableStateArray_0[1].getRow()));\n",
      "/* 159 */       if (shouldStop()) return;\n",
      "/* 160 */     }\n",
      "/* 161 */   }\n",
      "/* 162 */\n",
      "/* 163 */ }\n",
      "\n",
      "23/10/03 12:21:23 INFO CodeGenerator: Code generated in 15.72625 ms\n",
      "23/10/03 12:21:23 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "23/10/03 12:21:23 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "23/10/03 12:21:23 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$executeTake$2\n",
      "23/10/03 12:21:23 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++\n",
      "23/10/03 12:21:23 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "23/10/03 12:21:23 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "23/10/03 12:21:23 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/10/03 12:21:23 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 14 took 0.000059 seconds\n",
      "23/10/03 12:21:23 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "23/10/03 12:21:23 INFO DAGScheduler: Got job 4 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/10/03 12:21:23 INFO DAGScheduler: Final stage: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/10/03 12:21:23 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/10/03 12:21:23 INFO DAGScheduler: Missing parents: List()\n",
      "23/10/03 12:21:23 DEBUG DAGScheduler: submitStage(ResultStage 4 (name=showString at NativeMethodAccessorImpl.java:0;jobs=4))\n",
      "23/10/03 12:21:23 DEBUG DAGScheduler: missing: List()\n",
      "23/10/03 12:21:23 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[14] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/10/03 12:21:23 DEBUG DAGScheduler: submitMissingTasks(ResultStage 4)\n",
      "23/10/03 12:21:23 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 15.1 KiB, free 26.8 GiB)\n",
      "23/10/03 12:21:23 DEBUG BlockManager: Put block broadcast_4 locally took 0 ms\n",
      "23/10/03 12:21:23 DEBUG BlockManager: Putting block broadcast_4 without replication took 0 ms\n",
      "23/10/03 12:21:23 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 26.8 GiB)\n",
      "23/10/03 12:21:23 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_4_piece0 for BlockManagerId(driver, asusbc-rtl8117.lan, 34663, None)\n",
      "23/10/03 12:21:23 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on asusbc-rtl8117.lan:34663 (size: 7.0 KiB, free: 26.8 GiB)\n",
      "23/10/03 12:21:23 DEBUG BlockManagerMaster: Updated info of block broadcast_4_piece0\n",
      "23/10/03 12:21:23 DEBUG BlockManager: Told master about block broadcast_4_piece0\n",
      "23/10/03 12:21:23 DEBUG BlockManager: Put block broadcast_4_piece0 locally took 0 ms\n",
      "23/10/03 12:21:23 DEBUG BlockManager: Putting block broadcast_4_piece0 without replication took 0 ms\n",
      "23/10/03 12:21:23 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580\n",
      "23/10/03 12:21:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/10/03 12:21:23 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "23/10/03 12:21:23 DEBUG TaskSetManager: Epoch for TaskSet 4.0: 0\n",
      "23/10/03 12:21:23 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "23/10/03 12:21:23 DEBUG TaskSetManager: Valid locality levels for TaskSet 4.0: NO_PREF, ANY\n",
      "23/10/03 12:21:23 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_4.0, runningTasks: 0\n",
      "23/10/03 12:21:23 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (asusbc-rtl8117.lan, executor driver, partition 0, PROCESS_LOCAL, 7647 bytes) \n",
      "23/10/03 12:21:23 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "23/10/03 12:21:23 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
      "23/10/03 12:21:23 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 1\n",
      "23/10/03 12:21:23 DEBUG BlockManager: Getting local block broadcast_4\n",
      "23/10/03 12:21:23 DEBUG BlockManager: Level for block broadcast_4 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "23/10/03 12:21:23 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:23 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:23 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:23 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:23 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:23 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:23 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:23 DEBUG BasicConnectionProvider: JDBC connection initiated with URL: jdbc:postgresql://localhost:5432/tpch and properties: {password=tpch, user=tpch}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+--------------------+--------------------+\n",
      "|ps_partkey|ps_suppkey|ps_availqty|       ps_supplycost|          ps_comment|\n",
      "+----------+----------+-----------+--------------------+--------------------+\n",
      "|      6830|     61831|       4119|799.3300000000000...|ffix daringly unt...|\n",
      "|      6830|     89331|         40|982.5200000000000...|he finally final ...|\n",
      "|      6831|      6832|       4249|837.4200000000000...|. dugouts serve q...|\n",
      "|      6831|     34332|       1686|752.1000000000000...|y special request...|\n",
      "|      6831|     61832|       4076|266.5500000000000...|ccounts. carefull...|\n",
      "|      6831|     89332|       9790|525.0900000000000...|ilent deposits pl...|\n",
      "|      6832|      6833|       4967|142.1500000000000...|regular dependenc...|\n",
      "|      6832|     34333|       4624|813.1900000000000...|es. furiously iro...|\n",
      "|      6832|     61833|       6650|69.16000000000000...|ow ideas. furious...|\n",
      "|      6832|     89333|       4099|792.3400000000000...|special, final th...|\n",
      "|      6833|      6834|       2919|285.5200000000000...|gle fluffily amon...|\n",
      "|      6833|     34334|       7749|735.9900000000000...|l, ironic package...|\n",
      "|      6833|     61834|       4688|264.9800000000000...|ts are furiously ...|\n",
      "|      6833|     89334|       4436|182.1300000000000...|old accounts affi...|\n",
      "|      6834|      6835|       9704|419.2700000000000...|ial excuses about...|\n",
      "|      6834|     34335|       5305|989.0900000000000...|heodolites sleep ...|\n",
      "|      6834|     61835|       9508|318.9600000000000...|uffily regular re...|\n",
      "|      6834|     89335|       5152|385.7000000000000...|e. courts eat fur...|\n",
      "|      6835|      6836|       9193|109.1300000000000...| bold accounts. p...|\n",
      "|      6835|     34336|       9620|157.9000000000000...|ding to the water...|\n",
      "+----------+----------+-----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "customer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(179)1) / 1]\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 179\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 179\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(197)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 197\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 197\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(200)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 200\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 200\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(173)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 173\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 173\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(207)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 207\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 207\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(178)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 178\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 178\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(153)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 153\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 153\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(190)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 190\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 190\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(182)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 182\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 182\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(199)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 199\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 199\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(174)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 174\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 174\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(208)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 208\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 208\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(165)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 165\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 165\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(169)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 169\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 169\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(164)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 164\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 164\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(175)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 175\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 175\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(157)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 157\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 157\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(193)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 193\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 193\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(192)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 192\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 192\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(204)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 204\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 204\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(198)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 198\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 198\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(3)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning broadcast 3\n",
      "23/10/03 12:21:24 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 3\n",
      "23/10/03 12:21:24 DEBUG BlockManagerStorageEndpoint: removing broadcast 3\n",
      "23/10/03 12:21:24 DEBUG BlockManager: Removing broadcast 3\n",
      "23/10/03 12:21:24 DEBUG BlockManager: Removing block broadcast_3\n",
      "23/10/03 12:21:24 DEBUG MemoryStore: Block broadcast_3 of size 13072 dropped from memory (free 28802256472)\n",
      "23/10/03 12:21:24 DEBUG BlockManager: Removing block broadcast_3_piece0\n",
      "23/10/03 12:21:24 DEBUG MemoryStore: Block broadcast_3_piece0 of size 6439 dropped from memory (free 28802262911)\n",
      "23/10/03 12:21:24 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, asusbc-rtl8117.lan, 34663, None)\n",
      "23/10/03 12:21:24 INFO BlockManagerInfo: Removed broadcast_3_piece0 on asusbc-rtl8117.lan:34663 in memory (size: 6.3 KiB, free: 26.8 GiB)\n",
      "23/10/03 12:21:24 DEBUG BlockManagerMaster: Updated info of block broadcast_3_piece0\n",
      "23/10/03 12:21:24 DEBUG BlockManager: Told master about block broadcast_3_piece0\n",
      "23/10/03 12:21:24 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 3, response is 0\n",
      "23/10/03 12:21:24 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to asusbc-rtl8117.lan:44641\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned broadcast 3\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(191)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 191\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 191\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(188)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 188\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 188\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(206)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 206\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 206\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(205)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 205\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 205\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(168)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 168\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 168\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(196)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 196\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 196\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(162)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 162\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 162\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(195)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 195\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 195\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(185)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 185\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 185\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(156)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 156\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 156\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(180)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 180\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 180\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(155)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 155\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 155\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(167)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 167\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 167\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(184)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 184\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 184\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(189)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 189\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 189\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(186)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 186\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 186\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(177)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 177\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 177\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(170)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 170\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 170\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(158)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 158\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 158\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(202)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 202\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 202\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(187)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 187\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 187\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(203)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 203\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 203\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(181)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 181\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 181\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(152)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 152\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 152\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(172)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 172\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 172\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(160)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 160\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 160\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(166)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 166\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 166\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(161)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 161\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 161\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(183)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 183\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 183\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(176)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 176\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 176\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(159)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 159\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 159\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(154)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 154\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 154\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(163)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 163\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 163\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(194)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 194\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 194\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(171)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 171\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 171\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(201)\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaning accumulator 201\n",
      "23/10/03 12:21:24 DEBUG ContextCleaner: Cleaned accumulator 201\n",
      "23/10/03 12:21:24 INFO JDBCRDD: closed connection\n",
      "23/10/03 12:21:24 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 4537 bytes result sent to driver\n",
      "23/10/03 12:21:24 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 0\n",
      "23/10/03 12:21:24 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 1185 ms on asusbc-rtl8117.lan (executor driver) (1/1)\n",
      "23/10/03 12:21:24 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "23/10/03 12:21:24 INFO DAGScheduler: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0) finished in 1.193 s\n",
      "23/10/03 12:21:24 DEBUG DAGScheduler: After removal of stage 4, remaining stages = 0\n",
      "23/10/03 12:21:24 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/10/03 12:21:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "23/10/03 12:21:24 INFO DAGScheduler: Job 4 finished: showString at NativeMethodAccessorImpl.java:0, took 1.196335 s\n",
      "23/10/03 12:21:24 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, false].toString, input[1, string, false].toString, input[2, string, false].toString, input[3, string, false].toString, input[4, string, false].toString, input[5, string, false].toString, input[6, string, false].toString, input[7, string, false].toString, StructField(toprettystring(c_custkey),StringType,false), StructField(toprettystring(c_name),StringType,false), StructField(toprettystring(c_address),StringType,false), StructField(toprettystring(c_nationkey),StringType,false), StructField(toprettystring(c_phone),StringType,false), StructField(toprettystring(c_acctbal),StringType,false), StructField(toprettystring(c_mktsegment),StringType,false), StructField(toprettystring(c_comment),StringType,false)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[8];\n",
      "/* 024 */     createExternalRow_0_0(i, values_0);\n",
      "/* 025 */     createExternalRow_0_1(i, values_0);\n",
      "/* 026 */     createExternalRow_0_2(i, values_0);\n",
      "/* 027 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 028 */     if (false) {\n",
      "/* 029 */       mutableRow.setNullAt(0);\n",
      "/* 030 */     } else {\n",
      "/* 031 */\n",
      "/* 032 */       mutableRow.update(0, value_0);\n",
      "/* 033 */     }\n",
      "/* 034 */\n",
      "/* 035 */     return mutableRow;\n",
      "/* 036 */   }\n",
      "/* 037 */\n",
      "/* 038 */\n",
      "/* 039 */   private void createExternalRow_0_2(InternalRow i, Object[] values_0) {\n",
      "/* 040 */\n",
      "/* 041 */     UTF8String value_14 = i.getUTF8String(6);\n",
      "/* 042 */     boolean isNull_13 = true;\n",
      "/* 043 */     java.lang.String value_13 = null;\n",
      "/* 044 */     isNull_13 = false;\n",
      "/* 045 */     if (!isNull_13) {\n",
      "/* 046 */\n",
      "/* 047 */       Object funcResult_6 = null;\n",
      "/* 048 */       funcResult_6 = value_14.toString();\n",
      "/* 049 */       value_13 = (java.lang.String) funcResult_6;\n",
      "/* 050 */\n",
      "/* 051 */     }\n",
      "/* 052 */     if (isNull_13) {\n",
      "/* 053 */       values_0[6] = null;\n",
      "/* 054 */     } else {\n",
      "/* 055 */       values_0[6] = value_13;\n",
      "/* 056 */     }\n",
      "/* 057 */\n",
      "/* 058 */     UTF8String value_16 = i.getUTF8String(7);\n",
      "/* 059 */     boolean isNull_15 = true;\n",
      "/* 060 */     java.lang.String value_15 = null;\n",
      "/* 061 */     isNull_15 = false;\n",
      "/* 062 */     if (!isNull_15) {\n",
      "/* 063 */\n",
      "/* 064 */       Object funcResult_7 = null;\n",
      "/* 065 */       funcResult_7 = value_16.toString();\n",
      "/* 066 */       value_15 = (java.lang.String) funcResult_7;\n",
      "/* 067 */\n",
      "/* 068 */     }\n",
      "/* 069 */     if (isNull_15) {\n",
      "/* 070 */       values_0[7] = null;\n",
      "/* 071 */     } else {\n",
      "/* 072 */       values_0[7] = value_15;\n",
      "/* 073 */     }\n",
      "/* 074 */\n",
      "/* 075 */   }\n",
      "/* 076 */\n",
      "/* 077 */\n",
      "/* 078 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {\n",
      "/* 079 */\n",
      "/* 080 */     UTF8String value_8 = i.getUTF8String(3);\n",
      "/* 081 */     boolean isNull_7 = true;\n",
      "/* 082 */     java.lang.String value_7 = null;\n",
      "/* 083 */     isNull_7 = false;\n",
      "/* 084 */     if (!isNull_7) {\n",
      "/* 085 */\n",
      "/* 086 */       Object funcResult_3 = null;\n",
      "/* 087 */       funcResult_3 = value_8.toString();\n",
      "/* 088 */       value_7 = (java.lang.String) funcResult_3;\n",
      "/* 089 */\n",
      "/* 090 */     }\n",
      "/* 091 */     if (isNull_7) {\n",
      "/* 092 */       values_0[3] = null;\n",
      "/* 093 */     } else {\n",
      "/* 094 */       values_0[3] = value_7;\n",
      "/* 095 */     }\n",
      "/* 096 */\n",
      "/* 097 */     UTF8String value_10 = i.getUTF8String(4);\n",
      "/* 098 */     boolean isNull_9 = true;\n",
      "/* 099 */     java.lang.String value_9 = null;\n",
      "/* 100 */     isNull_9 = false;\n",
      "/* 101 */     if (!isNull_9) {\n",
      "/* 102 */\n",
      "/* 103 */       Object funcResult_4 = null;\n",
      "/* 104 */       funcResult_4 = value_10.toString();\n",
      "/* 105 */       value_9 = (java.lang.String) funcResult_4;\n",
      "/* 106 */\n",
      "/* 107 */     }\n",
      "/* 108 */     if (isNull_9) {\n",
      "/* 109 */       values_0[4] = null;\n",
      "/* 110 */     } else {\n",
      "/* 111 */       values_0[4] = value_9;\n",
      "/* 112 */     }\n",
      "/* 113 */\n",
      "/* 114 */     UTF8String value_12 = i.getUTF8String(5);\n",
      "/* 115 */     boolean isNull_11 = true;\n",
      "/* 116 */     java.lang.String value_11 = null;\n",
      "/* 117 */     isNull_11 = false;\n",
      "/* 118 */     if (!isNull_11) {\n",
      "/* 119 */\n",
      "/* 120 */       Object funcResult_5 = null;\n",
      "/* 121 */       funcResult_5 = value_12.toString();\n",
      "/* 122 */       value_11 = (java.lang.String) funcResult_5;\n",
      "/* 123 */\n",
      "/* 124 */     }\n",
      "/* 125 */     if (isNull_11) {\n",
      "/* 126 */       values_0[5] = null;\n",
      "/* 127 */     } else {\n",
      "/* 128 */       values_0[5] = value_11;\n",
      "/* 129 */     }\n",
      "/* 130 */\n",
      "/* 131 */   }\n",
      "/* 132 */\n",
      "/* 133 */\n",
      "/* 134 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {\n",
      "/* 135 */\n",
      "/* 136 */     UTF8String value_2 = i.getUTF8String(0);\n",
      "/* 137 */     boolean isNull_1 = true;\n",
      "/* 138 */     java.lang.String value_1 = null;\n",
      "/* 139 */     isNull_1 = false;\n",
      "/* 140 */     if (!isNull_1) {\n",
      "/* 141 */\n",
      "/* 142 */       Object funcResult_0 = null;\n",
      "/* 143 */       funcResult_0 = value_2.toString();\n",
      "/* 144 */       value_1 = (java.lang.String) funcResult_0;\n",
      "/* 145 */\n",
      "/* 146 */     }\n",
      "/* 147 */     if (isNull_1) {\n",
      "/* 148 */       values_0[0] = null;\n",
      "/* 149 */     } else {\n",
      "/* 150 */       values_0[0] = value_1;\n",
      "/* 151 */     }\n",
      "/* 152 */\n",
      "/* 153 */     UTF8String value_4 = i.getUTF8String(1);\n",
      "/* 154 */     boolean isNull_3 = true;\n",
      "/* 155 */     java.lang.String value_3 = null;\n",
      "/* 156 */     isNull_3 = false;\n",
      "/* 157 */     if (!isNull_3) {\n",
      "/* 158 */\n",
      "/* 159 */       Object funcResult_1 = null;\n",
      "/* 160 */       funcResult_1 = value_4.toString();\n",
      "/* 161 */       value_3 = (java.lang.String) funcResult_1;\n",
      "/* 162 */\n",
      "/* 163 */     }\n",
      "/* 164 */     if (isNull_3) {\n",
      "/* 165 */       values_0[1] = null;\n",
      "/* 166 */     } else {\n",
      "/* 167 */       values_0[1] = value_3;\n",
      "/* 168 */     }\n",
      "/* 169 */\n",
      "/* 170 */     UTF8String value_6 = i.getUTF8String(2);\n",
      "/* 171 */     boolean isNull_5 = true;\n",
      "/* 172 */     java.lang.String value_5 = null;\n",
      "/* 173 */     isNull_5 = false;\n",
      "/* 174 */     if (!isNull_5) {\n",
      "/* 175 */\n",
      "/* 176 */       Object funcResult_2 = null;\n",
      "/* 177 */       funcResult_2 = value_6.toString();\n",
      "/* 178 */       value_5 = (java.lang.String) funcResult_2;\n",
      "/* 179 */\n",
      "/* 180 */     }\n",
      "/* 181 */     if (isNull_5) {\n",
      "/* 182 */       values_0[2] = null;\n",
      "/* 183 */     } else {\n",
      "/* 184 */       values_0[2] = value_5;\n",
      "/* 185 */     }\n",
      "/* 186 */\n",
      "/* 187 */   }\n",
      "/* 188 */\n",
      "/* 189 */ }\n",
      "\n",
      "23/10/03 12:21:24 DEBUG CodeGenerator: \n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[8];\n",
      "/* 024 */     createExternalRow_0_0(i, values_0);\n",
      "/* 025 */     createExternalRow_0_1(i, values_0);\n",
      "/* 026 */     createExternalRow_0_2(i, values_0);\n",
      "/* 027 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 028 */     if (false) {\n",
      "/* 029 */       mutableRow.setNullAt(0);\n",
      "/* 030 */     } else {\n",
      "/* 031 */\n",
      "/* 032 */       mutableRow.update(0, value_0);\n",
      "/* 033 */     }\n",
      "/* 034 */\n",
      "/* 035 */     return mutableRow;\n",
      "/* 036 */   }\n",
      "/* 037 */\n",
      "/* 038 */\n",
      "/* 039 */   private void createExternalRow_0_2(InternalRow i, Object[] values_0) {\n",
      "/* 040 */\n",
      "/* 041 */     UTF8String value_14 = i.getUTF8String(6);\n",
      "/* 042 */     boolean isNull_13 = true;\n",
      "/* 043 */     java.lang.String value_13 = null;\n",
      "/* 044 */     isNull_13 = false;\n",
      "/* 045 */     if (!isNull_13) {\n",
      "/* 046 */\n",
      "/* 047 */       Object funcResult_6 = null;\n",
      "/* 048 */       funcResult_6 = value_14.toString();\n",
      "/* 049 */       value_13 = (java.lang.String) funcResult_6;\n",
      "/* 050 */\n",
      "/* 051 */     }\n",
      "/* 052 */     if (isNull_13) {\n",
      "/* 053 */       values_0[6] = null;\n",
      "/* 054 */     } else {\n",
      "/* 055 */       values_0[6] = value_13;\n",
      "/* 056 */     }\n",
      "/* 057 */\n",
      "/* 058 */     UTF8String value_16 = i.getUTF8String(7);\n",
      "/* 059 */     boolean isNull_15 = true;\n",
      "/* 060 */     java.lang.String value_15 = null;\n",
      "/* 061 */     isNull_15 = false;\n",
      "/* 062 */     if (!isNull_15) {\n",
      "/* 063 */\n",
      "/* 064 */       Object funcResult_7 = null;\n",
      "/* 065 */       funcResult_7 = value_16.toString();\n",
      "/* 066 */       value_15 = (java.lang.String) funcResult_7;\n",
      "/* 067 */\n",
      "/* 068 */     }\n",
      "/* 069 */     if (isNull_15) {\n",
      "/* 070 */       values_0[7] = null;\n",
      "/* 071 */     } else {\n",
      "/* 072 */       values_0[7] = value_15;\n",
      "/* 073 */     }\n",
      "/* 074 */\n",
      "/* 075 */   }\n",
      "/* 076 */\n",
      "/* 077 */\n",
      "/* 078 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {\n",
      "/* 079 */\n",
      "/* 080 */     UTF8String value_8 = i.getUTF8String(3);\n",
      "/* 081 */     boolean isNull_7 = true;\n",
      "/* 082 */     java.lang.String value_7 = null;\n",
      "/* 083 */     isNull_7 = false;\n",
      "/* 084 */     if (!isNull_7) {\n",
      "/* 085 */\n",
      "/* 086 */       Object funcResult_3 = null;\n",
      "/* 087 */       funcResult_3 = value_8.toString();\n",
      "/* 088 */       value_7 = (java.lang.String) funcResult_3;\n",
      "/* 089 */\n",
      "/* 090 */     }\n",
      "/* 091 */     if (isNull_7) {\n",
      "/* 092 */       values_0[3] = null;\n",
      "/* 093 */     } else {\n",
      "/* 094 */       values_0[3] = value_7;\n",
      "/* 095 */     }\n",
      "/* 096 */\n",
      "/* 097 */     UTF8String value_10 = i.getUTF8String(4);\n",
      "/* 098 */     boolean isNull_9 = true;\n",
      "/* 099 */     java.lang.String value_9 = null;\n",
      "/* 100 */     isNull_9 = false;\n",
      "/* 101 */     if (!isNull_9) {\n",
      "/* 102 */\n",
      "/* 103 */       Object funcResult_4 = null;\n",
      "/* 104 */       funcResult_4 = value_10.toString();\n",
      "/* 105 */       value_9 = (java.lang.String) funcResult_4;\n",
      "/* 106 */\n",
      "/* 107 */     }\n",
      "/* 108 */     if (isNull_9) {\n",
      "/* 109 */       values_0[4] = null;\n",
      "/* 110 */     } else {\n",
      "/* 111 */       values_0[4] = value_9;\n",
      "/* 112 */     }\n",
      "/* 113 */\n",
      "/* 114 */     UTF8String value_12 = i.getUTF8String(5);\n",
      "/* 115 */     boolean isNull_11 = true;\n",
      "/* 116 */     java.lang.String value_11 = null;\n",
      "/* 117 */     isNull_11 = false;\n",
      "/* 118 */     if (!isNull_11) {\n",
      "/* 119 */\n",
      "/* 120 */       Object funcResult_5 = null;\n",
      "/* 121 */       funcResult_5 = value_12.toString();\n",
      "/* 122 */       value_11 = (java.lang.String) funcResult_5;\n",
      "/* 123 */\n",
      "/* 124 */     }\n",
      "/* 125 */     if (isNull_11) {\n",
      "/* 126 */       values_0[5] = null;\n",
      "/* 127 */     } else {\n",
      "/* 128 */       values_0[5] = value_11;\n",
      "/* 129 */     }\n",
      "/* 130 */\n",
      "/* 131 */   }\n",
      "/* 132 */\n",
      "/* 133 */\n",
      "/* 134 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {\n",
      "/* 135 */\n",
      "/* 136 */     UTF8String value_2 = i.getUTF8String(0);\n",
      "/* 137 */     boolean isNull_1 = true;\n",
      "/* 138 */     java.lang.String value_1 = null;\n",
      "/* 139 */     isNull_1 = false;\n",
      "/* 140 */     if (!isNull_1) {\n",
      "/* 141 */\n",
      "/* 142 */       Object funcResult_0 = null;\n",
      "/* 143 */       funcResult_0 = value_2.toString();\n",
      "/* 144 */       value_1 = (java.lang.String) funcResult_0;\n",
      "/* 145 */\n",
      "/* 146 */     }\n",
      "/* 147 */     if (isNull_1) {\n",
      "/* 148 */       values_0[0] = null;\n",
      "/* 149 */     } else {\n",
      "/* 150 */       values_0[0] = value_1;\n",
      "/* 151 */     }\n",
      "/* 152 */\n",
      "/* 153 */     UTF8String value_4 = i.getUTF8String(1);\n",
      "/* 154 */     boolean isNull_3 = true;\n",
      "/* 155 */     java.lang.String value_3 = null;\n",
      "/* 156 */     isNull_3 = false;\n",
      "/* 157 */     if (!isNull_3) {\n",
      "/* 158 */\n",
      "/* 159 */       Object funcResult_1 = null;\n",
      "/* 160 */       funcResult_1 = value_4.toString();\n",
      "/* 161 */       value_3 = (java.lang.String) funcResult_1;\n",
      "/* 162 */\n",
      "/* 163 */     }\n",
      "/* 164 */     if (isNull_3) {\n",
      "/* 165 */       values_0[1] = null;\n",
      "/* 166 */     } else {\n",
      "/* 167 */       values_0[1] = value_3;\n",
      "/* 168 */     }\n",
      "/* 169 */\n",
      "/* 170 */     UTF8String value_6 = i.getUTF8String(2);\n",
      "/* 171 */     boolean isNull_5 = true;\n",
      "/* 172 */     java.lang.String value_5 = null;\n",
      "/* 173 */     isNull_5 = false;\n",
      "/* 174 */     if (!isNull_5) {\n",
      "/* 175 */\n",
      "/* 176 */       Object funcResult_2 = null;\n",
      "/* 177 */       funcResult_2 = value_6.toString();\n",
      "/* 178 */       value_5 = (java.lang.String) funcResult_2;\n",
      "/* 179 */\n",
      "/* 180 */     }\n",
      "/* 181 */     if (isNull_5) {\n",
      "/* 182 */       values_0[2] = null;\n",
      "/* 183 */     } else {\n",
      "/* 184 */       values_0[2] = value_5;\n",
      "/* 185 */     }\n",
      "/* 186 */\n",
      "/* 187 */   }\n",
      "/* 188 */\n",
      "/* 189 */ }\n",
      "\n",
      "23/10/03 12:21:24 INFO CodeGenerator: Code generated in 8.483344 ms\n",
      "23/10/03 12:21:24 DEBUG SparkSqlParser: Parsing command: customer\n",
      "23/10/03 12:21:24 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:24 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:24 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:24 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:24 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:24 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:24 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:24 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:24 DEBUG BasicConnectionProvider: JDBC connection initiated with URL: jdbc:postgresql://localhost:5432/tpch and properties: {password=tpch, user=tpch}\n",
      "23/10/03 12:21:24 DEBUG CatalystSqlParser: Parsing command: char(1)\n",
      "23/10/03 12:21:24 DEBUG CatalystSqlParser: Parsing command: char(15)\n",
      "23/10/03 12:21:24 DEBUG CatalystSqlParser: Parsing command: char(15)\n",
      "23/10/03 12:21:24 DEBUG CatalystSqlParser: Parsing command: varchar(79)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+--------------------+-----------+---------------+--------------------+------------+--------------------+\n",
      "|c_custkey|            c_name|           c_address|c_nationkey|        c_phone|           c_acctbal|c_mktsegment|           c_comment|\n",
      "+---------+------------------+--------------------+-----------+---------------+--------------------+------------+--------------------+\n",
      "|   523181|Customer#000523181|2Q8rumrtGDhWPXH,2...|         15|25-514-857-4459|1419.920000000000...|  HOUSEHOLD |requests sleep fi...|\n",
      "|   523182|Customer#000523182|       tGObfyDmLAA,F|         12|22-264-725-8704|3621.760000000000...|  BUILDING  |posits. pending r...|\n",
      "|   523183|Customer#000523183|          7wfNxigT4h|         16|26-125-929-2482|5915.310000000000...|  HOUSEHOLD |gular platelets. ...|\n",
      "|   523184|Customer#000523184|p7dgMHkd mwmLBZEF...|         11|21-616-381-9516|9718.400000000000...|  BUILDING  |platelets are flu...|\n",
      "|   523185|Customer#000523185|4fGZG4SYboME5yJJr...|          6|16-709-983-6038|7285.630000000000...|  HOUSEHOLD | boost about the ...|\n",
      "|   523186|Customer#000523186|   ,vIAbTIDfsu0 de9w|          5|15-686-887-6703|6556.120000000000...|  MACHINERY |tructions. perman...|\n",
      "|   523187|Customer#000523187|xWhA74ww7fam2DXTh...|         11|21-398-725-9896|-312.570000000000...|  HOUSEHOLD |s. unusual, ironi...|\n",
      "|   523188|Customer#000523188|4QcuLBd2WaZ5X1ll1...|         19|29-125-956-8920|2485.260000000000...|  BUILDING  |y even deposits. ...|\n",
      "|   523189|Customer#000523189|gUdj2bqOA4Q4svzxX...|          5|15-694-754-5985|679.0600000000000...|  AUTOMOBILE|use deposits. unu...|\n",
      "|   523190|Customer#000523190|PIkYNEk8b63xwRkk2...|         18|28-352-431-4522|3867.200000000000...|  FURNITURE |into beans nag bl...|\n",
      "|   523191|Customer#000523191|       9ISP1XmIgzjDv|         10|20-422-350-5621|6090.400000000000...|  AUTOMOBILE|, silent warhorse...|\n",
      "|   523192|Customer#000523192|    S9F2LNE2ca5rDcww|          5|15-836-742-5418|4343.790000000000...|  BUILDING  | slyly regular de...|\n",
      "|   523193|Customer#000523193|p6zJHICy3PDZATdhg...|          5|15-612-235-9948|8019.000000000000...|  MACHINERY |nic platelets-- c...|\n",
      "|   523194|Customer#000523194|WRoZgbsWMKsGyLwG2...|          8|18-339-539-3988|1377.020000000000...|  FURNITURE |h fluffily above ...|\n",
      "|   523195|Customer#000523195|      KzNqh5H4jijShP|         18|28-263-617-7203|8460.980000000000...|  MACHINERY |blithely pending ...|\n",
      "|   523196|Customer#000523196|bt1N0IJfsgS9LBEhpFK,|          3|13-641-303-6203|4689.940000000000...|  BUILDING  |the blithely unus...|\n",
      "|   523197|Customer#000523197|FDEqDeYNGJI0DiKa7...|         19|29-605-324-4629|6815.960000000000...|  AUTOMOBILE|even requests. ca...|\n",
      "|   523198|Customer#000523198|Z0e380Cul5FThpboJ...|         19|29-401-964-6992|-203.150000000000...|  AUTOMOBILE|lar excuses haggl...|\n",
      "|   523199|Customer#000523199|INBo1Z6bO G8gFVg9...|          7|17-136-313-9269|4530.310000000000...|  HOUSEHOLD | express requests...|\n",
      "|   523200|Customer#000523200|          7X5paj FB3|         18|28-530-360-5430|7901.130000000000...|  HOUSEHOLD | deposits cajole ...|\n",
      "+---------+------------------+--------------------+-----------+---------------+--------------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "orders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/03 12:21:24 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator scan_input_0;\n",
      "/* 010 */   private boolean project_resultIsNull_0;\n",
      "/* 011 */   private int project_argValue_0;\n",
      "/* 012 */   private boolean project_resultIsNull_1;\n",
      "/* 013 */   private int project_argValue_1;\n",
      "/* 014 */   private boolean project_resultIsNull_2;\n",
      "/* 015 */   private int project_argValue_2;\n",
      "/* 016 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] scan_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 017 */   private UTF8String[] project_mutableStateArray_0 = new UTF8String[3];\n",
      "/* 018 */\n",
      "/* 019 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 020 */     this.references = references;\n",
      "/* 021 */   }\n",
      "/* 022 */\n",
      "/* 023 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 024 */     partitionIndex = index;\n",
      "/* 025 */     this.inputs = inputs;\n",
      "/* 026 */     scan_input_0 = inputs[0];\n",
      "/* 027 */     scan_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 160);\n",
      "/* 028 */\n",
      "/* 029 */     scan_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 288);\n",
      "/* 030 */\n",
      "/* 031 */   }\n",
      "/* 032 */\n",
      "/* 033 */   protected void processNext() throws java.io.IOException {\n",
      "/* 034 */     while ( scan_input_0.hasNext()) {\n",
      "/* 035 */       InternalRow scan_row_0 = (InternalRow) scan_input_0.next();\n",
      "/* 036 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 037 */       // common sub-expressions\n",
      "/* 038 */\n",
      "/* 039 */       boolean scan_isNull_0 = scan_row_0.isNullAt(0);\n",
      "/* 040 */       int scan_value_0 = scan_isNull_0 ?\n",
      "/* 041 */       -1 : (scan_row_0.getInt(0));\n",
      "/* 042 */       UTF8String project_value_0;\n",
      "/* 043 */       if (scan_isNull_0) {\n",
      "/* 044 */         project_value_0 = UTF8String.fromString(\"NULL\");\n",
      "/* 045 */       } else {\n",
      "/* 046 */         project_value_0 = UTF8String.fromString(String.valueOf(scan_value_0));\n",
      "/* 047 */       }\n",
      "/* 048 */       boolean scan_isNull_1 = scan_row_0.isNullAt(1);\n",
      "/* 049 */       long scan_value_1 = scan_isNull_1 ?\n",
      "/* 050 */       -1L : (scan_row_0.getLong(1));\n",
      "/* 051 */       UTF8String project_value_2;\n",
      "/* 052 */       if (scan_isNull_1) {\n",
      "/* 053 */         project_value_2 = UTF8String.fromString(\"NULL\");\n",
      "/* 054 */       } else {\n",
      "/* 055 */         project_value_2 = UTF8String.fromString(String.valueOf(scan_value_1));\n",
      "/* 056 */       }\n",
      "/* 057 */       project_resultIsNull_0 = false;\n",
      "/* 058 */\n",
      "/* 059 */       if (!project_resultIsNull_0) {\n",
      "/* 060 */         boolean scan_isNull_2 = scan_row_0.isNullAt(2);\n",
      "/* 061 */         UTF8String scan_value_2 = scan_isNull_2 ?\n",
      "/* 062 */         null : (scan_row_0.getUTF8String(2));\n",
      "/* 063 */         project_resultIsNull_0 = scan_isNull_2;\n",
      "/* 064 */         project_mutableStateArray_0[0] = scan_value_2;\n",
      "/* 065 */       }\n",
      "/* 066 */\n",
      "/* 067 */       if (!project_resultIsNull_0) {\n",
      "/* 068 */         project_argValue_0 = 1;\n",
      "/* 069 */       }\n",
      "/* 070 */\n",
      "/* 071 */       boolean project_isNull_5 = project_resultIsNull_0;\n",
      "/* 072 */       UTF8String project_value_5 = null;\n",
      "/* 073 */       if (!project_resultIsNull_0) {\n",
      "/* 074 */         project_value_5 = org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils.readSidePadding(project_mutableStateArray_0[0], project_argValue_0);\n",
      "/* 075 */       }\n",
      "/* 076 */       UTF8String project_value_4;\n",
      "/* 077 */       if (project_isNull_5) {\n",
      "/* 078 */         project_value_4 = UTF8String.fromString(\"NULL\");\n",
      "/* 079 */       } else {\n",
      "/* 080 */         project_value_4 = project_value_5;\n",
      "/* 081 */       }\n",
      "/* 082 */       boolean scan_isNull_3 = scan_row_0.isNullAt(3);\n",
      "/* 083 */       Decimal scan_value_3 = scan_isNull_3 ?\n",
      "/* 084 */       null : (scan_row_0.getDecimal(3, 38, 18));\n",
      "/* 085 */       UTF8String project_value_8;\n",
      "/* 086 */       if (scan_isNull_3) {\n",
      "/* 087 */         project_value_8 = UTF8String.fromString(\"NULL\");\n",
      "/* 088 */       } else {\n",
      "/* 089 */         project_value_8 = UTF8String.fromString(scan_value_3.toPlainString());\n",
      "/* 090 */       }\n",
      "/* 091 */       boolean scan_isNull_4 = scan_row_0.isNullAt(4);\n",
      "/* 092 */       int scan_value_4 = scan_isNull_4 ?\n",
      "/* 093 */       -1 : (scan_row_0.getInt(4));\n",
      "/* 094 */       UTF8String project_value_10;\n",
      "/* 095 */       if (scan_isNull_4) {\n",
      "/* 096 */         project_value_10 = UTF8String.fromString(\"NULL\");\n",
      "/* 097 */       } else {\n",
      "/* 098 */         project_value_10 = UTF8String.fromString(((org.apache.spark.sql.catalyst.util.DefaultDateFormatter) references[1] /* dateFormatter */).format(scan_value_4));\n",
      "/* 099 */       }\n",
      "/* 100 */       project_resultIsNull_1 = false;\n",
      "/* 101 */\n",
      "/* 102 */       if (!project_resultIsNull_1) {\n",
      "/* 103 */         boolean scan_isNull_5 = scan_row_0.isNullAt(5);\n",
      "/* 104 */         UTF8String scan_value_5 = scan_isNull_5 ?\n",
      "/* 105 */         null : (scan_row_0.getUTF8String(5));\n",
      "/* 106 */         project_resultIsNull_1 = scan_isNull_5;\n",
      "/* 107 */         project_mutableStateArray_0[1] = scan_value_5;\n",
      "/* 108 */       }\n",
      "/* 109 */\n",
      "/* 110 */       if (!project_resultIsNull_1) {\n",
      "/* 111 */         project_argValue_1 = 15;\n",
      "/* 112 */       }\n",
      "/* 113 */\n",
      "/* 114 */       boolean project_isNull_13 = project_resultIsNull_1;\n",
      "/* 115 */       UTF8String project_value_13 = null;\n",
      "/* 116 */       if (!project_resultIsNull_1) {\n",
      "/* 117 */         project_value_13 = org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils.readSidePadding(project_mutableStateArray_0[1], project_argValue_1);\n",
      "/* 118 */       }\n",
      "/* 119 */       UTF8String project_value_12;\n",
      "/* 120 */       if (project_isNull_13) {\n",
      "/* 121 */         project_value_12 = UTF8String.fromString(\"NULL\");\n",
      "/* 122 */       } else {\n",
      "/* 123 */         project_value_12 = project_value_13;\n",
      "/* 124 */       }\n",
      "/* 125 */       project_resultIsNull_2 = false;\n",
      "/* 126 */\n",
      "/* 127 */       if (!project_resultIsNull_2) {\n",
      "/* 128 */         boolean scan_isNull_6 = scan_row_0.isNullAt(6);\n",
      "/* 129 */         UTF8String scan_value_6 = scan_isNull_6 ?\n",
      "/* 130 */         null : (scan_row_0.getUTF8String(6));\n",
      "/* 131 */         project_resultIsNull_2 = scan_isNull_6;\n",
      "/* 132 */         project_mutableStateArray_0[2] = scan_value_6;\n",
      "/* 133 */       }\n",
      "/* 134 */\n",
      "/* 135 */       if (!project_resultIsNull_2) {\n",
      "/* 136 */         project_argValue_2 = 15;\n",
      "/* 137 */       }\n",
      "/* 138 */\n",
      "/* 139 */       boolean project_isNull_17 = project_resultIsNull_2;\n",
      "/* 140 */       UTF8String project_value_17 = null;\n",
      "/* 141 */       if (!project_resultIsNull_2) {\n",
      "/* 142 */         project_value_17 = org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils.readSidePadding(project_mutableStateArray_0[2], project_argValue_2);\n",
      "/* 143 */       }\n",
      "/* 144 */       UTF8String project_value_16;\n",
      "/* 145 */       if (project_isNull_17) {\n",
      "/* 146 */         project_value_16 = UTF8String.fromString(\"NULL\");\n",
      "/* 147 */       } else {\n",
      "/* 148 */         project_value_16 = project_value_17;\n",
      "/* 149 */       }\n",
      "/* 150 */       boolean scan_isNull_7 = scan_row_0.isNullAt(7);\n",
      "/* 151 */       int scan_value_7 = scan_isNull_7 ?\n",
      "/* 152 */       -1 : (scan_row_0.getInt(7));\n",
      "/* 153 */       UTF8String project_value_20;\n",
      "/* 154 */       if (scan_isNull_7) {\n",
      "/* 155 */         project_value_20 = UTF8String.fromString(\"NULL\");\n",
      "/* 156 */       } else {\n",
      "/* 157 */         project_value_20 = UTF8String.fromString(String.valueOf(scan_value_7));\n",
      "/* 158 */       }\n",
      "/* 159 */       boolean scan_isNull_8 = scan_row_0.isNullAt(8);\n",
      "/* 160 */       UTF8String scan_value_8 = scan_isNull_8 ?\n",
      "/* 161 */       null : (scan_row_0.getUTF8String(8));\n",
      "/* 162 */       UTF8String project_value_22;\n",
      "/* 163 */       if (scan_isNull_8) {\n",
      "/* 164 */         project_value_22 = UTF8String.fromString(\"NULL\");\n",
      "/* 165 */       } else {\n",
      "/* 166 */         project_value_22 = scan_value_8;\n",
      "/* 167 */       }\n",
      "/* 168 */       scan_mutableStateArray_0[1].reset();\n",
      "/* 169 */\n",
      "/* 170 */       scan_mutableStateArray_0[1].write(0, project_value_0);\n",
      "/* 171 */\n",
      "/* 172 */       scan_mutableStateArray_0[1].write(1, project_value_2);\n",
      "/* 173 */\n",
      "/* 174 */       scan_mutableStateArray_0[1].write(2, project_value_4);\n",
      "/* 175 */\n",
      "/* 176 */       scan_mutableStateArray_0[1].write(3, project_value_8);\n",
      "/* 177 */\n",
      "/* 178 */       scan_mutableStateArray_0[1].write(4, project_value_10);\n",
      "/* 179 */\n",
      "/* 180 */       scan_mutableStateArray_0[1].write(5, project_value_12);\n",
      "/* 181 */\n",
      "/* 182 */       scan_mutableStateArray_0[1].write(6, project_value_16);\n",
      "/* 183 */\n",
      "/* 184 */       scan_mutableStateArray_0[1].write(7, project_value_20);\n",
      "/* 185 */\n",
      "/* 186 */       scan_mutableStateArray_0[1].write(8, project_value_22);\n",
      "/* 187 */       append((scan_mutableStateArray_0[1].getRow()));\n",
      "/* 188 */       if (shouldStop()) return;\n",
      "/* 189 */     }\n",
      "/* 190 */   }\n",
      "/* 191 */\n",
      "/* 192 */ }\n",
      "\n",
      "23/10/03 12:21:24 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator scan_input_0;\n",
      "/* 010 */   private boolean project_resultIsNull_0;\n",
      "/* 011 */   private int project_argValue_0;\n",
      "/* 012 */   private boolean project_resultIsNull_1;\n",
      "/* 013 */   private int project_argValue_1;\n",
      "/* 014 */   private boolean project_resultIsNull_2;\n",
      "/* 015 */   private int project_argValue_2;\n",
      "/* 016 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] scan_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 017 */   private UTF8String[] project_mutableStateArray_0 = new UTF8String[3];\n",
      "/* 018 */\n",
      "/* 019 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 020 */     this.references = references;\n",
      "/* 021 */   }\n",
      "/* 022 */\n",
      "/* 023 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 024 */     partitionIndex = index;\n",
      "/* 025 */     this.inputs = inputs;\n",
      "/* 026 */     scan_input_0 = inputs[0];\n",
      "/* 027 */     scan_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 160);\n",
      "/* 028 */\n",
      "/* 029 */     scan_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 288);\n",
      "/* 030 */\n",
      "/* 031 */   }\n",
      "/* 032 */\n",
      "/* 033 */   protected void processNext() throws java.io.IOException {\n",
      "/* 034 */     while ( scan_input_0.hasNext()) {\n",
      "/* 035 */       InternalRow scan_row_0 = (InternalRow) scan_input_0.next();\n",
      "/* 036 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 037 */       // common sub-expressions\n",
      "/* 038 */\n",
      "/* 039 */       boolean scan_isNull_0 = scan_row_0.isNullAt(0);\n",
      "/* 040 */       int scan_value_0 = scan_isNull_0 ?\n",
      "/* 041 */       -1 : (scan_row_0.getInt(0));\n",
      "/* 042 */       UTF8String project_value_0;\n",
      "/* 043 */       if (scan_isNull_0) {\n",
      "/* 044 */         project_value_0 = UTF8String.fromString(\"NULL\");\n",
      "/* 045 */       } else {\n",
      "/* 046 */         project_value_0 = UTF8String.fromString(String.valueOf(scan_value_0));\n",
      "/* 047 */       }\n",
      "/* 048 */       boolean scan_isNull_1 = scan_row_0.isNullAt(1);\n",
      "/* 049 */       long scan_value_1 = scan_isNull_1 ?\n",
      "/* 050 */       -1L : (scan_row_0.getLong(1));\n",
      "/* 051 */       UTF8String project_value_2;\n",
      "/* 052 */       if (scan_isNull_1) {\n",
      "/* 053 */         project_value_2 = UTF8String.fromString(\"NULL\");\n",
      "/* 054 */       } else {\n",
      "/* 055 */         project_value_2 = UTF8String.fromString(String.valueOf(scan_value_1));\n",
      "/* 056 */       }\n",
      "/* 057 */       project_resultIsNull_0 = false;\n",
      "/* 058 */\n",
      "/* 059 */       if (!project_resultIsNull_0) {\n",
      "/* 060 */         boolean scan_isNull_2 = scan_row_0.isNullAt(2);\n",
      "/* 061 */         UTF8String scan_value_2 = scan_isNull_2 ?\n",
      "/* 062 */         null : (scan_row_0.getUTF8String(2));\n",
      "/* 063 */         project_resultIsNull_0 = scan_isNull_2;\n",
      "/* 064 */         project_mutableStateArray_0[0] = scan_value_2;\n",
      "/* 065 */       }\n",
      "/* 066 */\n",
      "/* 067 */       if (!project_resultIsNull_0) {\n",
      "/* 068 */         project_argValue_0 = 1;\n",
      "/* 069 */       }\n",
      "/* 070 */\n",
      "/* 071 */       boolean project_isNull_5 = project_resultIsNull_0;\n",
      "/* 072 */       UTF8String project_value_5 = null;\n",
      "/* 073 */       if (!project_resultIsNull_0) {\n",
      "/* 074 */         project_value_5 = org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils.readSidePadding(project_mutableStateArray_0[0], project_argValue_0);\n",
      "/* 075 */       }\n",
      "/* 076 */       UTF8String project_value_4;\n",
      "/* 077 */       if (project_isNull_5) {\n",
      "/* 078 */         project_value_4 = UTF8String.fromString(\"NULL\");\n",
      "/* 079 */       } else {\n",
      "/* 080 */         project_value_4 = project_value_5;\n",
      "/* 081 */       }\n",
      "/* 082 */       boolean scan_isNull_3 = scan_row_0.isNullAt(3);\n",
      "/* 083 */       Decimal scan_value_3 = scan_isNull_3 ?\n",
      "/* 084 */       null : (scan_row_0.getDecimal(3, 38, 18));\n",
      "/* 085 */       UTF8String project_value_8;\n",
      "/* 086 */       if (scan_isNull_3) {\n",
      "/* 087 */         project_value_8 = UTF8String.fromString(\"NULL\");\n",
      "/* 088 */       } else {\n",
      "/* 089 */         project_value_8 = UTF8String.fromString(scan_value_3.toPlainString());\n",
      "/* 090 */       }\n",
      "/* 091 */       boolean scan_isNull_4 = scan_row_0.isNullAt(4);\n",
      "/* 092 */       int scan_value_4 = scan_isNull_4 ?\n",
      "/* 093 */       -1 : (scan_row_0.getInt(4));\n",
      "/* 094 */       UTF8String project_value_10;\n",
      "/* 095 */       if (scan_isNull_4) {\n",
      "/* 096 */         project_value_10 = UTF8String.fromString(\"NULL\");\n",
      "/* 097 */       } else {\n",
      "/* 098 */         project_value_10 = UTF8String.fromString(((org.apache.spark.sql.catalyst.util.DefaultDateFormatter) references[1] /* dateFormatter */).format(scan_value_4));\n",
      "/* 099 */       }\n",
      "/* 100 */       project_resultIsNull_1 = false;\n",
      "/* 101 */\n",
      "/* 102 */       if (!project_resultIsNull_1) {\n",
      "/* 103 */         boolean scan_isNull_5 = scan_row_0.isNullAt(5);\n",
      "/* 104 */         UTF8String scan_value_5 = scan_isNull_5 ?\n",
      "/* 105 */         null : (scan_row_0.getUTF8String(5));\n",
      "/* 106 */         project_resultIsNull_1 = scan_isNull_5;\n",
      "/* 107 */         project_mutableStateArray_0[1] = scan_value_5;\n",
      "/* 108 */       }\n",
      "/* 109 */\n",
      "/* 110 */       if (!project_resultIsNull_1) {\n",
      "/* 111 */         project_argValue_1 = 15;\n",
      "/* 112 */       }\n",
      "/* 113 */\n",
      "/* 114 */       boolean project_isNull_13 = project_resultIsNull_1;\n",
      "/* 115 */       UTF8String project_value_13 = null;\n",
      "/* 116 */       if (!project_resultIsNull_1) {\n",
      "/* 117 */         project_value_13 = org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils.readSidePadding(project_mutableStateArray_0[1], project_argValue_1);\n",
      "/* 118 */       }\n",
      "/* 119 */       UTF8String project_value_12;\n",
      "/* 120 */       if (project_isNull_13) {\n",
      "/* 121 */         project_value_12 = UTF8String.fromString(\"NULL\");\n",
      "/* 122 */       } else {\n",
      "/* 123 */         project_value_12 = project_value_13;\n",
      "/* 124 */       }\n",
      "/* 125 */       project_resultIsNull_2 = false;\n",
      "/* 126 */\n",
      "/* 127 */       if (!project_resultIsNull_2) {\n",
      "/* 128 */         boolean scan_isNull_6 = scan_row_0.isNullAt(6);\n",
      "/* 129 */         UTF8String scan_value_6 = scan_isNull_6 ?\n",
      "/* 130 */         null : (scan_row_0.getUTF8String(6));\n",
      "/* 131 */         project_resultIsNull_2 = scan_isNull_6;\n",
      "/* 132 */         project_mutableStateArray_0[2] = scan_value_6;\n",
      "/* 133 */       }\n",
      "/* 134 */\n",
      "/* 135 */       if (!project_resultIsNull_2) {\n",
      "/* 136 */         project_argValue_2 = 15;\n",
      "/* 137 */       }\n",
      "/* 138 */\n",
      "/* 139 */       boolean project_isNull_17 = project_resultIsNull_2;\n",
      "/* 140 */       UTF8String project_value_17 = null;\n",
      "/* 141 */       if (!project_resultIsNull_2) {\n",
      "/* 142 */         project_value_17 = org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils.readSidePadding(project_mutableStateArray_0[2], project_argValue_2);\n",
      "/* 143 */       }\n",
      "/* 144 */       UTF8String project_value_16;\n",
      "/* 145 */       if (project_isNull_17) {\n",
      "/* 146 */         project_value_16 = UTF8String.fromString(\"NULL\");\n",
      "/* 147 */       } else {\n",
      "/* 148 */         project_value_16 = project_value_17;\n",
      "/* 149 */       }\n",
      "/* 150 */       boolean scan_isNull_7 = scan_row_0.isNullAt(7);\n",
      "/* 151 */       int scan_value_7 = scan_isNull_7 ?\n",
      "/* 152 */       -1 : (scan_row_0.getInt(7));\n",
      "/* 153 */       UTF8String project_value_20;\n",
      "/* 154 */       if (scan_isNull_7) {\n",
      "/* 155 */         project_value_20 = UTF8String.fromString(\"NULL\");\n",
      "/* 156 */       } else {\n",
      "/* 157 */         project_value_20 = UTF8String.fromString(String.valueOf(scan_value_7));\n",
      "/* 158 */       }\n",
      "/* 159 */       boolean scan_isNull_8 = scan_row_0.isNullAt(8);\n",
      "/* 160 */       UTF8String scan_value_8 = scan_isNull_8 ?\n",
      "/* 161 */       null : (scan_row_0.getUTF8String(8));\n",
      "/* 162 */       UTF8String project_value_22;\n",
      "/* 163 */       if (scan_isNull_8) {\n",
      "/* 164 */         project_value_22 = UTF8String.fromString(\"NULL\");\n",
      "/* 165 */       } else {\n",
      "/* 166 */         project_value_22 = scan_value_8;\n",
      "/* 167 */       }\n",
      "/* 168 */       scan_mutableStateArray_0[1].reset();\n",
      "/* 169 */\n",
      "/* 170 */       scan_mutableStateArray_0[1].write(0, project_value_0);\n",
      "/* 171 */\n",
      "/* 172 */       scan_mutableStateArray_0[1].write(1, project_value_2);\n",
      "/* 173 */\n",
      "/* 174 */       scan_mutableStateArray_0[1].write(2, project_value_4);\n",
      "/* 175 */\n",
      "/* 176 */       scan_mutableStateArray_0[1].write(3, project_value_8);\n",
      "/* 177 */\n",
      "/* 178 */       scan_mutableStateArray_0[1].write(4, project_value_10);\n",
      "/* 179 */\n",
      "/* 180 */       scan_mutableStateArray_0[1].write(5, project_value_12);\n",
      "/* 181 */\n",
      "/* 182 */       scan_mutableStateArray_0[1].write(6, project_value_16);\n",
      "/* 183 */\n",
      "/* 184 */       scan_mutableStateArray_0[1].write(7, project_value_20);\n",
      "/* 185 */\n",
      "/* 186 */       scan_mutableStateArray_0[1].write(8, project_value_22);\n",
      "/* 187 */       append((scan_mutableStateArray_0[1].getRow()));\n",
      "/* 188 */       if (shouldStop()) return;\n",
      "/* 189 */     }\n",
      "/* 190 */   }\n",
      "/* 191 */\n",
      "/* 192 */ }\n",
      "\n",
      "23/10/03 12:21:24 INFO CodeGenerator: Code generated in 12.007419 ms\n",
      "23/10/03 12:21:24 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "23/10/03 12:21:24 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "23/10/03 12:21:24 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$executeTake$2\n",
      "23/10/03 12:21:24 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++\n",
      "23/10/03 12:21:24 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "23/10/03 12:21:24 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "23/10/03 12:21:24 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/10/03 12:21:24 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 17 took 0.000048 seconds\n",
      "23/10/03 12:21:24 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "23/10/03 12:21:24 INFO DAGScheduler: Got job 5 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/10/03 12:21:24 INFO DAGScheduler: Final stage: ResultStage 5 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/10/03 12:21:24 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/10/03 12:21:24 INFO DAGScheduler: Missing parents: List()\n",
      "23/10/03 12:21:24 DEBUG DAGScheduler: submitStage(ResultStage 5 (name=showString at NativeMethodAccessorImpl.java:0;jobs=5))\n",
      "23/10/03 12:21:24 DEBUG DAGScheduler: missing: List()\n",
      "23/10/03 12:21:24 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[17] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/10/03 12:21:24 DEBUG DAGScheduler: submitMissingTasks(ResultStage 5)\n",
      "23/10/03 12:21:24 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 16.9 KiB, free 26.8 GiB)\n",
      "23/10/03 12:21:24 DEBUG BlockManager: Put block broadcast_5 locally took 0 ms\n",
      "23/10/03 12:21:24 DEBUG BlockManager: Putting block broadcast_5 without replication took 0 ms\n",
      "23/10/03 12:21:24 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 26.8 GiB)\n",
      "23/10/03 12:21:24 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_5_piece0 for BlockManagerId(driver, asusbc-rtl8117.lan, 34663, None)\n",
      "23/10/03 12:21:24 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on asusbc-rtl8117.lan:34663 (size: 7.7 KiB, free: 26.8 GiB)\n",
      "23/10/03 12:21:24 DEBUG BlockManagerMaster: Updated info of block broadcast_5_piece0\n",
      "23/10/03 12:21:24 DEBUG BlockManager: Told master about block broadcast_5_piece0\n",
      "23/10/03 12:21:24 DEBUG BlockManager: Put block broadcast_5_piece0 locally took 0 ms\n",
      "23/10/03 12:21:24 DEBUG BlockManager: Putting block broadcast_5_piece0 without replication took 0 ms\n",
      "23/10/03 12:21:24 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580\n",
      "23/10/03 12:21:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[17] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/10/03 12:21:24 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
      "23/10/03 12:21:24 DEBUG TaskSetManager: Epoch for TaskSet 5.0: 0\n",
      "23/10/03 12:21:24 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "23/10/03 12:21:24 DEBUG TaskSetManager: Valid locality levels for TaskSet 5.0: NO_PREF, ANY\n",
      "23/10/03 12:21:24 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_5.0, runningTasks: 0\n",
      "23/10/03 12:21:24 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (asusbc-rtl8117.lan, executor driver, partition 0, PROCESS_LOCAL, 7647 bytes) \n",
      "23/10/03 12:21:24 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "23/10/03 12:21:24 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)\n",
      "23/10/03 12:21:24 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 1\n",
      "23/10/03 12:21:24 DEBUG BlockManager: Getting local block broadcast_5\n",
      "23/10/03 12:21:24 DEBUG BlockManager: Level for block broadcast_5 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "23/10/03 12:21:24 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:24 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:24 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:24 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:24 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:24 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:24 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:24 DEBUG BasicConnectionProvider: JDBC connection initiated with URL: jdbc:postgresql://localhost:5432/tpch and properties: {password=tpch, user=tpch}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(212)1) / 1]\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 212\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 212\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(209)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 209\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 209\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(260)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 260\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 260\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(219)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 219\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 219\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(251)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 251\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 251\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(242)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 242\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 242\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(243)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 243\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 243\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(244)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 244\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 244\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(222)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 222\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 222\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(254)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 254\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 254\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(225)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 225\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 225\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(257)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 257\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 257\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(262)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 262\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 262\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(226)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 226\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 226\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(258)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 258\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 258\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(259)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 259\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 259\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(265)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 265\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 265\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(261)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 261\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 261\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(241)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 241\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 241\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(231)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 231\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 231\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(255)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 255\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 255\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(235)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 235\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 235\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(227)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 227\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 227\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(223)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 223\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 223\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(215)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 215\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 215\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(252)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 252\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 252\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(248)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 248\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 248\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(245)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 245\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 245\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(214)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 214\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 214\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(213)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 213\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 213\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(210)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 210\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 210\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(216)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 216\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 216\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(229)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 229\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 229\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(263)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 263\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 263\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(249)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 249\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 249\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(238)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 238\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 238\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(264)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 264\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 264\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(230)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 230\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 230\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(250)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 250\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 250\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(253)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 253\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 253\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(233)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 233\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 233\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(221)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 221\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 221\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(218)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 218\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 218\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(211)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 211\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 211\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(256)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 256\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 256\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(237)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 237\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 237\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(224)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 224\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 224\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(4)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning broadcast 4\n",
      "23/10/03 12:21:27 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 4\n",
      "23/10/03 12:21:27 DEBUG BlockManagerStorageEndpoint: removing broadcast 4\n",
      "23/10/03 12:21:27 DEBUG BlockManager: Removing broadcast 4\n",
      "23/10/03 12:21:27 DEBUG BlockManager: Removing block broadcast_4_piece0\n",
      "23/10/03 12:21:27 DEBUG MemoryStore: Block broadcast_4_piece0 of size 7193 dropped from memory (free 28802244890)\n",
      "23/10/03 12:21:27 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_4_piece0 for BlockManagerId(driver, asusbc-rtl8117.lan, 34663, None)\n",
      "23/10/03 12:21:27 INFO BlockManagerInfo: Removed broadcast_4_piece0 on asusbc-rtl8117.lan:34663 in memory (size: 7.0 KiB, free: 26.8 GiB)\n",
      "23/10/03 12:21:27 DEBUG BlockManagerMaster: Updated info of block broadcast_4_piece0\n",
      "23/10/03 12:21:27 DEBUG BlockManager: Told master about block broadcast_4_piece0\n",
      "23/10/03 12:21:27 DEBUG BlockManager: Removing block broadcast_4\n",
      "23/10/03 12:21:27 DEBUG MemoryStore: Block broadcast_4 of size 15464 dropped from memory (free 28802260354)\n",
      "23/10/03 12:21:27 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 4, response is 0\n",
      "23/10/03 12:21:27 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to asusbc-rtl8117.lan:44641\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned broadcast 4\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(247)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 247\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 247\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(217)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 217\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 217\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(246)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 246\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 246\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(220)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 220\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 220\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(239)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 239\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 239\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(228)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 228\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 228\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(236)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 236\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 236\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(234)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 234\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 234\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(232)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 232\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 232\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Got cleaning task CleanAccum(240)\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaning accumulator 240\n",
      "23/10/03 12:21:27 DEBUG ContextCleaner: Cleaned accumulator 240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/03 12:21:31 DEBUG ExecutorMetricsPoller: removing (4, 0) from stageTCMP\n",
      "23/10/03 12:21:31 DEBUG ExecutorMetricsPoller: removing (3, 0) from stageTCMP\n",
      "23/10/03 12:21:35 INFO JDBCRDD: closed connection\n",
      "23/10/03 12:21:35 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 3608 bytes result sent to driver\n",
      "23/10/03 12:21:35 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 0\n",
      "23/10/03 12:21:35 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 11221 ms on asusbc-rtl8117.lan (executor driver) (1/1)\n",
      "23/10/03 12:21:35 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "23/10/03 12:21:35 INFO DAGScheduler: ResultStage 5 (showString at NativeMethodAccessorImpl.java:0) finished in 11.228 s\n",
      "23/10/03 12:21:35 DEBUG DAGScheduler: After removal of stage 5, remaining stages = 0\n",
      "23/10/03 12:21:35 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/10/03 12:21:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
      "23/10/03 12:21:35 INFO DAGScheduler: Job 5 finished: showString at NativeMethodAccessorImpl.java:0, took 11.230838 s\n",
      "23/10/03 12:21:35 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, false].toString, input[1, string, false].toString, input[2, string, false].toString, input[3, string, false].toString, input[4, string, false].toString, input[5, string, false].toString, input[6, string, false].toString, input[7, string, false].toString, input[8, string, false].toString, StructField(toprettystring(o_orderkey),StringType,false), StructField(toprettystring(o_custkey),StringType,false), StructField(toprettystring(o_orderstatus),StringType,false), StructField(toprettystring(o_totalprice),StringType,false), StructField(toprettystring(o_orderdate),StringType,false), StructField(toprettystring(o_orderpriority),StringType,false), StructField(toprettystring(o_clerk),StringType,false), StructField(toprettystring(o_shippriority),StringType,false), StructField(toprettystring(o_comment),StringType,false)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[9];\n",
      "/* 024 */     createExternalRow_0_0(i, values_0);\n",
      "/* 025 */     createExternalRow_0_1(i, values_0);\n",
      "/* 026 */     createExternalRow_0_2(i, values_0);\n",
      "/* 027 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 028 */     if (false) {\n",
      "/* 029 */       mutableRow.setNullAt(0);\n",
      "/* 030 */     } else {\n",
      "/* 031 */\n",
      "/* 032 */       mutableRow.update(0, value_0);\n",
      "/* 033 */     }\n",
      "/* 034 */\n",
      "/* 035 */     return mutableRow;\n",
      "/* 036 */   }\n",
      "/* 037 */\n",
      "/* 038 */\n",
      "/* 039 */   private void createExternalRow_0_2(InternalRow i, Object[] values_0) {\n",
      "/* 040 */\n",
      "/* 041 */     UTF8String value_14 = i.getUTF8String(6);\n",
      "/* 042 */     boolean isNull_13 = true;\n",
      "/* 043 */     java.lang.String value_13 = null;\n",
      "/* 044 */     isNull_13 = false;\n",
      "/* 045 */     if (!isNull_13) {\n",
      "/* 046 */\n",
      "/* 047 */       Object funcResult_6 = null;\n",
      "/* 048 */       funcResult_6 = value_14.toString();\n",
      "/* 049 */       value_13 = (java.lang.String) funcResult_6;\n",
      "/* 050 */\n",
      "/* 051 */     }\n",
      "/* 052 */     if (isNull_13) {\n",
      "/* 053 */       values_0[6] = null;\n",
      "/* 054 */     } else {\n",
      "/* 055 */       values_0[6] = value_13;\n",
      "/* 056 */     }\n",
      "/* 057 */\n",
      "/* 058 */     UTF8String value_16 = i.getUTF8String(7);\n",
      "/* 059 */     boolean isNull_15 = true;\n",
      "/* 060 */     java.lang.String value_15 = null;\n",
      "/* 061 */     isNull_15 = false;\n",
      "/* 062 */     if (!isNull_15) {\n",
      "/* 063 */\n",
      "/* 064 */       Object funcResult_7 = null;\n",
      "/* 065 */       funcResult_7 = value_16.toString();\n",
      "/* 066 */       value_15 = (java.lang.String) funcResult_7;\n",
      "/* 067 */\n",
      "/* 068 */     }\n",
      "/* 069 */     if (isNull_15) {\n",
      "/* 070 */       values_0[7] = null;\n",
      "/* 071 */     } else {\n",
      "/* 072 */       values_0[7] = value_15;\n",
      "/* 073 */     }\n",
      "/* 074 */\n",
      "/* 075 */     UTF8String value_18 = i.getUTF8String(8);\n",
      "/* 076 */     boolean isNull_17 = true;\n",
      "/* 077 */     java.lang.String value_17 = null;\n",
      "/* 078 */     isNull_17 = false;\n",
      "/* 079 */     if (!isNull_17) {\n",
      "/* 080 */\n",
      "/* 081 */       Object funcResult_8 = null;\n",
      "/* 082 */       funcResult_8 = value_18.toString();\n",
      "/* 083 */       value_17 = (java.lang.String) funcResult_8;\n",
      "/* 084 */\n",
      "/* 085 */     }\n",
      "/* 086 */     if (isNull_17) {\n",
      "/* 087 */       values_0[8] = null;\n",
      "/* 088 */     } else {\n",
      "/* 089 */       values_0[8] = value_17;\n",
      "/* 090 */     }\n",
      "/* 091 */\n",
      "/* 092 */   }\n",
      "/* 093 */\n",
      "/* 094 */\n",
      "/* 095 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {\n",
      "/* 096 */\n",
      "/* 097 */     UTF8String value_8 = i.getUTF8String(3);\n",
      "/* 098 */     boolean isNull_7 = true;\n",
      "/* 099 */     java.lang.String value_7 = null;\n",
      "/* 100 */     isNull_7 = false;\n",
      "/* 101 */     if (!isNull_7) {\n",
      "/* 102 */\n",
      "/* 103 */       Object funcResult_3 = null;\n",
      "/* 104 */       funcResult_3 = value_8.toString();\n",
      "/* 105 */       value_7 = (java.lang.String) funcResult_3;\n",
      "/* 106 */\n",
      "/* 107 */     }\n",
      "/* 108 */     if (isNull_7) {\n",
      "/* 109 */       values_0[3] = null;\n",
      "/* 110 */     } else {\n",
      "/* 111 */       values_0[3] = value_7;\n",
      "/* 112 */     }\n",
      "/* 113 */\n",
      "/* 114 */     UTF8String value_10 = i.getUTF8String(4);\n",
      "/* 115 */     boolean isNull_9 = true;\n",
      "/* 116 */     java.lang.String value_9 = null;\n",
      "/* 117 */     isNull_9 = false;\n",
      "/* 118 */     if (!isNull_9) {\n",
      "/* 119 */\n",
      "/* 120 */       Object funcResult_4 = null;\n",
      "/* 121 */       funcResult_4 = value_10.toString();\n",
      "/* 122 */       value_9 = (java.lang.String) funcResult_4;\n",
      "/* 123 */\n",
      "/* 124 */     }\n",
      "/* 125 */     if (isNull_9) {\n",
      "/* 126 */       values_0[4] = null;\n",
      "/* 127 */     } else {\n",
      "/* 128 */       values_0[4] = value_9;\n",
      "/* 129 */     }\n",
      "/* 130 */\n",
      "/* 131 */     UTF8String value_12 = i.getUTF8String(5);\n",
      "/* 132 */     boolean isNull_11 = true;\n",
      "/* 133 */     java.lang.String value_11 = null;\n",
      "/* 134 */     isNull_11 = false;\n",
      "/* 135 */     if (!isNull_11) {\n",
      "/* 136 */\n",
      "/* 137 */       Object funcResult_5 = null;\n",
      "/* 138 */       funcResult_5 = value_12.toString();\n",
      "/* 139 */       value_11 = (java.lang.String) funcResult_5;\n",
      "/* 140 */\n",
      "/* 141 */     }\n",
      "/* 142 */     if (isNull_11) {\n",
      "/* 143 */       values_0[5] = null;\n",
      "/* 144 */     } else {\n",
      "/* 145 */       values_0[5] = value_11;\n",
      "/* 146 */     }\n",
      "/* 147 */\n",
      "/* 148 */   }\n",
      "/* 149 */\n",
      "/* 150 */\n",
      "/* 151 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {\n",
      "/* 152 */\n",
      "/* 153 */     UTF8String value_2 = i.getUTF8String(0);\n",
      "/* 154 */     boolean isNull_1 = true;\n",
      "/* 155 */     java.lang.String value_1 = null;\n",
      "/* 156 */     isNull_1 = false;\n",
      "/* 157 */     if (!isNull_1) {\n",
      "/* 158 */\n",
      "/* 159 */       Object funcResult_0 = null;\n",
      "/* 160 */       funcResult_0 = value_2.toString();\n",
      "/* 161 */       value_1 = (java.lang.String) funcResult_0;\n",
      "/* 162 */\n",
      "/* 163 */     }\n",
      "/* 164 */     if (isNull_1) {\n",
      "/* 165 */       values_0[0] = null;\n",
      "/* 166 */     } else {\n",
      "/* 167 */       values_0[0] = value_1;\n",
      "/* 168 */     }\n",
      "/* 169 */\n",
      "/* 170 */     UTF8String value_4 = i.getUTF8String(1);\n",
      "/* 171 */     boolean isNull_3 = true;\n",
      "/* 172 */     java.lang.String value_3 = null;\n",
      "/* 173 */     isNull_3 = false;\n",
      "/* 174 */     if (!isNull_3) {\n",
      "/* 175 */\n",
      "/* 176 */       Object funcResult_1 = null;\n",
      "/* 177 */       funcResult_1 = value_4.toString();\n",
      "/* 178 */       value_3 = (java.lang.String) funcResult_1;\n",
      "/* 179 */\n",
      "/* 180 */     }\n",
      "/* 181 */     if (isNull_3) {\n",
      "/* 182 */       values_0[1] = null;\n",
      "/* 183 */     } else {\n",
      "/* 184 */       values_0[1] = value_3;\n",
      "/* 185 */     }\n",
      "/* 186 */\n",
      "/* 187 */     UTF8String value_6 = i.getUTF8String(2);\n",
      "/* 188 */     boolean isNull_5 = true;\n",
      "/* 189 */     java.lang.String value_5 = null;\n",
      "/* 190 */     isNull_5 = false;\n",
      "/* 191 */     if (!isNull_5) {\n",
      "/* 192 */\n",
      "/* 193 */       Object funcResult_2 = null;\n",
      "/* 194 */       funcResult_2 = value_6.toString();\n",
      "/* 195 */       value_5 = (java.lang.String) funcResult_2;\n",
      "/* 196 */\n",
      "/* 197 */     }\n",
      "/* 198 */     if (isNull_5) {\n",
      "/* 199 */       values_0[2] = null;\n",
      "/* 200 */     } else {\n",
      "/* 201 */       values_0[2] = value_5;\n",
      "/* 202 */     }\n",
      "/* 203 */\n",
      "/* 204 */   }\n",
      "/* 205 */\n",
      "/* 206 */ }\n",
      "\n",
      "23/10/03 12:21:35 DEBUG SparkSqlParser: Parsing command: orders\n",
      "23/10/03 12:21:35 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:35 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:35 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:35 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:35 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:35 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:35 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:35 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:35 DEBUG BasicConnectionProvider: JDBC connection initiated with URL: jdbc:postgresql://localhost:5432/tpch and properties: {password=tpch, user=tpch}\n",
      "23/10/03 12:21:35 DEBUG CatalystSqlParser: Parsing command: char(1)\n",
      "23/10/03 12:21:35 DEBUG CatalystSqlParser: Parsing command: char(1)\n",
      "23/10/03 12:21:35 DEBUG CatalystSqlParser: Parsing command: char(25)\n",
      "23/10/03 12:21:35 DEBUG CatalystSqlParser: Parsing command: char(10)\n",
      "23/10/03 12:21:35 DEBUG CatalystSqlParser: Parsing command: varchar(44)\n",
      "23/10/03 12:21:35 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator scan_input_0;\n",
      "/* 010 */   private boolean project_resultIsNull_0;\n",
      "/* 011 */   private int project_argValue_0;\n",
      "/* 012 */   private boolean project_resultIsNull_1;\n",
      "/* 013 */   private int project_argValue_1;\n",
      "/* 014 */   private boolean project_resultIsNull_2;\n",
      "/* 015 */   private int project_argValue_2;\n",
      "/* 016 */   private boolean project_resultIsNull_3;\n",
      "/* 017 */   private int project_argValue_3;\n",
      "/* 018 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] scan_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 019 */   private UTF8String[] project_mutableStateArray_0 = new UTF8String[4];\n",
      "/* 020 */\n",
      "/* 021 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 022 */     this.references = references;\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 026 */     partitionIndex = index;\n",
      "/* 027 */     this.inputs = inputs;\n",
      "/* 028 */     scan_input_0 = inputs[0];\n",
      "/* 029 */     scan_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(16, 288);\n",
      "/* 030 */\n",
      "/* 031 */     scan_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(16, 512);\n",
      "/* 032 */\n",
      "/* 033 */   }\n",
      "/* 034 */\n",
      "/* 035 */   protected void processNext() throws java.io.IOException {\n",
      "/* 036 */     while ( scan_input_0.hasNext()) {\n",
      "/* 037 */       InternalRow scan_row_0 = (InternalRow) scan_input_0.next();\n",
      "/* 038 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 039 */       // common sub-expressions\n",
      "/* 040 */\n",
      "/* 041 */       boolean scan_isNull_0 = scan_row_0.isNullAt(0);\n",
      "/* 042 */       long scan_value_0 = scan_isNull_0 ?\n",
      "/* 043 */       -1L : (scan_row_0.getLong(0));\n",
      "/* 044 */       UTF8String project_value_0;\n",
      "/* 045 */       if (scan_isNull_0) {\n",
      "/* 046 */         project_value_0 = UTF8String.fromString(\"NULL\");\n",
      "/* 047 */       } else {\n",
      "/* 048 */         project_value_0 = UTF8String.fromString(String.valueOf(scan_value_0));\n",
      "/* 049 */       }\n",
      "/* 050 */       boolean scan_isNull_1 = scan_row_0.isNullAt(1);\n",
      "/* 051 */       long scan_value_1 = scan_isNull_1 ?\n",
      "/* 052 */       -1L : (scan_row_0.getLong(1));\n",
      "/* 053 */       UTF8String project_value_2;\n",
      "/* 054 */       if (scan_isNull_1) {\n",
      "/* 055 */         project_value_2 = UTF8String.fromString(\"NULL\");\n",
      "/* 056 */       } else {\n",
      "/* 057 */         project_value_2 = UTF8String.fromString(String.valueOf(scan_value_1));\n",
      "/* 058 */       }\n",
      "/* 059 */       boolean scan_isNull_2 = scan_row_0.isNullAt(2);\n",
      "/* 060 */       long scan_value_2 = scan_isNull_2 ?\n",
      "/* 061 */       -1L : (scan_row_0.getLong(2));\n",
      "/* 062 */       UTF8String project_value_4;\n",
      "/* 063 */       if (scan_isNull_2) {\n",
      "/* 064 */         project_value_4 = UTF8String.fromString(\"NULL\");\n",
      "/* 065 */       } else {\n",
      "/* 066 */         project_value_4 = UTF8String.fromString(String.valueOf(scan_value_2));\n",
      "/* 067 */       }\n",
      "/* 068 */       boolean scan_isNull_3 = scan_row_0.isNullAt(3);\n",
      "/* 069 */       int scan_value_3 = scan_isNull_3 ?\n",
      "/* 070 */       -1 : (scan_row_0.getInt(3));\n",
      "/* 071 */       UTF8String project_value_6;\n",
      "/* 072 */       if (scan_isNull_3) {\n",
      "/* 073 */         project_value_6 = UTF8String.fromString(\"NULL\");\n",
      "/* 074 */       } else {\n",
      "/* 075 */         project_value_6 = UTF8String.fromString(String.valueOf(scan_value_3));\n",
      "/* 076 */       }\n",
      "/* 077 */       boolean scan_isNull_4 = scan_row_0.isNullAt(4);\n",
      "/* 078 */       Decimal scan_value_4 = scan_isNull_4 ?\n",
      "/* 079 */       null : (scan_row_0.getDecimal(4, 38, 18));\n",
      "/* 080 */       UTF8String project_value_8;\n",
      "/* 081 */       if (scan_isNull_4) {\n",
      "/* 082 */         project_value_8 = UTF8String.fromString(\"NULL\");\n",
      "/* 083 */       } else {\n",
      "/* 084 */         project_value_8 = UTF8String.fromString(scan_value_4.toPlainString());\n",
      "/* 085 */       }\n",
      "/* 086 */       boolean scan_isNull_5 = scan_row_0.isNullAt(5);\n",
      "/* 087 */       Decimal scan_value_5 = scan_isNull_5 ?\n",
      "/* 088 */       null : (scan_row_0.getDecimal(5, 38, 18));\n",
      "/* 089 */       UTF8String project_value_10;\n",
      "/* 090 */       if (scan_isNull_5) {\n",
      "/* 091 */         project_value_10 = UTF8String.fromString(\"NULL\");\n",
      "/* 092 */       } else {\n",
      "/* 093 */         project_value_10 = UTF8String.fromString(scan_value_5.toPlainString());\n",
      "/* 094 */       }\n",
      "/* 095 */       boolean scan_isNull_6 = scan_row_0.isNullAt(6);\n",
      "/* 096 */       Decimal scan_value_6 = scan_isNull_6 ?\n",
      "/* 097 */       null : (scan_row_0.getDecimal(6, 38, 18));\n",
      "/* 098 */       UTF8String project_value_12;\n",
      "/* 099 */       if (scan_isNull_6) {\n",
      "/* 100 */         project_value_12 = UTF8String.fromString(\"NULL\");\n",
      "/* 101 */       } else {\n",
      "/* 102 */         project_value_12 = UTF8String.fromString(scan_value_6.toPlainString());\n",
      "/* 103 */       }\n",
      "/* 104 */       boolean scan_isNull_7 = scan_row_0.isNullAt(7);\n",
      "/* 105 */       Decimal scan_value_7 = scan_isNull_7 ?\n",
      "/* 106 */       null : (scan_row_0.getDecimal(7, 38, 18));\n",
      "/* 107 */       UTF8String project_value_14;\n",
      "/* 108 */       if (scan_isNull_7) {\n",
      "/* 109 */         project_value_14 = UTF8String.fromString(\"NULL\");\n",
      "/* 110 */       } else {\n",
      "/* 111 */         project_value_14 = UTF8String.fromString(scan_value_7.toPlainString());\n",
      "/* 112 */       }\n",
      "/* 113 */       project_resultIsNull_0 = false;\n",
      "/* 114 */\n",
      "/* 115 */       if (!project_resultIsNull_0) {\n",
      "/* 116 */         boolean scan_isNull_8 = scan_row_0.isNullAt(8);\n",
      "/* 117 */         UTF8String scan_value_8 = scan_isNull_8 ?\n",
      "/* 118 */         null : (scan_row_0.getUTF8String(8));\n",
      "/* 119 */         project_resultIsNull_0 = scan_isNull_8;\n",
      "/* 120 */         project_mutableStateArray_0[0] = scan_value_8;\n",
      "/* 121 */       }\n",
      "/* 122 */\n",
      "/* 123 */       if (!project_resultIsNull_0) {\n",
      "/* 124 */         project_argValue_0 = 1;\n",
      "/* 125 */       }\n",
      "/* 126 */\n",
      "/* 127 */       boolean project_isNull_17 = project_resultIsNull_0;\n",
      "/* 128 */       UTF8String project_value_17 = null;\n",
      "/* 129 */       if (!project_resultIsNull_0) {\n",
      "/* 130 */         project_value_17 = org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils.readSidePadding(project_mutableStateArray_0[0], project_argValue_0);\n",
      "/* 131 */       }\n",
      "/* 132 */       UTF8String project_value_16;\n",
      "/* 133 */       if (project_isNull_17) {\n",
      "/* 134 */         project_value_16 = UTF8String.fromString(\"NULL\");\n",
      "/* 135 */       } else {\n",
      "/* 136 */         project_value_16 = project_value_17;\n",
      "/* 137 */       }\n",
      "/* 138 */       project_resultIsNull_1 = false;\n",
      "/* 139 */\n",
      "/* 140 */       if (!project_resultIsNull_1) {\n",
      "/* 141 */         boolean scan_isNull_9 = scan_row_0.isNullAt(9);\n",
      "/* 142 */         UTF8String scan_value_9 = scan_isNull_9 ?\n",
      "/* 143 */         null : (scan_row_0.getUTF8String(9));\n",
      "/* 144 */         project_resultIsNull_1 = scan_isNull_9;\n",
      "/* 145 */         project_mutableStateArray_0[1] = scan_value_9;\n",
      "/* 146 */       }\n",
      "/* 147 */\n",
      "/* 148 */       if (!project_resultIsNull_1) {\n",
      "/* 149 */         project_argValue_1 = 1;\n",
      "/* 150 */       }\n",
      "/* 151 */\n",
      "/* 152 */       boolean project_isNull_21 = project_resultIsNull_1;\n",
      "/* 153 */       UTF8String project_value_21 = null;\n",
      "/* 154 */       if (!project_resultIsNull_1) {\n",
      "/* 155 */         project_value_21 = org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils.readSidePadding(project_mutableStateArray_0[1], project_argValue_1);\n",
      "/* 156 */       }\n",
      "/* 157 */       UTF8String project_value_20;\n",
      "/* 158 */       if (project_isNull_21) {\n",
      "/* 159 */         project_value_20 = UTF8String.fromString(\"NULL\");\n",
      "/* 160 */       } else {\n",
      "/* 161 */         project_value_20 = project_value_21;\n",
      "/* 162 */       }\n",
      "/* 163 */       boolean scan_isNull_10 = scan_row_0.isNullAt(10);\n",
      "/* 164 */       int scan_value_10 = scan_isNull_10 ?\n",
      "/* 165 */       -1 : (scan_row_0.getInt(10));\n",
      "/* 166 */       UTF8String project_value_24;\n",
      "/* 167 */       if (scan_isNull_10) {\n",
      "/* 168 */         project_value_24 = UTF8String.fromString(\"NULL\");\n",
      "/* 169 */       } else {\n",
      "/* 170 */         project_value_24 = UTF8String.fromString(((org.apache.spark.sql.catalyst.util.DefaultDateFormatter) references[1] /* dateFormatter */).format(scan_value_10));\n",
      "/* 171 */       }\n",
      "/* 172 */       boolean scan_isNull_11 = scan_row_0.isNullAt(11);\n",
      "/* 173 */       int scan_value_11 = scan_isNull_11 ?\n",
      "/* 174 */       -1 : (scan_row_0.getInt(11));\n",
      "/* 175 */       UTF8String project_value_26;\n",
      "/* 176 */       if (scan_isNull_11) {\n",
      "/* 177 */         project_value_26 = UTF8String.fromString(\"NULL\");\n",
      "/* 178 */       } else {\n",
      "/* 179 */         project_value_26 = UTF8String.fromString(((org.apache.spark.sql.catalyst.util.DefaultDateFormatter) references[2] /* dateFormatter */).format(scan_value_11));\n",
      "/* 180 */       }\n",
      "/* 181 */       boolean scan_isNull_12 = scan_row_0.isNullAt(12);\n",
      "/* 182 */       int scan_value_12 = scan_isNull_12 ?\n",
      "/* 183 */       -1 : (scan_row_0.getInt(12));\n",
      "/* 184 */       UTF8String project_value_28;\n",
      "/* 185 */       if (scan_isNull_12) {\n",
      "/* 186 */         project_value_28 = UTF8String.fromString(\"NULL\");\n",
      "/* 187 */       } else {\n",
      "/* 188 */         project_value_28 = UTF8String.fromString(((org.apache.spark.sql.catalyst.util.DefaultDateFormatter) references[3] /* dateFormatter */).format(scan_value_12));\n",
      "/* 189 */       }\n",
      "/* 190 */       project_resultIsNull_2 = false;\n",
      "/* 191 */\n",
      "/* 192 */       if (!project_resultIsNull_2) {\n",
      "/* 193 */         boolean scan_isNull_13 = scan_row_0.isNullAt(13);\n",
      "/* 194 */         UTF8String scan_value_13 = scan_isNull_13 ?\n",
      "/* 195 */         null : (scan_row_0.getUTF8String(13));\n",
      "/* 196 */         project_resultIsNull_2 = scan_isNull_13;\n",
      "/* 197 */         project_mutableStateArray_0[2] = scan_value_13;\n",
      "/* 198 */       }\n",
      "/* 199 */\n",
      "/* 200 */       if (!project_resultIsNull_2) {\n",
      "/* 201 */         project_argValue_2 = 25;\n",
      "/* 202 */       }\n",
      "/* 203 */\n",
      "/* 204 */       boolean project_isNull_31 = project_resultIsNull_2;\n",
      "/* 205 */       UTF8String project_value_31 = null;\n",
      "/* 206 */       if (!project_resultIsNull_2) {\n",
      "/* 207 */         project_value_31 = org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils.readSidePadding(project_mutableStateArray_0[2], project_argValue_2);\n",
      "/* 208 */       }\n",
      "/* 209 */       UTF8String project_value_30;\n",
      "/* 210 */       if (project_isNull_31) {\n",
      "/* 211 */         project_value_30 = UTF8String.fromString(\"NULL\");\n",
      "/* 212 */       } else {\n",
      "/* 213 */         project_value_30 = project_value_31;\n",
      "/* 214 */       }\n",
      "/* 215 */       project_resultIsNull_3 = false;\n",
      "/* 216 */\n",
      "/* 217 */       if (!project_resultIsNull_3) {\n",
      "/* 218 */         boolean scan_isNull_14 = scan_row_0.isNullAt(14);\n",
      "/* 219 */         UTF8String scan_value_14 = scan_isNull_14 ?\n",
      "/* 220 */         null : (scan_row_0.getUTF8String(14));\n",
      "/* 221 */         project_resultIsNull_3 = scan_isNull_14;\n",
      "/* 222 */         project_mutableStateArray_0[3] = scan_value_14;\n",
      "/* 223 */       }\n",
      "/* 224 */\n",
      "/* 225 */       if (!project_resultIsNull_3) {\n",
      "/* 226 */         project_argValue_3 = 10;\n",
      "/* 227 */       }\n",
      "/* 228 */\n",
      "/* 229 */       boolean project_isNull_35 = project_resultIsNull_3;\n",
      "/* 230 */       UTF8String project_value_35 = null;\n",
      "/* 231 */       if (!project_resultIsNull_3) {\n",
      "/* 232 */         project_value_35 = org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils.readSidePadding(project_mutableStateArray_0[3], project_argValue_3);\n",
      "/* 233 */       }\n",
      "/* 234 */       UTF8String project_value_34;\n",
      "/* 235 */       if (project_isNull_35) {\n",
      "/* 236 */         project_value_34 = UTF8String.fromString(\"NULL\");\n",
      "/* 237 */       } else {\n",
      "/* 238 */         project_value_34 = project_value_35;\n",
      "/* 239 */       }\n",
      "/* 240 */       boolean scan_isNull_15 = scan_row_0.isNullAt(15);\n",
      "/* 241 */       UTF8String scan_value_15 = scan_isNull_15 ?\n",
      "/* 242 */       null : (scan_row_0.getUTF8String(15));\n",
      "/* 243 */       UTF8String project_value_38;\n",
      "/* 244 */       if (scan_isNull_15) {\n",
      "/* 245 */         project_value_38 = UTF8String.fromString(\"NULL\");\n",
      "/* 246 */       } else {\n",
      "/* 247 */         project_value_38 = scan_value_15;\n",
      "/* 248 */       }\n",
      "/* 249 */       scan_mutableStateArray_0[1].reset();\n",
      "/* 250 */\n",
      "/* 251 */       scan_mutableStateArray_0[1].write(0, project_value_0);\n",
      "/* 252 */\n",
      "/* 253 */       scan_mutableStateArray_0[1].write(1, project_value_2);\n",
      "/* 254 */\n",
      "/* 255 */       scan_mutableStateArray_0[1].write(2, project_value_4);\n",
      "/* 256 */\n",
      "/* 257 */       scan_mutableStateArray_0[1].write(3, project_value_6);\n",
      "/* 258 */\n",
      "/* 259 */       scan_mutableStateArray_0[1].write(4, project_value_8);\n",
      "/* 260 */\n",
      "/* 261 */       scan_mutableStateArray_0[1].write(5, project_value_10);\n",
      "/* 262 */\n",
      "/* 263 */       scan_mutableStateArray_0[1].write(6, project_value_12);\n",
      "/* 264 */\n",
      "/* 265 */       scan_mutableStateArray_0[1].write(7, project_value_14);\n",
      "/* 266 */\n",
      "/* 267 */       scan_mutableStateArray_0[1].write(8, project_value_16);\n",
      "/* 268 */\n",
      "/* 269 */       scan_mutableStateArray_0[1].write(9, project_value_20);\n",
      "/* 270 */\n",
      "/* 271 */       scan_mutableStateArray_0[1].write(10, project_value_24);\n",
      "/* 272 */\n",
      "/* 273 */       scan_mutableStateArray_0[1].write(11, project_value_26);\n",
      "/* 274 */\n",
      "/* 275 */       scan_mutableStateArray_0[1].write(12, project_value_28);\n",
      "/* 276 */\n",
      "/* 277 */       scan_mutableStateArray_0[1].write(13, project_value_30);\n",
      "/* 278 */\n",
      "/* 279 */       scan_mutableStateArray_0[1].write(14, project_value_34);\n",
      "/* 280 */\n",
      "/* 281 */       scan_mutableStateArray_0[1].write(15, project_value_38);\n",
      "/* 282 */       append((scan_mutableStateArray_0[1].getRow()));\n",
      "/* 283 */       if (shouldStop()) return;\n",
      "/* 284 */     }\n",
      "/* 285 */   }\n",
      "/* 286 */\n",
      "/* 287 */ }\n",
      "\n",
      "23/10/03 12:21:35 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator scan_input_0;\n",
      "/* 010 */   private boolean project_resultIsNull_0;\n",
      "/* 011 */   private int project_argValue_0;\n",
      "/* 012 */   private boolean project_resultIsNull_1;\n",
      "/* 013 */   private int project_argValue_1;\n",
      "/* 014 */   private boolean project_resultIsNull_2;\n",
      "/* 015 */   private int project_argValue_2;\n",
      "/* 016 */   private boolean project_resultIsNull_3;\n",
      "/* 017 */   private int project_argValue_3;\n",
      "/* 018 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] scan_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 019 */   private UTF8String[] project_mutableStateArray_0 = new UTF8String[4];\n",
      "/* 020 */\n",
      "/* 021 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 022 */     this.references = references;\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 026 */     partitionIndex = index;\n",
      "/* 027 */     this.inputs = inputs;\n",
      "/* 028 */     scan_input_0 = inputs[0];\n",
      "/* 029 */     scan_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(16, 288);\n",
      "/* 030 */\n",
      "/* 031 */     scan_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(16, 512);\n",
      "/* 032 */\n",
      "/* 033 */   }\n",
      "/* 034 */\n",
      "/* 035 */   protected void processNext() throws java.io.IOException {\n",
      "/* 036 */     while ( scan_input_0.hasNext()) {\n",
      "/* 037 */       InternalRow scan_row_0 = (InternalRow) scan_input_0.next();\n",
      "/* 038 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 039 */       // common sub-expressions\n",
      "/* 040 */\n",
      "/* 041 */       boolean scan_isNull_0 = scan_row_0.isNullAt(0);\n",
      "/* 042 */       long scan_value_0 = scan_isNull_0 ?\n",
      "/* 043 */       -1L : (scan_row_0.getLong(0));\n",
      "/* 044 */       UTF8String project_value_0;\n",
      "/* 045 */       if (scan_isNull_0) {\n",
      "/* 046 */         project_value_0 = UTF8String.fromString(\"NULL\");\n",
      "/* 047 */       } else {\n",
      "/* 048 */         project_value_0 = UTF8String.fromString(String.valueOf(scan_value_0));\n",
      "/* 049 */       }\n",
      "/* 050 */       boolean scan_isNull_1 = scan_row_0.isNullAt(1);\n",
      "/* 051 */       long scan_value_1 = scan_isNull_1 ?\n",
      "/* 052 */       -1L : (scan_row_0.getLong(1));\n",
      "/* 053 */       UTF8String project_value_2;\n",
      "/* 054 */       if (scan_isNull_1) {\n",
      "/* 055 */         project_value_2 = UTF8String.fromString(\"NULL\");\n",
      "/* 056 */       } else {\n",
      "/* 057 */         project_value_2 = UTF8String.fromString(String.valueOf(scan_value_1));\n",
      "/* 058 */       }\n",
      "/* 059 */       boolean scan_isNull_2 = scan_row_0.isNullAt(2);\n",
      "/* 060 */       long scan_value_2 = scan_isNull_2 ?\n",
      "/* 061 */       -1L : (scan_row_0.getLong(2));\n",
      "/* 062 */       UTF8String project_value_4;\n",
      "/* 063 */       if (scan_isNull_2) {\n",
      "/* 064 */         project_value_4 = UTF8String.fromString(\"NULL\");\n",
      "/* 065 */       } else {\n",
      "/* 066 */         project_value_4 = UTF8String.fromString(String.valueOf(scan_value_2));\n",
      "/* 067 */       }\n",
      "/* 068 */       boolean scan_isNull_3 = scan_row_0.isNullAt(3);\n",
      "/* 069 */       int scan_value_3 = scan_isNull_3 ?\n",
      "/* 070 */       -1 : (scan_row_0.getInt(3));\n",
      "/* 071 */       UTF8String project_value_6;\n",
      "/* 072 */       if (scan_isNull_3) {\n",
      "/* 073 */         project_value_6 = UTF8String.fromString(\"NULL\");\n",
      "/* 074 */       } else {\n",
      "/* 075 */         project_value_6 = UTF8String.fromString(String.valueOf(scan_value_3));\n",
      "/* 076 */       }\n",
      "/* 077 */       boolean scan_isNull_4 = scan_row_0.isNullAt(4);\n",
      "/* 078 */       Decimal scan_value_4 = scan_isNull_4 ?\n",
      "/* 079 */       null : (scan_row_0.getDecimal(4, 38, 18));\n",
      "/* 080 */       UTF8String project_value_8;\n",
      "/* 081 */       if (scan_isNull_4) {\n",
      "/* 082 */         project_value_8 = UTF8String.fromString(\"NULL\");\n",
      "/* 083 */       } else {\n",
      "/* 084 */         project_value_8 = UTF8String.fromString(scan_value_4.toPlainString());\n",
      "/* 085 */       }\n",
      "/* 086 */       boolean scan_isNull_5 = scan_row_0.isNullAt(5);\n",
      "/* 087 */       Decimal scan_value_5 = scan_isNull_5 ?\n",
      "/* 088 */       null : (scan_row_0.getDecimal(5, 38, 18));\n",
      "/* 089 */       UTF8String project_value_10;\n",
      "/* 090 */       if (scan_isNull_5) {\n",
      "/* 091 */         project_value_10 = UTF8String.fromString(\"NULL\");\n",
      "/* 092 */       } else {\n",
      "/* 093 */         project_value_10 = UTF8String.fromString(scan_value_5.toPlainString());\n",
      "/* 094 */       }\n",
      "/* 095 */       boolean scan_isNull_6 = scan_row_0.isNullAt(6);\n",
      "/* 096 */       Decimal scan_value_6 = scan_isNull_6 ?\n",
      "/* 097 */       null : (scan_row_0.getDecimal(6, 38, 18));\n",
      "/* 098 */       UTF8String project_value_12;\n",
      "/* 099 */       if (scan_isNull_6) {\n",
      "/* 100 */         project_value_12 = UTF8String.fromString(\"NULL\");\n",
      "/* 101 */       } else {\n",
      "/* 102 */         project_value_12 = UTF8String.fromString(scan_value_6.toPlainString());\n",
      "/* 103 */       }\n",
      "/* 104 */       boolean scan_isNull_7 = scan_row_0.isNullAt(7);\n",
      "/* 105 */       Decimal scan_value_7 = scan_isNull_7 ?\n",
      "/* 106 */       null : (scan_row_0.getDecimal(7, 38, 18));\n",
      "/* 107 */       UTF8String project_value_14;\n",
      "/* 108 */       if (scan_isNull_7) {\n",
      "/* 109 */         project_value_14 = UTF8String.fromString(\"NULL\");\n",
      "/* 110 */       } else {\n",
      "/* 111 */         project_value_14 = UTF8String.fromString(scan_value_7.toPlainString());\n",
      "/* 112 */       }\n",
      "/* 113 */       project_resultIsNull_0 = false;\n",
      "/* 114 */\n",
      "/* 115 */       if (!project_resultIsNull_0) {\n",
      "/* 116 */         boolean scan_isNull_8 = scan_row_0.isNullAt(8);\n",
      "/* 117 */         UTF8String scan_value_8 = scan_isNull_8 ?\n",
      "/* 118 */         null : (scan_row_0.getUTF8String(8));\n",
      "/* 119 */         project_resultIsNull_0 = scan_isNull_8;\n",
      "/* 120 */         project_mutableStateArray_0[0] = scan_value_8;\n",
      "/* 121 */       }\n",
      "/* 122 */\n",
      "/* 123 */       if (!project_resultIsNull_0) {\n",
      "/* 124 */         project_argValue_0 = 1;\n",
      "/* 125 */       }\n",
      "/* 126 */\n",
      "/* 127 */       boolean project_isNull_17 = project_resultIsNull_0;\n",
      "/* 128 */       UTF8String project_value_17 = null;\n",
      "/* 129 */       if (!project_resultIsNull_0) {\n",
      "/* 130 */         project_value_17 = org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils.readSidePadding(project_mutableStateArray_0[0], project_argValue_0);\n",
      "/* 131 */       }\n",
      "/* 132 */       UTF8String project_value_16;\n",
      "/* 133 */       if (project_isNull_17) {\n",
      "/* 134 */         project_value_16 = UTF8String.fromString(\"NULL\");\n",
      "/* 135 */       } else {\n",
      "/* 136 */         project_value_16 = project_value_17;\n",
      "/* 137 */       }\n",
      "/* 138 */       project_resultIsNull_1 = false;\n",
      "/* 139 */\n",
      "/* 140 */       if (!project_resultIsNull_1) {\n",
      "/* 141 */         boolean scan_isNull_9 = scan_row_0.isNullAt(9);\n",
      "/* 142 */         UTF8String scan_value_9 = scan_isNull_9 ?\n",
      "/* 143 */         null : (scan_row_0.getUTF8String(9));\n",
      "/* 144 */         project_resultIsNull_1 = scan_isNull_9;\n",
      "/* 145 */         project_mutableStateArray_0[1] = scan_value_9;\n",
      "/* 146 */       }\n",
      "/* 147 */\n",
      "/* 148 */       if (!project_resultIsNull_1) {\n",
      "/* 149 */         project_argValue_1 = 1;\n",
      "/* 150 */       }\n",
      "/* 151 */\n",
      "/* 152 */       boolean project_isNull_21 = project_resultIsNull_1;\n",
      "/* 153 */       UTF8String project_value_21 = null;\n",
      "/* 154 */       if (!project_resultIsNull_1) {\n",
      "/* 155 */         project_value_21 = org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils.readSidePadding(project_mutableStateArray_0[1], project_argValue_1);\n",
      "/* 156 */       }\n",
      "/* 157 */       UTF8String project_value_20;\n",
      "/* 158 */       if (project_isNull_21) {\n",
      "/* 159 */         project_value_20 = UTF8String.fromString(\"NULL\");\n",
      "/* 160 */       } else {\n",
      "/* 161 */         project_value_20 = project_value_21;\n",
      "/* 162 */       }\n",
      "/* 163 */       boolean scan_isNull_10 = scan_row_0.isNullAt(10);\n",
      "/* 164 */       int scan_value_10 = scan_isNull_10 ?\n",
      "/* 165 */       -1 : (scan_row_0.getInt(10));\n",
      "/* 166 */       UTF8String project_value_24;\n",
      "/* 167 */       if (scan_isNull_10) {\n",
      "/* 168 */         project_value_24 = UTF8String.fromString(\"NULL\");\n",
      "/* 169 */       } else {\n",
      "/* 170 */         project_value_24 = UTF8String.fromString(((org.apache.spark.sql.catalyst.util.DefaultDateFormatter) references[1] /* dateFormatter */).format(scan_value_10));\n",
      "/* 171 */       }\n",
      "/* 172 */       boolean scan_isNull_11 = scan_row_0.isNullAt(11);\n",
      "/* 173 */       int scan_value_11 = scan_isNull_11 ?\n",
      "/* 174 */       -1 : (scan_row_0.getInt(11));\n",
      "/* 175 */       UTF8String project_value_26;\n",
      "/* 176 */       if (scan_isNull_11) {\n",
      "/* 177 */         project_value_26 = UTF8String.fromString(\"NULL\");\n",
      "/* 178 */       } else {\n",
      "/* 179 */         project_value_26 = UTF8String.fromString(((org.apache.spark.sql.catalyst.util.DefaultDateFormatter) references[2] /* dateFormatter */).format(scan_value_11));\n",
      "/* 180 */       }\n",
      "/* 181 */       boolean scan_isNull_12 = scan_row_0.isNullAt(12);\n",
      "/* 182 */       int scan_value_12 = scan_isNull_12 ?\n",
      "/* 183 */       -1 : (scan_row_0.getInt(12));\n",
      "/* 184 */       UTF8String project_value_28;\n",
      "/* 185 */       if (scan_isNull_12) {\n",
      "/* 186 */         project_value_28 = UTF8String.fromString(\"NULL\");\n",
      "/* 187 */       } else {\n",
      "/* 188 */         project_value_28 = UTF8String.fromString(((org.apache.spark.sql.catalyst.util.DefaultDateFormatter) references[3] /* dateFormatter */).format(scan_value_12));\n",
      "/* 189 */       }\n",
      "/* 190 */       project_resultIsNull_2 = false;\n",
      "/* 191 */\n",
      "/* 192 */       if (!project_resultIsNull_2) {\n",
      "/* 193 */         boolean scan_isNull_13 = scan_row_0.isNullAt(13);\n",
      "/* 194 */         UTF8String scan_value_13 = scan_isNull_13 ?\n",
      "/* 195 */         null : (scan_row_0.getUTF8String(13));\n",
      "/* 196 */         project_resultIsNull_2 = scan_isNull_13;\n",
      "/* 197 */         project_mutableStateArray_0[2] = scan_value_13;\n",
      "/* 198 */       }\n",
      "/* 199 */\n",
      "/* 200 */       if (!project_resultIsNull_2) {\n",
      "/* 201 */         project_argValue_2 = 25;\n",
      "/* 202 */       }\n",
      "/* 203 */\n",
      "/* 204 */       boolean project_isNull_31 = project_resultIsNull_2;\n",
      "/* 205 */       UTF8String project_value_31 = null;\n",
      "/* 206 */       if (!project_resultIsNull_2) {\n",
      "/* 207 */         project_value_31 = org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils.readSidePadding(project_mutableStateArray_0[2], project_argValue_2);\n",
      "/* 208 */       }\n",
      "/* 209 */       UTF8String project_value_30;\n",
      "/* 210 */       if (project_isNull_31) {\n",
      "/* 211 */         project_value_30 = UTF8String.fromString(\"NULL\");\n",
      "/* 212 */       } else {\n",
      "/* 213 */         project_value_30 = project_value_31;\n",
      "/* 214 */       }\n",
      "/* 215 */       project_resultIsNull_3 = false;\n",
      "/* 216 */\n",
      "/* 217 */       if (!project_resultIsNull_3) {\n",
      "/* 218 */         boolean scan_isNull_14 = scan_row_0.isNullAt(14);\n",
      "/* 219 */         UTF8String scan_value_14 = scan_isNull_14 ?\n",
      "/* 220 */         null : (scan_row_0.getUTF8String(14));\n",
      "/* 221 */         project_resultIsNull_3 = scan_isNull_14;\n",
      "/* 222 */         project_mutableStateArray_0[3] = scan_value_14;\n",
      "/* 223 */       }\n",
      "/* 224 */\n",
      "/* 225 */       if (!project_resultIsNull_3) {\n",
      "/* 226 */         project_argValue_3 = 10;\n",
      "/* 227 */       }\n",
      "/* 228 */\n",
      "/* 229 */       boolean project_isNull_35 = project_resultIsNull_3;\n",
      "/* 230 */       UTF8String project_value_35 = null;\n",
      "/* 231 */       if (!project_resultIsNull_3) {\n",
      "/* 232 */         project_value_35 = org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils.readSidePadding(project_mutableStateArray_0[3], project_argValue_3);\n",
      "/* 233 */       }\n",
      "/* 234 */       UTF8String project_value_34;\n",
      "/* 235 */       if (project_isNull_35) {\n",
      "/* 236 */         project_value_34 = UTF8String.fromString(\"NULL\");\n",
      "/* 237 */       } else {\n",
      "/* 238 */         project_value_34 = project_value_35;\n",
      "/* 239 */       }\n",
      "/* 240 */       boolean scan_isNull_15 = scan_row_0.isNullAt(15);\n",
      "/* 241 */       UTF8String scan_value_15 = scan_isNull_15 ?\n",
      "/* 242 */       null : (scan_row_0.getUTF8String(15));\n",
      "/* 243 */       UTF8String project_value_38;\n",
      "/* 244 */       if (scan_isNull_15) {\n",
      "/* 245 */         project_value_38 = UTF8String.fromString(\"NULL\");\n",
      "/* 246 */       } else {\n",
      "/* 247 */         project_value_38 = scan_value_15;\n",
      "/* 248 */       }\n",
      "/* 249 */       scan_mutableStateArray_0[1].reset();\n",
      "/* 250 */\n",
      "/* 251 */       scan_mutableStateArray_0[1].write(0, project_value_0);\n",
      "/* 252 */\n",
      "/* 253 */       scan_mutableStateArray_0[1].write(1, project_value_2);\n",
      "/* 254 */\n",
      "/* 255 */       scan_mutableStateArray_0[1].write(2, project_value_4);\n",
      "/* 256 */\n",
      "/* 257 */       scan_mutableStateArray_0[1].write(3, project_value_6);\n",
      "/* 258 */\n",
      "/* 259 */       scan_mutableStateArray_0[1].write(4, project_value_8);\n",
      "/* 260 */\n",
      "/* 261 */       scan_mutableStateArray_0[1].write(5, project_value_10);\n",
      "/* 262 */\n",
      "/* 263 */       scan_mutableStateArray_0[1].write(6, project_value_12);\n",
      "/* 264 */\n",
      "/* 265 */       scan_mutableStateArray_0[1].write(7, project_value_14);\n",
      "/* 266 */\n",
      "/* 267 */       scan_mutableStateArray_0[1].write(8, project_value_16);\n",
      "/* 268 */\n",
      "/* 269 */       scan_mutableStateArray_0[1].write(9, project_value_20);\n",
      "/* 270 */\n",
      "/* 271 */       scan_mutableStateArray_0[1].write(10, project_value_24);\n",
      "/* 272 */\n",
      "/* 273 */       scan_mutableStateArray_0[1].write(11, project_value_26);\n",
      "/* 274 */\n",
      "/* 275 */       scan_mutableStateArray_0[1].write(12, project_value_28);\n",
      "/* 276 */\n",
      "/* 277 */       scan_mutableStateArray_0[1].write(13, project_value_30);\n",
      "/* 278 */\n",
      "/* 279 */       scan_mutableStateArray_0[1].write(14, project_value_34);\n",
      "/* 280 */\n",
      "/* 281 */       scan_mutableStateArray_0[1].write(15, project_value_38);\n",
      "/* 282 */       append((scan_mutableStateArray_0[1].getRow()));\n",
      "/* 283 */       if (shouldStop()) return;\n",
      "/* 284 */     }\n",
      "/* 285 */   }\n",
      "/* 286 */\n",
      "/* 287 */ }\n",
      "\n",
      "23/10/03 12:21:35 INFO CodeGenerator: Code generated in 14.854664 ms\n",
      "23/10/03 12:21:35 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "23/10/03 12:21:35 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "23/10/03 12:21:35 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$executeTake$2\n",
      "23/10/03 12:21:35 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++\n",
      "23/10/03 12:21:35 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "23/10/03 12:21:35 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "23/10/03 12:21:35 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/10/03 12:21:35 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 20 took 0.000053 seconds\n",
      "23/10/03 12:21:35 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "23/10/03 12:21:35 INFO DAGScheduler: Got job 6 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/10/03 12:21:35 INFO DAGScheduler: Final stage: ResultStage 6 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/10/03 12:21:35 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/10/03 12:21:35 INFO DAGScheduler: Missing parents: List()\n",
      "23/10/03 12:21:35 DEBUG DAGScheduler: submitStage(ResultStage 6 (name=showString at NativeMethodAccessorImpl.java:0;jobs=6))\n",
      "23/10/03 12:21:35 DEBUG DAGScheduler: missing: List()\n",
      "23/10/03 12:21:35 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[20] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/10/03 12:21:35 DEBUG DAGScheduler: submitMissingTasks(ResultStage 6)\n",
      "23/10/03 12:21:35 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 20.6 KiB, free 26.8 GiB)\n",
      "23/10/03 12:21:35 DEBUG BlockManager: Put block broadcast_6 locally took 0 ms\n",
      "23/10/03 12:21:35 DEBUG BlockManager: Putting block broadcast_6 without replication took 0 ms\n",
      "23/10/03 12:21:35 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 26.8 GiB)\n",
      "23/10/03 12:21:35 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_6_piece0 for BlockManagerId(driver, asusbc-rtl8117.lan, 34663, None)\n",
      "23/10/03 12:21:35 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on asusbc-rtl8117.lan:34663 (size: 8.6 KiB, free: 26.8 GiB)\n",
      "23/10/03 12:21:35 DEBUG BlockManagerMaster: Updated info of block broadcast_6_piece0\n",
      "23/10/03 12:21:35 DEBUG BlockManager: Told master about block broadcast_6_piece0\n",
      "23/10/03 12:21:35 DEBUG BlockManager: Put block broadcast_6_piece0 locally took 0 ms\n",
      "23/10/03 12:21:35 DEBUG BlockManager: Putting block broadcast_6_piece0 without replication took 0 ms\n",
      "23/10/03 12:21:35 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580\n",
      "23/10/03 12:21:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[20] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/10/03 12:21:35 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
      "23/10/03 12:21:35 DEBUG TaskSetManager: Epoch for TaskSet 6.0: 0\n",
      "23/10/03 12:21:35 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "23/10/03 12:21:35 DEBUG TaskSetManager: Valid locality levels for TaskSet 6.0: NO_PREF, ANY\n",
      "23/10/03 12:21:35 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_6.0, runningTasks: 0\n",
      "23/10/03 12:21:35 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (asusbc-rtl8117.lan, executor driver, partition 0, PROCESS_LOCAL, 7647 bytes) \n",
      "23/10/03 12:21:35 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "23/10/03 12:21:35 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)\n",
      "23/10/03 12:21:35 DEBUG ExecutorMetricsPoller: stageTCMP: (6, 0) -> 1\n",
      "23/10/03 12:21:35 DEBUG BlockManager: Getting local block broadcast_6\n",
      "23/10/03 12:21:35 DEBUG BlockManager: Level for block broadcast_6 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "23/10/03 12:21:35 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:35 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:35 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:35 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:35 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:35 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:35 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:21:35 DEBUG BasicConnectionProvider: JDBC connection initiated with URL: jdbc:postgresql://localhost:5432/tpch and properties: {password=tpch, user=tpch}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+--------------------+-----------+---------------+---------------+--------------+--------------------+\n",
      "|o_orderkey|o_custkey|o_orderstatus|        o_totalprice|o_orderdate|o_orderpriority|        o_clerk|o_shippriority|           o_comment|\n",
      "+----------+---------+-------------+--------------------+-----------+---------------+---------------+--------------+--------------------+\n",
      "|   2502852|  1284280|            O|144403.5900000000...| 1997-12-15|1-URGENT       |Clerk#000006634|             0|the blithely regu...|\n",
      "|   2502853|  1230202|            O|225554.3900000000...| 1995-06-26|1-URGENT       |Clerk#000009585|             0|ructions integrat...|\n",
      "|   2502854|  1502923|            F|63146.54000000000...| 1993-05-17|4-NOT SPECIFIED|Clerk#000008108|             0| deposits about t...|\n",
      "|   2502855|  1409368|            O|7604.730000000000...| 1997-05-12|4-NOT SPECIFIED|Clerk#000001918|             0| affix carefully ...|\n",
      "|   2502880|  1476386|            O|51964.20000000000...| 1995-11-11|4-NOT SPECIFIED|Clerk#000001244|             0|according to the ...|\n",
      "|   2502881|   908666|            F|60178.36000000000...| 1993-03-12|5-LOW          |Clerk#000001557|             0|may affix slyly b...|\n",
      "|   2502882|  1191613|            F|279058.1000000000...| 1992-03-10|5-LOW          |Clerk#000001361|             0|ily. furiously ev...|\n",
      "|   2502883|  1357688|            F|56044.74000000000...| 1992-11-19|5-LOW          |Clerk#000010124|             0|usly final foxes ...|\n",
      "|   2502884|   808549|            F|267181.6900000000...| 1993-05-15|3-MEDIUM       |Clerk#000004888|             0|ins. carefully re...|\n",
      "|   2502885|  1502273|            O|85519.78000000000...| 1997-11-20|3-MEDIUM       |Clerk#000006010|             0|nd the blithely u...|\n",
      "|   2502886|   399970|            O|14394.65000000000...| 1995-10-22|5-LOW          |Clerk#000009537|             0|slyly special pin...|\n",
      "|   2502887|   187552|            F|39378.97000000000...| 1994-01-09|5-LOW          |Clerk#000010355|             0|furiously slyly i...|\n",
      "|   2502912|   654208|            O|303326.6800000000...| 1995-11-21|3-MEDIUM       |Clerk#000009909|             0|arefully close re...|\n",
      "|   2502913|  1315858|            F|234053.6000000000...| 1994-03-01|3-MEDIUM       |Clerk#000004549|             0|long the final pl...|\n",
      "|   2502914|   654785|            O|154514.0800000000...| 1997-03-26|1-URGENT       |Clerk#000006557|             0|uriously regular ...|\n",
      "|   2502915|  1113128|            O|78369.99000000000...| 1998-06-12|4-NOT SPECIFIED|Clerk#000002616|             0|ackages. furiousl...|\n",
      "|   2502916|   632722|            F|54448.41000000000...| 1995-01-09|2-HIGH         |Clerk#000007336|             0|egular packages a...|\n",
      "|   2502917|  1543199|            F|230087.1800000000...| 1993-06-25|4-NOT SPECIFIED|Clerk#000006415|             0|eans. quickly iro...|\n",
      "|   2502918|   187331|            O|53684.32000000000...| 1996-12-03|5-LOW          |Clerk#000005015|             0|s. dependencies h...|\n",
      "|   2502919|   271483|            O|237156.7700000000...| 1996-03-15|1-URGENT       |Clerk#000003671|             0|unusual, busy dep...|\n",
      "+----------+---------+-------------+--------------------+-----------+---------------+---------------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "lineitem\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/03 12:21:41 DEBUG ExecutorMetricsPoller: removing (5, 0) from stageTCMP 1]\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(299)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 299\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 299\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(277)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 277\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 277\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(317)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 317\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 317\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(296)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 296\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 296\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(313)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 313\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 313\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(310)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 310\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 310\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(288)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 288\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 288\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(272)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 272\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 272\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(292)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 292\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 292\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(276)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 276\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 276\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(278)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 278\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 278\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(314)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 314\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 314\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(282)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 282\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 282\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(298)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 298\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 298\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(312)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 312\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 312\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(291)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 291\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 291\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(293)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 293\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 293\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(302)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 302\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 302\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(294)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 294\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 294\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(285)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 285\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 285\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(289)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 289\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 289\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(304)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 304\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 304\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(306)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 306\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 306\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(300)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 300\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 300\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(274)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 274\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 274\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(320)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 320\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 320\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(267)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 267\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 267\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(297)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 297\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 297\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(273)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 273\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 273\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(287)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 287\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 287\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(305)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 305\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 305\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(316)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 316\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 316\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(284)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 284\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 284\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(283)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 283\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 283\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(275)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 275\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 275\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(303)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 303\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 303\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(319)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 319\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 319\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(311)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 311\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 311\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(309)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 309\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 309\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(321)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 321\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 321\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(308)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 308\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 308\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(279)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 279\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 279\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(301)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 301\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 301\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(322)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 322\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 322\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(266)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 266\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 266\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(268)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 268\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 268\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(5)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning broadcast 5\n",
      "23/10/03 12:22:03 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 5\n",
      "23/10/03 12:22:03 DEBUG BlockManagerStorageEndpoint: removing broadcast 5\n",
      "23/10/03 12:22:03 DEBUG BlockManager: Removing broadcast 5\n",
      "23/10/03 12:22:03 DEBUG BlockManager: Removing block broadcast_5_piece0\n",
      "23/10/03 12:22:03 DEBUG MemoryStore: Block broadcast_5_piece0 of size 7902 dropped from memory (free 28802238349)\n",
      "23/10/03 12:22:03 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_5_piece0 for BlockManagerId(driver, asusbc-rtl8117.lan, 34663, None)\n",
      "23/10/03 12:22:03 INFO BlockManagerInfo: Removed broadcast_5_piece0 on asusbc-rtl8117.lan:34663 in memory (size: 7.7 KiB, free: 26.8 GiB)\n",
      "23/10/03 12:22:03 DEBUG BlockManagerMaster: Updated info of block broadcast_5_piece0\n",
      "23/10/03 12:22:03 DEBUG BlockManager: Told master about block broadcast_5_piece0\n",
      "23/10/03 12:22:03 DEBUG BlockManager: Removing block broadcast_5\n",
      "23/10/03 12:22:03 DEBUG MemoryStore: Block broadcast_5 of size 17312 dropped from memory (free 28802255661)\n",
      "23/10/03 12:22:03 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 5, response is 0\n",
      "23/10/03 12:22:03 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to asusbc-rtl8117.lan:44641\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned broadcast 5\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(286)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 286\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 286\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(281)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 281\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 281\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(295)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 295\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 295\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(271)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 271\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 271\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(269)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 269\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 269\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(290)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 290\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 290\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(270)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 270\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 270\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(307)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 307\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 307\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(280)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 280\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 280\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(315)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 315\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 315\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Got cleaning task CleanAccum(318)\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaning accumulator 318\n",
      "23/10/03 12:22:03 DEBUG ContextCleaner: Cleaned accumulator 318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/03 12:22:51 INFO JDBCRDD: closed connection                   (0 + 1) / 1]\n",
      "23/10/03 12:22:51 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 4185 bytes result sent to driver\n",
      "23/10/03 12:22:51 DEBUG ExecutorMetricsPoller: stageTCMP: (6, 0) -> 0\n",
      "23/10/03 12:22:51 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 75425 ms on asusbc-rtl8117.lan (executor driver) (1/1)\n",
      "23/10/03 12:22:51 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "23/10/03 12:22:51 INFO DAGScheduler: ResultStage 6 (showString at NativeMethodAccessorImpl.java:0) finished in 75.432 s\n",
      "23/10/03 12:22:51 DEBUG DAGScheduler: After removal of stage 6, remaining stages = 0\n",
      "23/10/03 12:22:51 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/10/03 12:22:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
      "23/10/03 12:22:51 INFO DAGScheduler: Job 6 finished: showString at NativeMethodAccessorImpl.java:0, took 75.435172 s\n",
      "23/10/03 12:22:51 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "23/10/03 12:22:51 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, false].toString, input[1, string, false].toString, input[2, string, false].toString, input[3, string, false].toString, input[4, string, false].toString, input[5, string, false].toString, input[6, string, false].toString, input[7, string, false].toString, input[8, string, false].toString, input[9, string, false].toString, input[10, string, false].toString, input[11, string, false].toString, input[12, string, false].toString, input[13, string, false].toString, input[14, string, false].toString, input[15, string, false].toString, StructField(toprettystring(l_orderkey),StringType,false), StructField(toprettystring(l_partkey),StringType,false), StructField(toprettystring(l_suppkey),StringType,false), StructField(toprettystring(l_linenumber),StringType,false), StructField(toprettystring(l_quantity),StringType,false), StructField(toprettystring(l_extendedprice),StringType,false), StructField(toprettystring(l_discount),StringType,false), StructField(toprettystring(l_tax),StringType,false), ... 8 more fields):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[16];\n",
      "/* 024 */     createExternalRow_0_0(i, values_0);\n",
      "/* 025 */     createExternalRow_0_1(i, values_0);\n",
      "/* 026 */     createExternalRow_0_2(i, values_0);\n",
      "/* 027 */     createExternalRow_0_3(i, values_0);\n",
      "/* 028 */     createExternalRow_0_4(i, values_0);\n",
      "/* 029 */     createExternalRow_0_5(i, values_0);\n",
      "/* 030 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 031 */     if (false) {\n",
      "/* 032 */       mutableRow.setNullAt(0);\n",
      "/* 033 */     } else {\n",
      "/* 034 */\n",
      "/* 035 */       mutableRow.update(0, value_0);\n",
      "/* 036 */     }\n",
      "/* 037 */\n",
      "/* 038 */     return mutableRow;\n",
      "/* 039 */   }\n",
      "/* 040 */\n",
      "/* 041 */\n",
      "/* 042 */   private void createExternalRow_0_2(InternalRow i, Object[] values_0) {\n",
      "/* 043 */\n",
      "/* 044 */     UTF8String value_14 = i.getUTF8String(6);\n",
      "/* 045 */     boolean isNull_13 = true;\n",
      "/* 046 */     java.lang.String value_13 = null;\n",
      "/* 047 */     isNull_13 = false;\n",
      "/* 048 */     if (!isNull_13) {\n",
      "/* 049 */\n",
      "/* 050 */       Object funcResult_6 = null;\n",
      "/* 051 */       funcResult_6 = value_14.toString();\n",
      "/* 052 */       value_13 = (java.lang.String) funcResult_6;\n",
      "/* 053 */\n",
      "/* 054 */     }\n",
      "/* 055 */     if (isNull_13) {\n",
      "/* 056 */       values_0[6] = null;\n",
      "/* 057 */     } else {\n",
      "/* 058 */       values_0[6] = value_13;\n",
      "/* 059 */     }\n",
      "/* 060 */\n",
      "/* 061 */     UTF8String value_16 = i.getUTF8String(7);\n",
      "/* 062 */     boolean isNull_15 = true;\n",
      "/* 063 */     java.lang.String value_15 = null;\n",
      "/* 064 */     isNull_15 = false;\n",
      "/* 065 */     if (!isNull_15) {\n",
      "/* 066 */\n",
      "/* 067 */       Object funcResult_7 = null;\n",
      "/* 068 */       funcResult_7 = value_16.toString();\n",
      "/* 069 */       value_15 = (java.lang.String) funcResult_7;\n",
      "/* 070 */\n",
      "/* 071 */     }\n",
      "/* 072 */     if (isNull_15) {\n",
      "/* 073 */       values_0[7] = null;\n",
      "/* 074 */     } else {\n",
      "/* 075 */       values_0[7] = value_15;\n",
      "/* 076 */     }\n",
      "/* 077 */\n",
      "/* 078 */     UTF8String value_18 = i.getUTF8String(8);\n",
      "/* 079 */     boolean isNull_17 = true;\n",
      "/* 080 */     java.lang.String value_17 = null;\n",
      "/* 081 */     isNull_17 = false;\n",
      "/* 082 */     if (!isNull_17) {\n",
      "/* 083 */\n",
      "/* 084 */       Object funcResult_8 = null;\n",
      "/* 085 */       funcResult_8 = value_18.toString();\n",
      "/* 086 */       value_17 = (java.lang.String) funcResult_8;\n",
      "/* 087 */\n",
      "/* 088 */     }\n",
      "/* 089 */     if (isNull_17) {\n",
      "/* 090 */       values_0[8] = null;\n",
      "/* 091 */     } else {\n",
      "/* 092 */       values_0[8] = value_17;\n",
      "/* 093 */     }\n",
      "/* 094 */\n",
      "/* 095 */   }\n",
      "/* 096 */\n",
      "/* 097 */\n",
      "/* 098 */   private void createExternalRow_0_5(InternalRow i, Object[] values_0) {\n",
      "/* 099 */\n",
      "/* 100 */     UTF8String value_32 = i.getUTF8String(15);\n",
      "/* 101 */     boolean isNull_31 = true;\n",
      "/* 102 */     java.lang.String value_31 = null;\n",
      "/* 103 */     isNull_31 = false;\n",
      "/* 104 */     if (!isNull_31) {\n",
      "/* 105 */\n",
      "/* 106 */       Object funcResult_15 = null;\n",
      "/* 107 */       funcResult_15 = value_32.toString();\n",
      "/* 108 */       value_31 = (java.lang.String) funcResult_15;\n",
      "/* 109 */\n",
      "/* 110 */     }\n",
      "/* 111 */     if (isNull_31) {\n",
      "/* 112 */       values_0[15] = null;\n",
      "/* 113 */     } else {\n",
      "/* 114 */       values_0[15] = value_31;\n",
      "/* 115 */     }\n",
      "/* 116 */\n",
      "/* 117 */   }\n",
      "/* 118 */\n",
      "/* 119 */\n",
      "/* 120 */   private void createExternalRow_0_4(InternalRow i, Object[] values_0) {\n",
      "/* 121 */\n",
      "/* 122 */     UTF8String value_26 = i.getUTF8String(12);\n",
      "/* 123 */     boolean isNull_25 = true;\n",
      "/* 124 */     java.lang.String value_25 = null;\n",
      "/* 125 */     isNull_25 = false;\n",
      "/* 126 */     if (!isNull_25) {\n",
      "/* 127 */\n",
      "/* 128 */       Object funcResult_12 = null;\n",
      "/* 129 */       funcResult_12 = value_26.toString();\n",
      "/* 130 */       value_25 = (java.lang.String) funcResult_12;\n",
      "/* 131 */\n",
      "/* 132 */     }\n",
      "/* 133 */     if (isNull_25) {\n",
      "/* 134 */       values_0[12] = null;\n",
      "/* 135 */     } else {\n",
      "/* 136 */       values_0[12] = value_25;\n",
      "/* 137 */     }\n",
      "/* 138 */\n",
      "/* 139 */     UTF8String value_28 = i.getUTF8String(13);\n",
      "/* 140 */     boolean isNull_27 = true;\n",
      "/* 141 */     java.lang.String value_27 = null;\n",
      "/* 142 */     isNull_27 = false;\n",
      "/* 143 */     if (!isNull_27) {\n",
      "/* 144 */\n",
      "/* 145 */       Object funcResult_13 = null;\n",
      "/* 146 */       funcResult_13 = value_28.toString();\n",
      "/* 147 */       value_27 = (java.lang.String) funcResult_13;\n",
      "/* 148 */\n",
      "/* 149 */     }\n",
      "/* 150 */     if (isNull_27) {\n",
      "/* 151 */       values_0[13] = null;\n",
      "/* 152 */     } else {\n",
      "/* 153 */       values_0[13] = value_27;\n",
      "/* 154 */     }\n",
      "/* 155 */\n",
      "/* 156 */     UTF8String value_30 = i.getUTF8String(14);\n",
      "/* 157 */     boolean isNull_29 = true;\n",
      "/* 158 */     java.lang.String value_29 = null;\n",
      "/* 159 */     isNull_29 = false;\n",
      "/* 160 */     if (!isNull_29) {\n",
      "/* 161 */\n",
      "/* 162 */       Object funcResult_14 = null;\n",
      "/* 163 */       funcResult_14 = value_30.toString();\n",
      "/* 164 */       value_29 = (java.lang.String) funcResult_14;\n",
      "/* 165 */\n",
      "/* 166 */     }\n",
      "/* 167 */     if (isNull_29) {\n",
      "/* 168 */       values_0[14] = null;\n",
      "/* 169 */     } else {\n",
      "/* 170 */       values_0[14] = value_29;\n",
      "/* 171 */     }\n",
      "/* 172 */\n",
      "/* 173 */   }\n",
      "/* 174 */\n",
      "/* 175 */\n",
      "/* 176 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {\n",
      "/* 177 */\n",
      "/* 178 */     UTF8String value_8 = i.getUTF8String(3);\n",
      "/* 179 */     boolean isNull_7 = true;\n",
      "/* 180 */     java.lang.String value_7 = null;\n",
      "/* 181 */     isNull_7 = false;\n",
      "/* 182 */     if (!isNull_7) {\n",
      "/* 183 */\n",
      "/* 184 */       Object funcResult_3 = null;\n",
      "/* 185 */       funcResult_3 = value_8.toString();\n",
      "/* 186 */       value_7 = (java.lang.String) funcResult_3;\n",
      "/* 187 */\n",
      "/* 188 */     }\n",
      "/* 189 */     if (isNull_7) {\n",
      "/* 190 */       values_0[3] = null;\n",
      "/* 191 */     } else {\n",
      "/* 192 */       values_0[3] = value_7;\n",
      "/* 193 */     }\n",
      "/* 194 */\n",
      "/* 195 */     UTF8String value_10 = i.getUTF8String(4);\n",
      "/* 196 */     boolean isNull_9 = true;\n",
      "/* 197 */     java.lang.String value_9 = null;\n",
      "/* 198 */     isNull_9 = false;\n",
      "/* 199 */     if (!isNull_9) {\n",
      "/* 200 */\n",
      "/* 201 */       Object funcResult_4 = null;\n",
      "/* 202 */       funcResult_4 = value_10.toString();\n",
      "/* 203 */       value_9 = (java.lang.String) funcResult_4;\n",
      "/* 204 */\n",
      "/* 205 */     }\n",
      "/* 206 */     if (isNull_9) {\n",
      "/* 207 */       values_0[4] = null;\n",
      "/* 208 */     } else {\n",
      "/* 209 */       values_0[4] = value_9;\n",
      "/* 210 */     }\n",
      "/* 211 */\n",
      "/* 212 */     UTF8String value_12 = i.getUTF8String(5);\n",
      "/* 213 */     boolean isNull_11 = true;\n",
      "/* 214 */     java.lang.String value_11 = null;\n",
      "/* 215 */     isNull_11 = false;\n",
      "/* 216 */     if (!isNull_11) {\n",
      "/* 217 */\n",
      "/* 218 */       Object funcResult_5 = null;\n",
      "/* 219 */       funcResult_5 = value_12.toString();\n",
      "/* 220 */       value_11 = (java.lang.String) funcResult_5;\n",
      "/* 221 */\n",
      "/* 222 */     }\n",
      "/* 223 */     if (isNull_11) {\n",
      "/* 224 */       values_0[5] = null;\n",
      "/* 225 */     } else {\n",
      "/* 226 */       values_0[5] = value_11;\n",
      "/* 227 */     }\n",
      "/* 228 */\n",
      "/* 229 */   }\n",
      "/* 230 */\n",
      "/* 231 */\n",
      "/* 232 */   private void createExternalRow_0_3(InternalRow i, Object[] values_0) {\n",
      "/* 233 */\n",
      "/* 234 */     UTF8String value_20 = i.getUTF8String(9);\n",
      "/* 235 */     boolean isNull_19 = true;\n",
      "/* 236 */     java.lang.String value_19 = null;\n",
      "/* 237 */     isNull_19 = false;\n",
      "/* 238 */     if (!isNull_19) {\n",
      "/* 239 */\n",
      "/* 240 */       Object funcResult_9 = null;\n",
      "/* 241 */       funcResult_9 = value_20.toString();\n",
      "/* 242 */       value_19 = (java.lang.String) funcResult_9;\n",
      "/* 243 */\n",
      "/* 244 */     }\n",
      "/* 245 */     if (isNull_19) {\n",
      "/* 246 */       values_0[9] = null;\n",
      "/* 247 */     } else {\n",
      "/* 248 */       values_0[9] = value_19;\n",
      "/* 249 */     }\n",
      "/* 250 */\n",
      "/* 251 */     UTF8String value_22 = i.getUTF8String(10);\n",
      "/* 252 */     boolean isNull_21 = true;\n",
      "/* 253 */     java.lang.String value_21 = null;\n",
      "/* 254 */     isNull_21 = false;\n",
      "/* 255 */     if (!isNull_21) {\n",
      "/* 256 */\n",
      "/* 257 */       Object funcResult_10 = null;\n",
      "/* 258 */       funcResult_10 = value_22.toString();\n",
      "/* 259 */       value_21 = (java.lang.String) funcResult_10;\n",
      "/* 260 */\n",
      "/* 261 */     }\n",
      "/* 262 */     if (isNull_21) {\n",
      "/* 263 */       values_0[10] = null;\n",
      "/* 264 */     } else {\n",
      "/* 265 */       values_0[10] = value_21;\n",
      "/* 266 */     }\n",
      "/* 267 */\n",
      "/* 268 */     UTF8String value_24 = i.getUTF8String(11);\n",
      "/* 269 */     boolean isNull_23 = true;\n",
      "/* 270 */     java.lang.String value_23 = null;\n",
      "/* 271 */     isNull_23 = false;\n",
      "/* 272 */     if (!isNull_23) {\n",
      "/* 273 */\n",
      "/* 274 */       Object funcResult_11 = null;\n",
      "/* 275 */       funcResult_11 = value_24.toString();\n",
      "/* 276 */       value_23 = (java.lang.String) funcResult_11;\n",
      "/* 277 */\n",
      "/* 278 */     }\n",
      "/* 279 */     if (isNull_23) {\n",
      "/* 280 */       values_0[11] = null;\n",
      "/* 281 */     } else {\n",
      "/* 282 */       values_0[11] = value_23;\n",
      "/* 283 */     }\n",
      "/* 284 */\n",
      "/* 285 */   }\n",
      "/* 286 */\n",
      "/* 287 */\n",
      "/* 288 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {\n",
      "/* 289 */\n",
      "/* 290 */     UTF8String value_2 = i.getUTF8String(0);\n",
      "/* 291 */     boolean isNull_1 = true;\n",
      "/* 292 */     java.lang.String value_1 = null;\n",
      "/* 293 */     isNull_1 = false;\n",
      "/* 294 */     if (!isNull_1) {\n",
      "/* 295 */\n",
      "/* 296 */       Object funcResult_0 = null;\n",
      "/* 297 */       funcResult_0 = value_2.toString();\n",
      "/* 298 */       value_1 = (java.lang.String) funcResult_0;\n",
      "/* 299 */\n",
      "/* 300 */     }\n",
      "/* 301 */     if (isNull_1) {\n",
      "/* 302 */       values_0[0] = null;\n",
      "/* 303 */     } else {\n",
      "/* 304 */       values_0[0] = value_1;\n",
      "/* 305 */     }\n",
      "/* 306 */\n",
      "/* 307 */     UTF8String value_4 = i.getUTF8String(1);\n",
      "/* 308 */     boolean isNull_3 = true;\n",
      "/* 309 */     java.lang.String value_3 = null;\n",
      "/* 310 */     isNull_3 = false;\n",
      "/* 311 */     if (!isNull_3) {\n",
      "/* 312 */\n",
      "/* 313 */       Object funcResult_1 = null;\n",
      "/* 314 */       funcResult_1 = value_4.toString();\n",
      "/* 315 */       value_3 = (java.lang.String) funcResult_1;\n",
      "/* 316 */\n",
      "/* 317 */     }\n",
      "/* 318 */     if (isNull_3) {\n",
      "/* 319 */       values_0[1] = null;\n",
      "/* 320 */     } else {\n",
      "/* 321 */       values_0[1] = value_3;\n",
      "/* 322 */     }\n",
      "/* 323 */\n",
      "/* 324 */     UTF8String value_6 = i.getUTF8String(2);\n",
      "/* 325 */     boolean isNull_5 = true;\n",
      "/* 326 */     java.lang.String value_5 = null;\n",
      "/* 327 */     isNull_5 = false;\n",
      "/* 328 */     if (!isNull_5) {\n",
      "/* 329 */\n",
      "/* 330 */       Object funcResult_2 = null;\n",
      "/* 331 */       funcResult_2 = value_6.toString();\n",
      "/* 332 */       value_5 = (java.lang.String) funcResult_2;\n",
      "/* 333 */\n",
      "/* 334 */     }\n",
      "/* 335 */     if (isNull_5) {\n",
      "/* 336 */       values_0[2] = null;\n",
      "/* 337 */     } else {\n",
      "/* 338 */       values_0[2] = value_5;\n",
      "/* 339 */     }\n",
      "/* 340 */\n",
      "/* 341 */   }\n",
      "/* 342 */\n",
      "/* 343 */ }\n",
      "\n",
      "23/10/03 12:22:51 DEBUG CodeGenerator: \n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[16];\n",
      "/* 024 */     createExternalRow_0_0(i, values_0);\n",
      "/* 025 */     createExternalRow_0_1(i, values_0);\n",
      "/* 026 */     createExternalRow_0_2(i, values_0);\n",
      "/* 027 */     createExternalRow_0_3(i, values_0);\n",
      "/* 028 */     createExternalRow_0_4(i, values_0);\n",
      "/* 029 */     createExternalRow_0_5(i, values_0);\n",
      "/* 030 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 031 */     if (false) {\n",
      "/* 032 */       mutableRow.setNullAt(0);\n",
      "/* 033 */     } else {\n",
      "/* 034 */\n",
      "/* 035 */       mutableRow.update(0, value_0);\n",
      "/* 036 */     }\n",
      "/* 037 */\n",
      "/* 038 */     return mutableRow;\n",
      "/* 039 */   }\n",
      "/* 040 */\n",
      "/* 041 */\n",
      "/* 042 */   private void createExternalRow_0_2(InternalRow i, Object[] values_0) {\n",
      "/* 043 */\n",
      "/* 044 */     UTF8String value_14 = i.getUTF8String(6);\n",
      "/* 045 */     boolean isNull_13 = true;\n",
      "/* 046 */     java.lang.String value_13 = null;\n",
      "/* 047 */     isNull_13 = false;\n",
      "/* 048 */     if (!isNull_13) {\n",
      "/* 049 */\n",
      "/* 050 */       Object funcResult_6 = null;\n",
      "/* 051 */       funcResult_6 = value_14.toString();\n",
      "/* 052 */       value_13 = (java.lang.String) funcResult_6;\n",
      "/* 053 */\n",
      "/* 054 */     }\n",
      "/* 055 */     if (isNull_13) {\n",
      "/* 056 */       values_0[6] = null;\n",
      "/* 057 */     } else {\n",
      "/* 058 */       values_0[6] = value_13;\n",
      "/* 059 */     }\n",
      "/* 060 */\n",
      "/* 061 */     UTF8String value_16 = i.getUTF8String(7);\n",
      "/* 062 */     boolean isNull_15 = true;\n",
      "/* 063 */     java.lang.String value_15 = null;\n",
      "/* 064 */     isNull_15 = false;\n",
      "/* 065 */     if (!isNull_15) {\n",
      "/* 066 */\n",
      "/* 067 */       Object funcResult_7 = null;\n",
      "/* 068 */       funcResult_7 = value_16.toString();\n",
      "/* 069 */       value_15 = (java.lang.String) funcResult_7;\n",
      "/* 070 */\n",
      "/* 071 */     }\n",
      "/* 072 */     if (isNull_15) {\n",
      "/* 073 */       values_0[7] = null;\n",
      "/* 074 */     } else {\n",
      "/* 075 */       values_0[7] = value_15;\n",
      "/* 076 */     }\n",
      "/* 077 */\n",
      "/* 078 */     UTF8String value_18 = i.getUTF8String(8);\n",
      "/* 079 */     boolean isNull_17 = true;\n",
      "/* 080 */     java.lang.String value_17 = null;\n",
      "/* 081 */     isNull_17 = false;\n",
      "/* 082 */     if (!isNull_17) {\n",
      "/* 083 */\n",
      "/* 084 */       Object funcResult_8 = null;\n",
      "/* 085 */       funcResult_8 = value_18.toString();\n",
      "/* 086 */       value_17 = (java.lang.String) funcResult_8;\n",
      "/* 087 */\n",
      "/* 088 */     }\n",
      "/* 089 */     if (isNull_17) {\n",
      "/* 090 */       values_0[8] = null;\n",
      "/* 091 */     } else {\n",
      "/* 092 */       values_0[8] = value_17;\n",
      "/* 093 */     }\n",
      "/* 094 */\n",
      "/* 095 */   }\n",
      "/* 096 */\n",
      "/* 097 */\n",
      "/* 098 */   private void createExternalRow_0_5(InternalRow i, Object[] values_0) {\n",
      "/* 099 */\n",
      "/* 100 */     UTF8String value_32 = i.getUTF8String(15);\n",
      "/* 101 */     boolean isNull_31 = true;\n",
      "/* 102 */     java.lang.String value_31 = null;\n",
      "/* 103 */     isNull_31 = false;\n",
      "/* 104 */     if (!isNull_31) {\n",
      "/* 105 */\n",
      "/* 106 */       Object funcResult_15 = null;\n",
      "/* 107 */       funcResult_15 = value_32.toString();\n",
      "/* 108 */       value_31 = (java.lang.String) funcResult_15;\n",
      "/* 109 */\n",
      "/* 110 */     }\n",
      "/* 111 */     if (isNull_31) {\n",
      "/* 112 */       values_0[15] = null;\n",
      "/* 113 */     } else {\n",
      "/* 114 */       values_0[15] = value_31;\n",
      "/* 115 */     }\n",
      "/* 116 */\n",
      "/* 117 */   }\n",
      "/* 118 */\n",
      "/* 119 */\n",
      "/* 120 */   private void createExternalRow_0_4(InternalRow i, Object[] values_0) {\n",
      "/* 121 */\n",
      "/* 122 */     UTF8String value_26 = i.getUTF8String(12);\n",
      "/* 123 */     boolean isNull_25 = true;\n",
      "/* 124 */     java.lang.String value_25 = null;\n",
      "/* 125 */     isNull_25 = false;\n",
      "/* 126 */     if (!isNull_25) {\n",
      "/* 127 */\n",
      "/* 128 */       Object funcResult_12 = null;\n",
      "/* 129 */       funcResult_12 = value_26.toString();\n",
      "/* 130 */       value_25 = (java.lang.String) funcResult_12;\n",
      "/* 131 */\n",
      "/* 132 */     }\n",
      "/* 133 */     if (isNull_25) {\n",
      "/* 134 */       values_0[12] = null;\n",
      "/* 135 */     } else {\n",
      "/* 136 */       values_0[12] = value_25;\n",
      "/* 137 */     }\n",
      "/* 138 */\n",
      "/* 139 */     UTF8String value_28 = i.getUTF8String(13);\n",
      "/* 140 */     boolean isNull_27 = true;\n",
      "/* 141 */     java.lang.String value_27 = null;\n",
      "/* 142 */     isNull_27 = false;\n",
      "/* 143 */     if (!isNull_27) {\n",
      "/* 144 */\n",
      "/* 145 */       Object funcResult_13 = null;\n",
      "/* 146 */       funcResult_13 = value_28.toString();\n",
      "/* 147 */       value_27 = (java.lang.String) funcResult_13;\n",
      "/* 148 */\n",
      "/* 149 */     }\n",
      "/* 150 */     if (isNull_27) {\n",
      "/* 151 */       values_0[13] = null;\n",
      "/* 152 */     } else {\n",
      "/* 153 */       values_0[13] = value_27;\n",
      "/* 154 */     }\n",
      "/* 155 */\n",
      "/* 156 */     UTF8String value_30 = i.getUTF8String(14);\n",
      "/* 157 */     boolean isNull_29 = true;\n",
      "/* 158 */     java.lang.String value_29 = null;\n",
      "/* 159 */     isNull_29 = false;\n",
      "/* 160 */     if (!isNull_29) {\n",
      "/* 161 */\n",
      "/* 162 */       Object funcResult_14 = null;\n",
      "/* 163 */       funcResult_14 = value_30.toString();\n",
      "/* 164 */       value_29 = (java.lang.String) funcResult_14;\n",
      "/* 165 */\n",
      "/* 166 */     }\n",
      "/* 167 */     if (isNull_29) {\n",
      "/* 168 */       values_0[14] = null;\n",
      "/* 169 */     } else {\n",
      "/* 170 */       values_0[14] = value_29;\n",
      "/* 171 */     }\n",
      "/* 172 */\n",
      "/* 173 */   }\n",
      "/* 174 */\n",
      "/* 175 */\n",
      "/* 176 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {\n",
      "/* 177 */\n",
      "/* 178 */     UTF8String value_8 = i.getUTF8String(3);\n",
      "/* 179 */     boolean isNull_7 = true;\n",
      "/* 180 */     java.lang.String value_7 = null;\n",
      "/* 181 */     isNull_7 = false;\n",
      "/* 182 */     if (!isNull_7) {\n",
      "/* 183 */\n",
      "/* 184 */       Object funcResult_3 = null;\n",
      "/* 185 */       funcResult_3 = value_8.toString();\n",
      "/* 186 */       value_7 = (java.lang.String) funcResult_3;\n",
      "/* 187 */\n",
      "/* 188 */     }\n",
      "/* 189 */     if (isNull_7) {\n",
      "/* 190 */       values_0[3] = null;\n",
      "/* 191 */     } else {\n",
      "/* 192 */       values_0[3] = value_7;\n",
      "/* 193 */     }\n",
      "/* 194 */\n",
      "/* 195 */     UTF8String value_10 = i.getUTF8String(4);\n",
      "/* 196 */     boolean isNull_9 = true;\n",
      "/* 197 */     java.lang.String value_9 = null;\n",
      "/* 198 */     isNull_9 = false;\n",
      "/* 199 */     if (!isNull_9) {\n",
      "/* 200 */\n",
      "/* 201 */       Object funcResult_4 = null;\n",
      "/* 202 */       funcResult_4 = value_10.toString();\n",
      "/* 203 */       value_9 = (java.lang.String) funcResult_4;\n",
      "/* 204 */\n",
      "/* 205 */     }\n",
      "/* 206 */     if (isNull_9) {\n",
      "/* 207 */       values_0[4] = null;\n",
      "/* 208 */     } else {\n",
      "/* 209 */       values_0[4] = value_9;\n",
      "/* 210 */     }\n",
      "/* 211 */\n",
      "/* 212 */     UTF8String value_12 = i.getUTF8String(5);\n",
      "/* 213 */     boolean isNull_11 = true;\n",
      "/* 214 */     java.lang.String value_11 = null;\n",
      "/* 215 */     isNull_11 = false;\n",
      "/* 216 */     if (!isNull_11) {\n",
      "/* 217 */\n",
      "/* 218 */       Object funcResult_5 = null;\n",
      "/* 219 */       funcResult_5 = value_12.toString();\n",
      "/* 220 */       value_11 = (java.lang.String) funcResult_5;\n",
      "/* 221 */\n",
      "/* 222 */     }\n",
      "/* 223 */     if (isNull_11) {\n",
      "/* 224 */       values_0[5] = null;\n",
      "/* 225 */     } else {\n",
      "/* 226 */       values_0[5] = value_11;\n",
      "/* 227 */     }\n",
      "/* 228 */\n",
      "/* 229 */   }\n",
      "/* 230 */\n",
      "/* 231 */\n",
      "/* 232 */   private void createExternalRow_0_3(InternalRow i, Object[] values_0) {\n",
      "/* 233 */\n",
      "/* 234 */     UTF8String value_20 = i.getUTF8String(9);\n",
      "/* 235 */     boolean isNull_19 = true;\n",
      "/* 236 */     java.lang.String value_19 = null;\n",
      "/* 237 */     isNull_19 = false;\n",
      "/* 238 */     if (!isNull_19) {\n",
      "/* 239 */\n",
      "/* 240 */       Object funcResult_9 = null;\n",
      "/* 241 */       funcResult_9 = value_20.toString();\n",
      "/* 242 */       value_19 = (java.lang.String) funcResult_9;\n",
      "/* 243 */\n",
      "/* 244 */     }\n",
      "/* 245 */     if (isNull_19) {\n",
      "/* 246 */       values_0[9] = null;\n",
      "/* 247 */     } else {\n",
      "/* 248 */       values_0[9] = value_19;\n",
      "/* 249 */     }\n",
      "/* 250 */\n",
      "/* 251 */     UTF8String value_22 = i.getUTF8String(10);\n",
      "/* 252 */     boolean isNull_21 = true;\n",
      "/* 253 */     java.lang.String value_21 = null;\n",
      "/* 254 */     isNull_21 = false;\n",
      "/* 255 */     if (!isNull_21) {\n",
      "/* 256 */\n",
      "/* 257 */       Object funcResult_10 = null;\n",
      "/* 258 */       funcResult_10 = value_22.toString();\n",
      "/* 259 */       value_21 = (java.lang.String) funcResult_10;\n",
      "/* 260 */\n",
      "/* 261 */     }\n",
      "/* 262 */     if (isNull_21) {\n",
      "/* 263 */       values_0[10] = null;\n",
      "/* 264 */     } else {\n",
      "/* 265 */       values_0[10] = value_21;\n",
      "/* 266 */     }\n",
      "/* 267 */\n",
      "/* 268 */     UTF8String value_24 = i.getUTF8String(11);\n",
      "/* 269 */     boolean isNull_23 = true;\n",
      "/* 270 */     java.lang.String value_23 = null;\n",
      "/* 271 */     isNull_23 = false;\n",
      "/* 272 */     if (!isNull_23) {\n",
      "/* 273 */\n",
      "/* 274 */       Object funcResult_11 = null;\n",
      "/* 275 */       funcResult_11 = value_24.toString();\n",
      "/* 276 */       value_23 = (java.lang.String) funcResult_11;\n",
      "/* 277 */\n",
      "/* 278 */     }\n",
      "/* 279 */     if (isNull_23) {\n",
      "/* 280 */       values_0[11] = null;\n",
      "/* 281 */     } else {\n",
      "/* 282 */       values_0[11] = value_23;\n",
      "/* 283 */     }\n",
      "/* 284 */\n",
      "/* 285 */   }\n",
      "/* 286 */\n",
      "/* 287 */\n",
      "/* 288 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {\n",
      "/* 289 */\n",
      "/* 290 */     UTF8String value_2 = i.getUTF8String(0);\n",
      "/* 291 */     boolean isNull_1 = true;\n",
      "/* 292 */     java.lang.String value_1 = null;\n",
      "/* 293 */     isNull_1 = false;\n",
      "/* 294 */     if (!isNull_1) {\n",
      "/* 295 */\n",
      "/* 296 */       Object funcResult_0 = null;\n",
      "/* 297 */       funcResult_0 = value_2.toString();\n",
      "/* 298 */       value_1 = (java.lang.String) funcResult_0;\n",
      "/* 299 */\n",
      "/* 300 */     }\n",
      "/* 301 */     if (isNull_1) {\n",
      "/* 302 */       values_0[0] = null;\n",
      "/* 303 */     } else {\n",
      "/* 304 */       values_0[0] = value_1;\n",
      "/* 305 */     }\n",
      "/* 306 */\n",
      "/* 307 */     UTF8String value_4 = i.getUTF8String(1);\n",
      "/* 308 */     boolean isNull_3 = true;\n",
      "/* 309 */     java.lang.String value_3 = null;\n",
      "/* 310 */     isNull_3 = false;\n",
      "/* 311 */     if (!isNull_3) {\n",
      "/* 312 */\n",
      "/* 313 */       Object funcResult_1 = null;\n",
      "/* 314 */       funcResult_1 = value_4.toString();\n",
      "/* 315 */       value_3 = (java.lang.String) funcResult_1;\n",
      "/* 316 */\n",
      "/* 317 */     }\n",
      "/* 318 */     if (isNull_3) {\n",
      "/* 319 */       values_0[1] = null;\n",
      "/* 320 */     } else {\n",
      "/* 321 */       values_0[1] = value_3;\n",
      "/* 322 */     }\n",
      "/* 323 */\n",
      "/* 324 */     UTF8String value_6 = i.getUTF8String(2);\n",
      "/* 325 */     boolean isNull_5 = true;\n",
      "/* 326 */     java.lang.String value_5 = null;\n",
      "/* 327 */     isNull_5 = false;\n",
      "/* 328 */     if (!isNull_5) {\n",
      "/* 329 */\n",
      "/* 330 */       Object funcResult_2 = null;\n",
      "/* 331 */       funcResult_2 = value_6.toString();\n",
      "/* 332 */       value_5 = (java.lang.String) funcResult_2;\n",
      "/* 333 */\n",
      "/* 334 */     }\n",
      "/* 335 */     if (isNull_5) {\n",
      "/* 336 */       values_0[2] = null;\n",
      "/* 337 */     } else {\n",
      "/* 338 */       values_0[2] = value_5;\n",
      "/* 339 */     }\n",
      "/* 340 */\n",
      "/* 341 */   }\n",
      "/* 342 */\n",
      "/* 343 */ }\n",
      "\n",
      "23/10/03 12:22:51 INFO CodeGenerator: Code generated in 13.739346 ms\n",
      "23/10/03 12:22:51 DEBUG SparkSqlParser: Parsing command: lineitem\n",
      "23/10/03 12:22:51 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:22:51 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:22:51 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:22:51 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:22:51 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:22:51 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:22:51 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:22:51 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:22:51 DEBUG BasicConnectionProvider: JDBC connection initiated with URL: jdbc:postgresql://localhost:5432/tpch and properties: {password=tpch, user=tpch}\n",
      "23/10/03 12:22:51 DEBUG CatalystSqlParser: Parsing command: char(25)\n",
      "23/10/03 12:22:51 DEBUG CatalystSqlParser: Parsing command: varchar(152)\n",
      "23/10/03 12:22:51 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator scan_input_0;\n",
      "/* 010 */   private boolean project_resultIsNull_0;\n",
      "/* 011 */   private int project_argValue_0;\n",
      "/* 012 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] scan_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 013 */   private UTF8String[] project_mutableStateArray_0 = new UTF8String[1];\n",
      "/* 014 */\n",
      "/* 015 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 016 */     this.references = references;\n",
      "/* 017 */   }\n",
      "/* 018 */\n",
      "/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 020 */     partitionIndex = index;\n",
      "/* 021 */     this.inputs = inputs;\n",
      "/* 022 */     scan_input_0 = inputs[0];\n",
      "/* 023 */     scan_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 64);\n",
      "/* 024 */\n",
      "/* 025 */     scan_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 128);\n",
      "/* 026 */\n",
      "/* 027 */   }\n",
      "/* 028 */\n",
      "/* 029 */   protected void processNext() throws java.io.IOException {\n",
      "/* 030 */     while ( scan_input_0.hasNext()) {\n",
      "/* 031 */       InternalRow scan_row_0 = (InternalRow) scan_input_0.next();\n",
      "/* 032 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 033 */       // common sub-expressions\n",
      "/* 034 */\n",
      "/* 035 */       boolean scan_isNull_0 = scan_row_0.isNullAt(0);\n",
      "/* 036 */       int scan_value_0 = scan_isNull_0 ?\n",
      "/* 037 */       -1 : (scan_row_0.getInt(0));\n",
      "/* 038 */       UTF8String project_value_0;\n",
      "/* 039 */       if (scan_isNull_0) {\n",
      "/* 040 */         project_value_0 = UTF8String.fromString(\"NULL\");\n",
      "/* 041 */       } else {\n",
      "/* 042 */         project_value_0 = UTF8String.fromString(String.valueOf(scan_value_0));\n",
      "/* 043 */       }\n",
      "/* 044 */       project_resultIsNull_0 = false;\n",
      "/* 045 */\n",
      "/* 046 */       if (!project_resultIsNull_0) {\n",
      "/* 047 */         boolean scan_isNull_1 = scan_row_0.isNullAt(1);\n",
      "/* 048 */         UTF8String scan_value_1 = scan_isNull_1 ?\n",
      "/* 049 */         null : (scan_row_0.getUTF8String(1));\n",
      "/* 050 */         project_resultIsNull_0 = scan_isNull_1;\n",
      "/* 051 */         project_mutableStateArray_0[0] = scan_value_1;\n",
      "/* 052 */       }\n",
      "/* 053 */\n",
      "/* 054 */       if (!project_resultIsNull_0) {\n",
      "/* 055 */         project_argValue_0 = 25;\n",
      "/* 056 */       }\n",
      "/* 057 */\n",
      "/* 058 */       boolean project_isNull_3 = project_resultIsNull_0;\n",
      "/* 059 */       UTF8String project_value_3 = null;\n",
      "/* 060 */       if (!project_resultIsNull_0) {\n",
      "/* 061 */         project_value_3 = org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils.readSidePadding(project_mutableStateArray_0[0], project_argValue_0);\n",
      "/* 062 */       }\n",
      "/* 063 */       UTF8String project_value_2;\n",
      "/* 064 */       if (project_isNull_3) {\n",
      "/* 065 */         project_value_2 = UTF8String.fromString(\"NULL\");\n",
      "/* 066 */       } else {\n",
      "/* 067 */         project_value_2 = project_value_3;\n",
      "/* 068 */       }\n",
      "/* 069 */       boolean scan_isNull_2 = scan_row_0.isNullAt(2);\n",
      "/* 070 */       long scan_value_2 = scan_isNull_2 ?\n",
      "/* 071 */       -1L : (scan_row_0.getLong(2));\n",
      "/* 072 */       UTF8String project_value_6;\n",
      "/* 073 */       if (scan_isNull_2) {\n",
      "/* 074 */         project_value_6 = UTF8String.fromString(\"NULL\");\n",
      "/* 075 */       } else {\n",
      "/* 076 */         project_value_6 = UTF8String.fromString(String.valueOf(scan_value_2));\n",
      "/* 077 */       }\n",
      "/* 078 */       boolean scan_isNull_3 = scan_row_0.isNullAt(3);\n",
      "/* 079 */       UTF8String scan_value_3 = scan_isNull_3 ?\n",
      "/* 080 */       null : (scan_row_0.getUTF8String(3));\n",
      "/* 081 */       UTF8String project_value_8;\n",
      "/* 082 */       if (scan_isNull_3) {\n",
      "/* 083 */         project_value_8 = UTF8String.fromString(\"NULL\");\n",
      "/* 084 */       } else {\n",
      "/* 085 */         project_value_8 = scan_value_3;\n",
      "/* 086 */       }\n",
      "/* 087 */       scan_mutableStateArray_0[1].reset();\n",
      "/* 088 */\n",
      "/* 089 */       scan_mutableStateArray_0[1].write(0, project_value_0);\n",
      "/* 090 */\n",
      "/* 091 */       scan_mutableStateArray_0[1].write(1, project_value_2);\n",
      "/* 092 */\n",
      "/* 093 */       scan_mutableStateArray_0[1].write(2, project_value_6);\n",
      "/* 094 */\n",
      "/* 095 */       scan_mutableStateArray_0[1].write(3, project_value_8);\n",
      "/* 096 */       append((scan_mutableStateArray_0[1].getRow()));\n",
      "/* 097 */       if (shouldStop()) return;\n",
      "/* 098 */     }\n",
      "/* 099 */   }\n",
      "/* 100 */\n",
      "/* 101 */ }\n",
      "\n",
      "23/10/03 12:22:51 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator scan_input_0;\n",
      "/* 010 */   private boolean project_resultIsNull_0;\n",
      "/* 011 */   private int project_argValue_0;\n",
      "/* 012 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] scan_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 013 */   private UTF8String[] project_mutableStateArray_0 = new UTF8String[1];\n",
      "/* 014 */\n",
      "/* 015 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 016 */     this.references = references;\n",
      "/* 017 */   }\n",
      "/* 018 */\n",
      "/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 020 */     partitionIndex = index;\n",
      "/* 021 */     this.inputs = inputs;\n",
      "/* 022 */     scan_input_0 = inputs[0];\n",
      "/* 023 */     scan_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 64);\n",
      "/* 024 */\n",
      "/* 025 */     scan_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 128);\n",
      "/* 026 */\n",
      "/* 027 */   }\n",
      "/* 028 */\n",
      "/* 029 */   protected void processNext() throws java.io.IOException {\n",
      "/* 030 */     while ( scan_input_0.hasNext()) {\n",
      "/* 031 */       InternalRow scan_row_0 = (InternalRow) scan_input_0.next();\n",
      "/* 032 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 033 */       // common sub-expressions\n",
      "/* 034 */\n",
      "/* 035 */       boolean scan_isNull_0 = scan_row_0.isNullAt(0);\n",
      "/* 036 */       int scan_value_0 = scan_isNull_0 ?\n",
      "/* 037 */       -1 : (scan_row_0.getInt(0));\n",
      "/* 038 */       UTF8String project_value_0;\n",
      "/* 039 */       if (scan_isNull_0) {\n",
      "/* 040 */         project_value_0 = UTF8String.fromString(\"NULL\");\n",
      "/* 041 */       } else {\n",
      "/* 042 */         project_value_0 = UTF8String.fromString(String.valueOf(scan_value_0));\n",
      "/* 043 */       }\n",
      "/* 044 */       project_resultIsNull_0 = false;\n",
      "/* 045 */\n",
      "/* 046 */       if (!project_resultIsNull_0) {\n",
      "/* 047 */         boolean scan_isNull_1 = scan_row_0.isNullAt(1);\n",
      "/* 048 */         UTF8String scan_value_1 = scan_isNull_1 ?\n",
      "/* 049 */         null : (scan_row_0.getUTF8String(1));\n",
      "/* 050 */         project_resultIsNull_0 = scan_isNull_1;\n",
      "/* 051 */         project_mutableStateArray_0[0] = scan_value_1;\n",
      "/* 052 */       }\n",
      "/* 053 */\n",
      "/* 054 */       if (!project_resultIsNull_0) {\n",
      "/* 055 */         project_argValue_0 = 25;\n",
      "/* 056 */       }\n",
      "/* 057 */\n",
      "/* 058 */       boolean project_isNull_3 = project_resultIsNull_0;\n",
      "/* 059 */       UTF8String project_value_3 = null;\n",
      "/* 060 */       if (!project_resultIsNull_0) {\n",
      "/* 061 */         project_value_3 = org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils.readSidePadding(project_mutableStateArray_0[0], project_argValue_0);\n",
      "/* 062 */       }\n",
      "/* 063 */       UTF8String project_value_2;\n",
      "/* 064 */       if (project_isNull_3) {\n",
      "/* 065 */         project_value_2 = UTF8String.fromString(\"NULL\");\n",
      "/* 066 */       } else {\n",
      "/* 067 */         project_value_2 = project_value_3;\n",
      "/* 068 */       }\n",
      "/* 069 */       boolean scan_isNull_2 = scan_row_0.isNullAt(2);\n",
      "/* 070 */       long scan_value_2 = scan_isNull_2 ?\n",
      "/* 071 */       -1L : (scan_row_0.getLong(2));\n",
      "/* 072 */       UTF8String project_value_6;\n",
      "/* 073 */       if (scan_isNull_2) {\n",
      "/* 074 */         project_value_6 = UTF8String.fromString(\"NULL\");\n",
      "/* 075 */       } else {\n",
      "/* 076 */         project_value_6 = UTF8String.fromString(String.valueOf(scan_value_2));\n",
      "/* 077 */       }\n",
      "/* 078 */       boolean scan_isNull_3 = scan_row_0.isNullAt(3);\n",
      "/* 079 */       UTF8String scan_value_3 = scan_isNull_3 ?\n",
      "/* 080 */       null : (scan_row_0.getUTF8String(3));\n",
      "/* 081 */       UTF8String project_value_8;\n",
      "/* 082 */       if (scan_isNull_3) {\n",
      "/* 083 */         project_value_8 = UTF8String.fromString(\"NULL\");\n",
      "/* 084 */       } else {\n",
      "/* 085 */         project_value_8 = scan_value_3;\n",
      "/* 086 */       }\n",
      "/* 087 */       scan_mutableStateArray_0[1].reset();\n",
      "/* 088 */\n",
      "/* 089 */       scan_mutableStateArray_0[1].write(0, project_value_0);\n",
      "/* 090 */\n",
      "/* 091 */       scan_mutableStateArray_0[1].write(1, project_value_2);\n",
      "/* 092 */\n",
      "/* 093 */       scan_mutableStateArray_0[1].write(2, project_value_6);\n",
      "/* 094 */\n",
      "/* 095 */       scan_mutableStateArray_0[1].write(3, project_value_8);\n",
      "/* 096 */       append((scan_mutableStateArray_0[1].getRow()));\n",
      "/* 097 */       if (shouldStop()) return;\n",
      "/* 098 */     }\n",
      "/* 099 */   }\n",
      "/* 100 */\n",
      "/* 101 */ }\n",
      "\n",
      "23/10/03 12:22:51 INFO CodeGenerator: Code generated in 9.40526 ms\n",
      "23/10/03 12:22:51 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "23/10/03 12:22:51 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "23/10/03 12:22:51 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$executeTake$2\n",
      "23/10/03 12:22:51 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++\n",
      "23/10/03 12:22:51 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "23/10/03 12:22:51 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "23/10/03 12:22:51 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/10/03 12:22:51 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 23 took 0.000072 seconds\n",
      "23/10/03 12:22:51 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "23/10/03 12:22:51 INFO DAGScheduler: Got job 7 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/10/03 12:22:51 INFO DAGScheduler: Final stage: ResultStage 7 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/10/03 12:22:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/10/03 12:22:51 INFO DAGScheduler: Missing parents: List()\n",
      "23/10/03 12:22:51 DEBUG DAGScheduler: submitStage(ResultStage 7 (name=showString at NativeMethodAccessorImpl.java:0;jobs=7))\n",
      "23/10/03 12:22:51 DEBUG DAGScheduler: missing: List()\n",
      "23/10/03 12:22:51 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[23] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/10/03 12:22:51 DEBUG DAGScheduler: submitMissingTasks(ResultStage 7)\n",
      "23/10/03 12:22:51 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 12.9 KiB, free 26.8 GiB)\n",
      "23/10/03 12:22:51 DEBUG BlockManager: Put block broadcast_7 locally took 0 ms\n",
      "23/10/03 12:22:51 DEBUG BlockManager: Putting block broadcast_7 without replication took 0 ms\n",
      "23/10/03 12:22:51 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 26.8 GiB)\n",
      "23/10/03 12:22:51 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_7_piece0 for BlockManagerId(driver, asusbc-rtl8117.lan, 34663, None)\n",
      "23/10/03 12:22:51 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on asusbc-rtl8117.lan:34663 (size: 6.4 KiB, free: 26.8 GiB)\n",
      "23/10/03 12:22:51 DEBUG BlockManagerMaster: Updated info of block broadcast_7_piece0\n",
      "23/10/03 12:22:51 DEBUG BlockManager: Told master about block broadcast_7_piece0\n",
      "23/10/03 12:22:51 DEBUG BlockManager: Put block broadcast_7_piece0 locally took 1 ms\n",
      "23/10/03 12:22:51 DEBUG BlockManager: Putting block broadcast_7_piece0 without replication took 1 ms\n",
      "23/10/03 12:22:51 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580\n",
      "23/10/03 12:22:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[23] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/10/03 12:22:51 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
      "23/10/03 12:22:51 DEBUG TaskSetManager: Epoch for TaskSet 7.0: 0\n",
      "23/10/03 12:22:51 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "23/10/03 12:22:51 DEBUG TaskSetManager: Valid locality levels for TaskSet 7.0: NO_PREF, ANY\n",
      "23/10/03 12:22:51 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_7.0, runningTasks: 0\n",
      "23/10/03 12:22:51 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (asusbc-rtl8117.lan, executor driver, partition 0, PROCESS_LOCAL, 7647 bytes) \n",
      "23/10/03 12:22:51 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "23/10/03 12:22:51 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)\n",
      "23/10/03 12:22:51 DEBUG ExecutorMetricsPoller: stageTCMP: (7, 0) -> 1\n",
      "23/10/03 12:22:51 DEBUG BlockManager: Getting local block broadcast_7\n",
      "23/10/03 12:22:51 DEBUG BlockManager: Level for block broadcast_7 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "23/10/03 12:22:51 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:22:51 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:22:51 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:22:51 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:22:51 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:22:51 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:22:51 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:22:51 DEBUG BasicConnectionProvider: JDBC connection initiated with URL: jdbc:postgresql://localhost:5432/tpch and properties: {password=tpch, user=tpch}\n",
      "23/10/03 12:22:51 INFO JDBCRDD: closed connection\n",
      "23/10/03 12:22:51 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 3184 bytes result sent to driver\n",
      "23/10/03 12:22:51 DEBUG ExecutorMetricsPoller: stageTCMP: (7, 0) -> 0\n",
      "23/10/03 12:22:51 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 24 ms on asusbc-rtl8117.lan (executor driver) (1/1)\n",
      "23/10/03 12:22:51 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "23/10/03 12:22:51 INFO DAGScheduler: ResultStage 7 (showString at NativeMethodAccessorImpl.java:0) finished in 0.030 s\n",
      "23/10/03 12:22:51 DEBUG DAGScheduler: After removal of stage 7, remaining stages = 0\n",
      "23/10/03 12:22:51 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/10/03 12:22:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
      "23/10/03 12:22:51 INFO DAGScheduler: Job 7 finished: showString at NativeMethodAccessorImpl.java:0, took 0.032998 s\n",
      "23/10/03 12:22:51 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, false].toString, input[1, string, false].toString, input[2, string, false].toString, input[3, string, false].toString, StructField(toprettystring(n_nationkey),StringType,false), StructField(toprettystring(n_name),StringType,false), StructField(toprettystring(n_regionkey),StringType,false), StructField(toprettystring(n_comment),StringType,false)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[4];\n",
      "/* 024 */     createExternalRow_0_0(i, values_0);\n",
      "/* 025 */     createExternalRow_0_1(i, values_0);\n",
      "/* 026 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 027 */     if (false) {\n",
      "/* 028 */       mutableRow.setNullAt(0);\n",
      "/* 029 */     } else {\n",
      "/* 030 */\n",
      "/* 031 */       mutableRow.update(0, value_0);\n",
      "/* 032 */     }\n",
      "/* 033 */\n",
      "/* 034 */     return mutableRow;\n",
      "/* 035 */   }\n",
      "/* 036 */\n",
      "/* 037 */\n",
      "/* 038 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {\n",
      "/* 039 */\n",
      "/* 040 */     UTF8String value_8 = i.getUTF8String(3);\n",
      "/* 041 */     boolean isNull_7 = true;\n",
      "/* 042 */     java.lang.String value_7 = null;\n",
      "/* 043 */     isNull_7 = false;\n",
      "/* 044 */     if (!isNull_7) {\n",
      "/* 045 */\n",
      "/* 046 */       Object funcResult_3 = null;\n",
      "/* 047 */       funcResult_3 = value_8.toString();\n",
      "/* 048 */       value_7 = (java.lang.String) funcResult_3;\n",
      "/* 049 */\n",
      "/* 050 */     }\n",
      "/* 051 */     if (isNull_7) {\n",
      "/* 052 */       values_0[3] = null;\n",
      "/* 053 */     } else {\n",
      "/* 054 */       values_0[3] = value_7;\n",
      "/* 055 */     }\n",
      "/* 056 */\n",
      "/* 057 */   }\n",
      "/* 058 */\n",
      "/* 059 */\n",
      "/* 060 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {\n",
      "/* 061 */\n",
      "/* 062 */     UTF8String value_2 = i.getUTF8String(0);\n",
      "/* 063 */     boolean isNull_1 = true;\n",
      "/* 064 */     java.lang.String value_1 = null;\n",
      "/* 065 */     isNull_1 = false;\n",
      "/* 066 */     if (!isNull_1) {\n",
      "/* 067 */\n",
      "/* 068 */       Object funcResult_0 = null;\n",
      "/* 069 */       funcResult_0 = value_2.toString();\n",
      "/* 070 */       value_1 = (java.lang.String) funcResult_0;\n",
      "/* 071 */\n",
      "/* 072 */     }\n",
      "/* 073 */     if (isNull_1) {\n",
      "/* 074 */       values_0[0] = null;\n",
      "/* 075 */     } else {\n",
      "/* 076 */       values_0[0] = value_1;\n",
      "/* 077 */     }\n",
      "/* 078 */\n",
      "/* 079 */     UTF8String value_4 = i.getUTF8String(1);\n",
      "/* 080 */     boolean isNull_3 = true;\n",
      "/* 081 */     java.lang.String value_3 = null;\n",
      "/* 082 */     isNull_3 = false;\n",
      "/* 083 */     if (!isNull_3) {\n",
      "/* 084 */\n",
      "/* 085 */       Object funcResult_1 = null;\n",
      "/* 086 */       funcResult_1 = value_4.toString();\n",
      "/* 087 */       value_3 = (java.lang.String) funcResult_1;\n",
      "/* 088 */\n",
      "/* 089 */     }\n",
      "/* 090 */     if (isNull_3) {\n",
      "/* 091 */       values_0[1] = null;\n",
      "/* 092 */     } else {\n",
      "/* 093 */       values_0[1] = value_3;\n",
      "/* 094 */     }\n",
      "/* 095 */\n",
      "/* 096 */     UTF8String value_6 = i.getUTF8String(2);\n",
      "/* 097 */     boolean isNull_5 = true;\n",
      "/* 098 */     java.lang.String value_5 = null;\n",
      "/* 099 */     isNull_5 = false;\n",
      "/* 100 */     if (!isNull_5) {\n",
      "/* 101 */\n",
      "/* 102 */       Object funcResult_2 = null;\n",
      "/* 103 */       funcResult_2 = value_6.toString();\n",
      "/* 104 */       value_5 = (java.lang.String) funcResult_2;\n",
      "/* 105 */\n",
      "/* 106 */     }\n",
      "/* 107 */     if (isNull_5) {\n",
      "/* 108 */       values_0[2] = null;\n",
      "/* 109 */     } else {\n",
      "/* 110 */       values_0[2] = value_5;\n",
      "/* 111 */     }\n",
      "/* 112 */\n",
      "/* 113 */   }\n",
      "/* 114 */\n",
      "/* 115 */ }\n",
      "\n",
      "23/10/03 12:22:51 DEBUG CodeGenerator: \n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[4];\n",
      "/* 024 */     createExternalRow_0_0(i, values_0);\n",
      "/* 025 */     createExternalRow_0_1(i, values_0);\n",
      "/* 026 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 027 */     if (false) {\n",
      "/* 028 */       mutableRow.setNullAt(0);\n",
      "/* 029 */     } else {\n",
      "/* 030 */\n",
      "/* 031 */       mutableRow.update(0, value_0);\n",
      "/* 032 */     }\n",
      "/* 033 */\n",
      "/* 034 */     return mutableRow;\n",
      "/* 035 */   }\n",
      "/* 036 */\n",
      "/* 037 */\n",
      "/* 038 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {\n",
      "/* 039 */\n",
      "/* 040 */     UTF8String value_8 = i.getUTF8String(3);\n",
      "/* 041 */     boolean isNull_7 = true;\n",
      "/* 042 */     java.lang.String value_7 = null;\n",
      "/* 043 */     isNull_7 = false;\n",
      "/* 044 */     if (!isNull_7) {\n",
      "/* 045 */\n",
      "/* 046 */       Object funcResult_3 = null;\n",
      "/* 047 */       funcResult_3 = value_8.toString();\n",
      "/* 048 */       value_7 = (java.lang.String) funcResult_3;\n",
      "/* 049 */\n",
      "/* 050 */     }\n",
      "/* 051 */     if (isNull_7) {\n",
      "/* 052 */       values_0[3] = null;\n",
      "/* 053 */     } else {\n",
      "/* 054 */       values_0[3] = value_7;\n",
      "/* 055 */     }\n",
      "/* 056 */\n",
      "/* 057 */   }\n",
      "/* 058 */\n",
      "/* 059 */\n",
      "/* 060 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {\n",
      "/* 061 */\n",
      "/* 062 */     UTF8String value_2 = i.getUTF8String(0);\n",
      "/* 063 */     boolean isNull_1 = true;\n",
      "/* 064 */     java.lang.String value_1 = null;\n",
      "/* 065 */     isNull_1 = false;\n",
      "/* 066 */     if (!isNull_1) {\n",
      "/* 067 */\n",
      "/* 068 */       Object funcResult_0 = null;\n",
      "/* 069 */       funcResult_0 = value_2.toString();\n",
      "/* 070 */       value_1 = (java.lang.String) funcResult_0;\n",
      "/* 071 */\n",
      "/* 072 */     }\n",
      "/* 073 */     if (isNull_1) {\n",
      "/* 074 */       values_0[0] = null;\n",
      "/* 075 */     } else {\n",
      "/* 076 */       values_0[0] = value_1;\n",
      "/* 077 */     }\n",
      "/* 078 */\n",
      "/* 079 */     UTF8String value_4 = i.getUTF8String(1);\n",
      "/* 080 */     boolean isNull_3 = true;\n",
      "/* 081 */     java.lang.String value_3 = null;\n",
      "/* 082 */     isNull_3 = false;\n",
      "/* 083 */     if (!isNull_3) {\n",
      "/* 084 */\n",
      "/* 085 */       Object funcResult_1 = null;\n",
      "/* 086 */       funcResult_1 = value_4.toString();\n",
      "/* 087 */       value_3 = (java.lang.String) funcResult_1;\n",
      "/* 088 */\n",
      "/* 089 */     }\n",
      "/* 090 */     if (isNull_3) {\n",
      "/* 091 */       values_0[1] = null;\n",
      "/* 092 */     } else {\n",
      "/* 093 */       values_0[1] = value_3;\n",
      "/* 094 */     }\n",
      "/* 095 */\n",
      "/* 096 */     UTF8String value_6 = i.getUTF8String(2);\n",
      "/* 097 */     boolean isNull_5 = true;\n",
      "/* 098 */     java.lang.String value_5 = null;\n",
      "/* 099 */     isNull_5 = false;\n",
      "/* 100 */     if (!isNull_5) {\n",
      "/* 101 */\n",
      "/* 102 */       Object funcResult_2 = null;\n",
      "/* 103 */       funcResult_2 = value_6.toString();\n",
      "/* 104 */       value_5 = (java.lang.String) funcResult_2;\n",
      "/* 105 */\n",
      "/* 106 */     }\n",
      "/* 107 */     if (isNull_5) {\n",
      "/* 108 */       values_0[2] = null;\n",
      "/* 109 */     } else {\n",
      "/* 110 */       values_0[2] = value_5;\n",
      "/* 111 */     }\n",
      "/* 112 */\n",
      "/* 113 */   }\n",
      "/* 114 */\n",
      "/* 115 */ }\n",
      "\n",
      "23/10/03 12:22:51 INFO CodeGenerator: Code generated in 7.163093 ms\n",
      "23/10/03 12:22:51 DEBUG SparkSqlParser: Parsing command: nation\n",
      "23/10/03 12:22:51 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:22:51 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:22:51 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:22:51 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:22:51 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:22:51 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:22:51 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:22:51 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:22:51 DEBUG BasicConnectionProvider: JDBC connection initiated with URL: jdbc:postgresql://localhost:5432/tpch and properties: {password=tpch, user=tpch}\n",
      "23/10/03 12:22:51 DEBUG CatalystSqlParser: Parsing command: char(25)\n",
      "23/10/03 12:22:51 DEBUG CatalystSqlParser: Parsing command: varchar(152)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+------------+--------------------+--------------------+--------------------+--------------------+------------+------------+----------+------------+-------------+--------------------+----------+--------------------+\n",
      "|l_orderkey|l_partkey|l_suppkey|l_linenumber|          l_quantity|     l_extendedprice|          l_discount|               l_tax|l_returnflag|l_linestatus|l_shipdate|l_commitdate|l_receiptdate|      l_shipinstruct|l_shipmode|           l_comment|\n",
      "+----------+---------+---------+------------+--------------------+--------------------+--------------------+--------------------+------------+------------+----------+------------+-------------+--------------------+----------+--------------------+\n",
      "|    249472|  1500803|    15830|           1|16.00000000000000...|28859.68000000000...|0.070000000000000000|0.030000000000000000|           N|           O|1995-12-15|  1996-01-11|   1995-12-23|NONE             ...|SHIP      |     n accounts afte|\n",
      "|    249472|   981486|    18995|           2|43.00000000000000...|67399.92000000000...|0.050000000000000000|0.050000000000000000|           N|           O|1996-01-12|  1996-03-01|   1996-01-21|NONE             ...|TRUCK     |     wake ironically|\n",
      "|    249472|   223952|    58957|           3|49.00000000000000...|91921.06000000000...|0.030000000000000000|0.020000000000000000|           N|           O|1996-02-09|  1996-01-05|   1996-02-22|TAKE BACK RETURN ...|SHIP      |bove the carefull...|\n",
      "|    249472|  1946625|    21660|           4|16.00000000000000...|26744.48000000000...|0.010000000000000000|0.000000000000000000|           N|           O|1996-03-02|  1996-02-05|   1996-03-25|DELIVER IN PERSON...|SHIP      |          furiously |\n",
      "|    249472|   720913|    33432|           5|49.00000000000000...|94760.12000000000...|0.090000000000000000|0.050000000000000000|           N|           O|1996-02-27|  1996-02-02|   1996-02-29|COLLECT COD      ...|TRUCK     |final dolphins ea...|\n",
      "|    249472|   978961|    16470|           6|37.00000000000000...|75477.04000000000...|0.000000000000000000|0.050000000000000000|           N|           O|1996-03-23|  1996-02-28|   1996-03-28|COLLECT COD      ...|REG AIR   |symptotes. reques...|\n",
      "|    249473|  1543347|    58376|           1|40.00000000000000...|55610.80000000000...|0.040000000000000000|0.070000000000000000|           A|           F|1994-12-26|  1994-11-16|   1995-01-07|DELIVER IN PERSON...|FOB       | fluffily even as...|\n",
      "|    249473|  1020952|    85971|           2|25.00000000000000...|46822.50000000000...|0.100000000000000000|0.020000000000000000|           A|           F|1995-01-19|  1994-12-21|   1995-01-26|TAKE BACK RETURN ...|SHIP      |ructions play acc...|\n",
      "|    249473|  1336325|    71350|           3|20.00000000000000...|27225.20000000000...|0.040000000000000000|0.000000000000000000|           A|           F|1994-10-13|  1994-11-20|   1994-10-29|DELIVER IN PERSON...|TRUCK     |instructions slee...|\n",
      "|    249474|   953033|    45558|           1|10.00000000000000...|10859.90000000000...|0.100000000000000000|0.080000000000000000|           A|           F|1993-12-05|  1993-11-01|   1993-12-24|COLLECT COD      ...|TRUCK     |          ar account|\n",
      "|    249474|  1620949|   108464|           2|9.000000000000000000|16828.74000000000...|0.090000000000000000|0.010000000000000000|           A|           F|1993-10-10|  1993-09-25|   1993-10-31|NONE             ...|REG AIR   |mise furiously ev...|\n",
      "|    249474|   674404|    96923|           3|46.00000000000000...|63405.02000000000...|0.040000000000000000|0.040000000000000000|           A|           F|1993-11-08|  1993-10-20|   1993-11-17|DELIVER IN PERSON...|FOB       |      special instru|\n",
      "|    249474|   303944|    56451|           4|21.00000000000000...|40906.53000000000...|0.090000000000000000|0.000000000000000000|           A|           F|1993-08-26|  1993-10-31|   1993-09-05|COLLECT COD      ...|AIR       |pecial packages. ...|\n",
      "|    249475|  1791743|    59260|           1|41.00000000000000...|75221.06000000000...|0.010000000000000000|0.030000000000000000|           N|           O|1997-09-01|  1997-08-18|   1997-09-04|NONE             ...|SHIP      |       osits. slyly |\n",
      "|    249476|   360117|     2627|           1|49.00000000000000...|57677.90000000000...|0.080000000000000000|0.070000000000000000|           R|           F|1993-05-19|  1993-07-09|   1993-06-10|NONE             ...|RAIL      |          s the pack|\n",
      "|    249476|   285730|    38237|           2|7.000000000000000000|12010.04000000000...|0.070000000000000000|0.020000000000000000|           A|           F|1993-06-04|  1993-07-05|   1993-06-19|NONE             ...|FOB       |ffix quickly slyl...|\n",
      "|    249476|  1854773|    94774|           3|19.00000000000000...|32825.92000000000...|0.000000000000000000|0.020000000000000000|           A|           F|1993-08-04|  1993-05-31|   1993-08-14|COLLECT COD      ...|FOB       |           even acco|\n",
      "|    249476|  1364651|    99676|           4|14.00000000000000...|24018.26000000000...|0.040000000000000000|0.070000000000000000|           A|           F|1993-06-19|  1993-06-13|   1993-07-04|TAKE BACK RETURN ...|TRUCK     |         efully iron|\n",
      "|    249477|  1917703|    47704|           1|10.00000000000000...|17206.10000000000...|0.020000000000000000|0.000000000000000000|           A|           F|1992-07-27|  1992-08-22|   1992-07-31|COLLECT COD      ...|TRUCK     |  carefully silent f|\n",
      "|    249478|   909880|    84897|           1|27.00000000000000...|51025.68000000000...|0.090000000000000000|0.010000000000000000|           N|           O|1997-05-22|  1997-06-18|   1997-06-06|COLLECT COD      ...|AIR       |ide of the ironic...|\n",
      "+----------+---------+---------+------------+--------------------+--------------------+--------------------+--------------------+------------+------------+----------+------------+-------------+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "nation\n",
      "+-----------+--------------------+-----------+--------------------+\n",
      "|n_nationkey|              n_name|n_regionkey|           n_comment|\n",
      "+-----------+--------------------+-----------+--------------------+\n",
      "|          0|ALGERIA          ...|          0| haggle. carefull...|\n",
      "|          1|ARGENTINA        ...|          1|al foxes promise ...|\n",
      "|          2|BRAZIL           ...|          1|y alongside of th...|\n",
      "|          3|CANADA           ...|          1|eas hang ironic, ...|\n",
      "|          4|EGYPT            ...|          4|y above the caref...|\n",
      "|          5|ETHIOPIA         ...|          0|ven packages wake...|\n",
      "|          6|FRANCE           ...|          3|refully final req...|\n",
      "|          7|GERMANY          ...|          3|l platelets. regu...|\n",
      "|          8|INDIA            ...|          2|ss excuses cajole...|\n",
      "|          9|INDONESIA        ...|          2| slyly express as...|\n",
      "|         10|IRAN             ...|          4|efully alongside ...|\n",
      "|         11|IRAQ             ...|          4|nic deposits boos...|\n",
      "|         12|JAPAN            ...|          2|ously. final, exp...|\n",
      "|         13|JORDAN           ...|          4|ic deposits are b...|\n",
      "|         14|KENYA            ...|          0| pending excuses ...|\n",
      "|         15|MOROCCO          ...|          0|rns. blithely bol...|\n",
      "|         16|MOZAMBIQUE       ...|          0|s. ironic, unusua...|\n",
      "|         17|PERU             ...|          1|platelets. blithe...|\n",
      "|         18|CHINA            ...|          2|c dependencies. f...|\n",
      "|         19|ROMANIA          ...|          3|ular asymptotes a...|\n",
      "+-----------+--------------------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "region\n",
      "+-----------+--------------------+--------------------+\n",
      "|r_regionkey|              r_name|           r_comment|\n",
      "+-----------+--------------------+--------------------+\n",
      "|          0|AFRICA           ...|lar deposits. bli...|\n",
      "|          1|AMERICA          ...|hs use ironic, ev...|\n",
      "|          2|ASIA             ...|ges. thinly even ...|\n",
      "|          3|EUROPE           ...|ly final courts c...|\n",
      "|          4|MIDDLE EAST      ...|uickly special ac...|\n",
      "+-----------+--------------------+--------------------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/03 12:22:51 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator scan_input_0;\n",
      "/* 010 */   private boolean project_resultIsNull_0;\n",
      "/* 011 */   private int project_argValue_0;\n",
      "/* 012 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] scan_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 013 */   private UTF8String[] project_mutableStateArray_0 = new UTF8String[1];\n",
      "/* 014 */\n",
      "/* 015 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 016 */     this.references = references;\n",
      "/* 017 */   }\n",
      "/* 018 */\n",
      "/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 020 */     partitionIndex = index;\n",
      "/* 021 */     this.inputs = inputs;\n",
      "/* 022 */     scan_input_0 = inputs[0];\n",
      "/* 023 */     scan_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 64);\n",
      "/* 024 */\n",
      "/* 025 */     scan_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 96);\n",
      "/* 026 */\n",
      "/* 027 */   }\n",
      "/* 028 */\n",
      "/* 029 */   protected void processNext() throws java.io.IOException {\n",
      "/* 030 */     while ( scan_input_0.hasNext()) {\n",
      "/* 031 */       InternalRow scan_row_0 = (InternalRow) scan_input_0.next();\n",
      "/* 032 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 033 */       // common sub-expressions\n",
      "/* 034 */\n",
      "/* 035 */       boolean scan_isNull_0 = scan_row_0.isNullAt(0);\n",
      "/* 036 */       int scan_value_0 = scan_isNull_0 ?\n",
      "/* 037 */       -1 : (scan_row_0.getInt(0));\n",
      "/* 038 */       UTF8String project_value_0;\n",
      "/* 039 */       if (scan_isNull_0) {\n",
      "/* 040 */         project_value_0 = UTF8String.fromString(\"NULL\");\n",
      "/* 041 */       } else {\n",
      "/* 042 */         project_value_0 = UTF8String.fromString(String.valueOf(scan_value_0));\n",
      "/* 043 */       }\n",
      "/* 044 */       project_resultIsNull_0 = false;\n",
      "/* 045 */\n",
      "/* 046 */       if (!project_resultIsNull_0) {\n",
      "/* 047 */         boolean scan_isNull_1 = scan_row_0.isNullAt(1);\n",
      "/* 048 */         UTF8String scan_value_1 = scan_isNull_1 ?\n",
      "/* 049 */         null : (scan_row_0.getUTF8String(1));\n",
      "/* 050 */         project_resultIsNull_0 = scan_isNull_1;\n",
      "/* 051 */         project_mutableStateArray_0[0] = scan_value_1;\n",
      "/* 052 */       }\n",
      "/* 053 */\n",
      "/* 054 */       if (!project_resultIsNull_0) {\n",
      "/* 055 */         project_argValue_0 = 25;\n",
      "/* 056 */       }\n",
      "/* 057 */\n",
      "/* 058 */       boolean project_isNull_3 = project_resultIsNull_0;\n",
      "/* 059 */       UTF8String project_value_3 = null;\n",
      "/* 060 */       if (!project_resultIsNull_0) {\n",
      "/* 061 */         project_value_3 = org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils.readSidePadding(project_mutableStateArray_0[0], project_argValue_0);\n",
      "/* 062 */       }\n",
      "/* 063 */       UTF8String project_value_2;\n",
      "/* 064 */       if (project_isNull_3) {\n",
      "/* 065 */         project_value_2 = UTF8String.fromString(\"NULL\");\n",
      "/* 066 */       } else {\n",
      "/* 067 */         project_value_2 = project_value_3;\n",
      "/* 068 */       }\n",
      "/* 069 */       boolean scan_isNull_2 = scan_row_0.isNullAt(2);\n",
      "/* 070 */       UTF8String scan_value_2 = scan_isNull_2 ?\n",
      "/* 071 */       null : (scan_row_0.getUTF8String(2));\n",
      "/* 072 */       UTF8String project_value_6;\n",
      "/* 073 */       if (scan_isNull_2) {\n",
      "/* 074 */         project_value_6 = UTF8String.fromString(\"NULL\");\n",
      "/* 075 */       } else {\n",
      "/* 076 */         project_value_6 = scan_value_2;\n",
      "/* 077 */       }\n",
      "/* 078 */       scan_mutableStateArray_0[1].reset();\n",
      "/* 079 */\n",
      "/* 080 */       scan_mutableStateArray_0[1].write(0, project_value_0);\n",
      "/* 081 */\n",
      "/* 082 */       scan_mutableStateArray_0[1].write(1, project_value_2);\n",
      "/* 083 */\n",
      "/* 084 */       scan_mutableStateArray_0[1].write(2, project_value_6);\n",
      "/* 085 */       append((scan_mutableStateArray_0[1].getRow()));\n",
      "/* 086 */       if (shouldStop()) return;\n",
      "/* 087 */     }\n",
      "/* 088 */   }\n",
      "/* 089 */\n",
      "/* 090 */ }\n",
      "\n",
      "23/10/03 12:22:51 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator scan_input_0;\n",
      "/* 010 */   private boolean project_resultIsNull_0;\n",
      "/* 011 */   private int project_argValue_0;\n",
      "/* 012 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] scan_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 013 */   private UTF8String[] project_mutableStateArray_0 = new UTF8String[1];\n",
      "/* 014 */\n",
      "/* 015 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 016 */     this.references = references;\n",
      "/* 017 */   }\n",
      "/* 018 */\n",
      "/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 020 */     partitionIndex = index;\n",
      "/* 021 */     this.inputs = inputs;\n",
      "/* 022 */     scan_input_0 = inputs[0];\n",
      "/* 023 */     scan_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 64);\n",
      "/* 024 */\n",
      "/* 025 */     scan_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 96);\n",
      "/* 026 */\n",
      "/* 027 */   }\n",
      "/* 028 */\n",
      "/* 029 */   protected void processNext() throws java.io.IOException {\n",
      "/* 030 */     while ( scan_input_0.hasNext()) {\n",
      "/* 031 */       InternalRow scan_row_0 = (InternalRow) scan_input_0.next();\n",
      "/* 032 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 033 */       // common sub-expressions\n",
      "/* 034 */\n",
      "/* 035 */       boolean scan_isNull_0 = scan_row_0.isNullAt(0);\n",
      "/* 036 */       int scan_value_0 = scan_isNull_0 ?\n",
      "/* 037 */       -1 : (scan_row_0.getInt(0));\n",
      "/* 038 */       UTF8String project_value_0;\n",
      "/* 039 */       if (scan_isNull_0) {\n",
      "/* 040 */         project_value_0 = UTF8String.fromString(\"NULL\");\n",
      "/* 041 */       } else {\n",
      "/* 042 */         project_value_0 = UTF8String.fromString(String.valueOf(scan_value_0));\n",
      "/* 043 */       }\n",
      "/* 044 */       project_resultIsNull_0 = false;\n",
      "/* 045 */\n",
      "/* 046 */       if (!project_resultIsNull_0) {\n",
      "/* 047 */         boolean scan_isNull_1 = scan_row_0.isNullAt(1);\n",
      "/* 048 */         UTF8String scan_value_1 = scan_isNull_1 ?\n",
      "/* 049 */         null : (scan_row_0.getUTF8String(1));\n",
      "/* 050 */         project_resultIsNull_0 = scan_isNull_1;\n",
      "/* 051 */         project_mutableStateArray_0[0] = scan_value_1;\n",
      "/* 052 */       }\n",
      "/* 053 */\n",
      "/* 054 */       if (!project_resultIsNull_0) {\n",
      "/* 055 */         project_argValue_0 = 25;\n",
      "/* 056 */       }\n",
      "/* 057 */\n",
      "/* 058 */       boolean project_isNull_3 = project_resultIsNull_0;\n",
      "/* 059 */       UTF8String project_value_3 = null;\n",
      "/* 060 */       if (!project_resultIsNull_0) {\n",
      "/* 061 */         project_value_3 = org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils.readSidePadding(project_mutableStateArray_0[0], project_argValue_0);\n",
      "/* 062 */       }\n",
      "/* 063 */       UTF8String project_value_2;\n",
      "/* 064 */       if (project_isNull_3) {\n",
      "/* 065 */         project_value_2 = UTF8String.fromString(\"NULL\");\n",
      "/* 066 */       } else {\n",
      "/* 067 */         project_value_2 = project_value_3;\n",
      "/* 068 */       }\n",
      "/* 069 */       boolean scan_isNull_2 = scan_row_0.isNullAt(2);\n",
      "/* 070 */       UTF8String scan_value_2 = scan_isNull_2 ?\n",
      "/* 071 */       null : (scan_row_0.getUTF8String(2));\n",
      "/* 072 */       UTF8String project_value_6;\n",
      "/* 073 */       if (scan_isNull_2) {\n",
      "/* 074 */         project_value_6 = UTF8String.fromString(\"NULL\");\n",
      "/* 075 */       } else {\n",
      "/* 076 */         project_value_6 = scan_value_2;\n",
      "/* 077 */       }\n",
      "/* 078 */       scan_mutableStateArray_0[1].reset();\n",
      "/* 079 */\n",
      "/* 080 */       scan_mutableStateArray_0[1].write(0, project_value_0);\n",
      "/* 081 */\n",
      "/* 082 */       scan_mutableStateArray_0[1].write(1, project_value_2);\n",
      "/* 083 */\n",
      "/* 084 */       scan_mutableStateArray_0[1].write(2, project_value_6);\n",
      "/* 085 */       append((scan_mutableStateArray_0[1].getRow()));\n",
      "/* 086 */       if (shouldStop()) return;\n",
      "/* 087 */     }\n",
      "/* 088 */   }\n",
      "/* 089 */\n",
      "/* 090 */ }\n",
      "\n",
      "23/10/03 12:22:51 INFO CodeGenerator: Code generated in 7.737661 ms\n",
      "23/10/03 12:22:51 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "23/10/03 12:22:51 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "23/10/03 12:22:51 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$executeTake$2\n",
      "23/10/03 12:22:51 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++\n",
      "23/10/03 12:22:51 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "23/10/03 12:22:51 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "23/10/03 12:22:51 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/10/03 12:22:51 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 26 took 0.000057 seconds\n",
      "23/10/03 12:22:51 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "23/10/03 12:22:51 INFO DAGScheduler: Got job 8 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/10/03 12:22:51 INFO DAGScheduler: Final stage: ResultStage 8 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/10/03 12:22:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/10/03 12:22:51 INFO DAGScheduler: Missing parents: List()\n",
      "23/10/03 12:22:51 DEBUG DAGScheduler: submitStage(ResultStage 8 (name=showString at NativeMethodAccessorImpl.java:0;jobs=8))\n",
      "23/10/03 12:22:51 DEBUG DAGScheduler: missing: List()\n",
      "23/10/03 12:22:51 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[26] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/10/03 12:22:51 DEBUG DAGScheduler: submitMissingTasks(ResultStage 8)\n",
      "23/10/03 12:22:51 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 12.4 KiB, free 26.8 GiB)\n",
      "23/10/03 12:22:51 DEBUG BlockManager: Put block broadcast_8 locally took 0 ms\n",
      "23/10/03 12:22:51 DEBUG BlockManager: Putting block broadcast_8 without replication took 0 ms\n",
      "23/10/03 12:22:51 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 26.8 GiB)\n",
      "23/10/03 12:22:51 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_8_piece0 for BlockManagerId(driver, asusbc-rtl8117.lan, 34663, None)\n",
      "23/10/03 12:22:51 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on asusbc-rtl8117.lan:34663 (size: 6.3 KiB, free: 26.8 GiB)\n",
      "23/10/03 12:22:51 DEBUG BlockManagerMaster: Updated info of block broadcast_8_piece0\n",
      "23/10/03 12:22:51 DEBUG BlockManager: Told master about block broadcast_8_piece0\n",
      "23/10/03 12:22:51 DEBUG BlockManager: Put block broadcast_8_piece0 locally took 1 ms\n",
      "23/10/03 12:22:51 DEBUG BlockManager: Putting block broadcast_8_piece0 without replication took 1 ms\n",
      "23/10/03 12:22:51 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1580\n",
      "23/10/03 12:22:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[26] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/10/03 12:22:51 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
      "23/10/03 12:22:51 DEBUG TaskSetManager: Epoch for TaskSet 8.0: 0\n",
      "23/10/03 12:22:51 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "23/10/03 12:22:51 DEBUG TaskSetManager: Valid locality levels for TaskSet 8.0: NO_PREF, ANY\n",
      "23/10/03 12:22:51 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_8.0, runningTasks: 0\n",
      "23/10/03 12:22:51 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (asusbc-rtl8117.lan, executor driver, partition 0, PROCESS_LOCAL, 7647 bytes) \n",
      "23/10/03 12:22:51 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "23/10/03 12:22:51 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)\n",
      "23/10/03 12:22:51 DEBUG ExecutorMetricsPoller: stageTCMP: (8, 0) -> 1\n",
      "23/10/03 12:22:51 DEBUG BlockManager: Getting local block broadcast_8\n",
      "23/10/03 12:22:51 DEBUG BlockManager: Level for block broadcast_8 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "23/10/03 12:22:51 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:22:51 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:22:51 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:22:51 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:22:51 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:22:51 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:22:51 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "23/10/03 12:22:51 DEBUG BasicConnectionProvider: JDBC connection initiated with URL: jdbc:postgresql://localhost:5432/tpch and properties: {password=tpch, user=tpch}\n",
      "23/10/03 12:22:51 INFO JDBCRDD: closed connection\n",
      "23/10/03 12:22:51 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 1907 bytes result sent to driver\n",
      "23/10/03 12:22:51 DEBUG ExecutorMetricsPoller: stageTCMP: (8, 0) -> 0\n",
      "23/10/03 12:22:51 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 20 ms on asusbc-rtl8117.lan (executor driver) (1/1)\n",
      "23/10/03 12:22:51 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "23/10/03 12:22:51 INFO DAGScheduler: ResultStage 8 (showString at NativeMethodAccessorImpl.java:0) finished in 0.026 s\n",
      "23/10/03 12:22:51 DEBUG DAGScheduler: After removal of stage 8, remaining stages = 0\n",
      "23/10/03 12:22:51 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/10/03 12:22:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
      "23/10/03 12:22:51 INFO DAGScheduler: Job 8 finished: showString at NativeMethodAccessorImpl.java:0, took 0.027296 s\n",
      "23/10/03 12:22:51 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, false].toString, input[1, string, false].toString, input[2, string, false].toString, StructField(toprettystring(r_regionkey),StringType,false), StructField(toprettystring(r_name),StringType,false), StructField(toprettystring(r_comment),StringType,false)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     UTF8String value_2 = i.getUTF8String(0);\n",
      "/* 039 */     boolean isNull_1 = true;\n",
      "/* 040 */     java.lang.String value_1 = null;\n",
      "/* 041 */     isNull_1 = false;\n",
      "/* 042 */     if (!isNull_1) {\n",
      "/* 043 */\n",
      "/* 044 */       Object funcResult_0 = null;\n",
      "/* 045 */       funcResult_0 = value_2.toString();\n",
      "/* 046 */       value_1 = (java.lang.String) funcResult_0;\n",
      "/* 047 */\n",
      "/* 048 */     }\n",
      "/* 049 */     if (isNull_1) {\n",
      "/* 050 */       values_0[0] = null;\n",
      "/* 051 */     } else {\n",
      "/* 052 */       values_0[0] = value_1;\n",
      "/* 053 */     }\n",
      "/* 054 */\n",
      "/* 055 */     UTF8String value_4 = i.getUTF8String(1);\n",
      "/* 056 */     boolean isNull_3 = true;\n",
      "/* 057 */     java.lang.String value_3 = null;\n",
      "/* 058 */     isNull_3 = false;\n",
      "/* 059 */     if (!isNull_3) {\n",
      "/* 060 */\n",
      "/* 061 */       Object funcResult_1 = null;\n",
      "/* 062 */       funcResult_1 = value_4.toString();\n",
      "/* 063 */       value_3 = (java.lang.String) funcResult_1;\n",
      "/* 064 */\n",
      "/* 065 */     }\n",
      "/* 066 */     if (isNull_3) {\n",
      "/* 067 */       values_0[1] = null;\n",
      "/* 068 */     } else {\n",
      "/* 069 */       values_0[1] = value_3;\n",
      "/* 070 */     }\n",
      "/* 071 */\n",
      "/* 072 */     UTF8String value_6 = i.getUTF8String(2);\n",
      "/* 073 */     boolean isNull_5 = true;\n",
      "/* 074 */     java.lang.String value_5 = null;\n",
      "/* 075 */     isNull_5 = false;\n",
      "/* 076 */     if (!isNull_5) {\n",
      "/* 077 */\n",
      "/* 078 */       Object funcResult_2 = null;\n",
      "/* 079 */       funcResult_2 = value_6.toString();\n",
      "/* 080 */       value_5 = (java.lang.String) funcResult_2;\n",
      "/* 081 */\n",
      "/* 082 */     }\n",
      "/* 083 */     if (isNull_5) {\n",
      "/* 084 */       values_0[2] = null;\n",
      "/* 085 */     } else {\n",
      "/* 086 */       values_0[2] = value_5;\n",
      "/* 087 */     }\n",
      "/* 088 */\n",
      "/* 089 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 090 */\n",
      "/* 091 */     return value_0;\n",
      "/* 092 */   }\n",
      "/* 093 */\n",
      "/* 094 */ }\n",
      "\n",
      "23/10/03 12:22:51 DEBUG CodeGenerator: \n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     UTF8String value_2 = i.getUTF8String(0);\n",
      "/* 039 */     boolean isNull_1 = true;\n",
      "/* 040 */     java.lang.String value_1 = null;\n",
      "/* 041 */     isNull_1 = false;\n",
      "/* 042 */     if (!isNull_1) {\n",
      "/* 043 */\n",
      "/* 044 */       Object funcResult_0 = null;\n",
      "/* 045 */       funcResult_0 = value_2.toString();\n",
      "/* 046 */       value_1 = (java.lang.String) funcResult_0;\n",
      "/* 047 */\n",
      "/* 048 */     }\n",
      "/* 049 */     if (isNull_1) {\n",
      "/* 050 */       values_0[0] = null;\n",
      "/* 051 */     } else {\n",
      "/* 052 */       values_0[0] = value_1;\n",
      "/* 053 */     }\n",
      "/* 054 */\n",
      "/* 055 */     UTF8String value_4 = i.getUTF8String(1);\n",
      "/* 056 */     boolean isNull_3 = true;\n",
      "/* 057 */     java.lang.String value_3 = null;\n",
      "/* 058 */     isNull_3 = false;\n",
      "/* 059 */     if (!isNull_3) {\n",
      "/* 060 */\n",
      "/* 061 */       Object funcResult_1 = null;\n",
      "/* 062 */       funcResult_1 = value_4.toString();\n",
      "/* 063 */       value_3 = (java.lang.String) funcResult_1;\n",
      "/* 064 */\n",
      "/* 065 */     }\n",
      "/* 066 */     if (isNull_3) {\n",
      "/* 067 */       values_0[1] = null;\n",
      "/* 068 */     } else {\n",
      "/* 069 */       values_0[1] = value_3;\n",
      "/* 070 */     }\n",
      "/* 071 */\n",
      "/* 072 */     UTF8String value_6 = i.getUTF8String(2);\n",
      "/* 073 */     boolean isNull_5 = true;\n",
      "/* 074 */     java.lang.String value_5 = null;\n",
      "/* 075 */     isNull_5 = false;\n",
      "/* 076 */     if (!isNull_5) {\n",
      "/* 077 */\n",
      "/* 078 */       Object funcResult_2 = null;\n",
      "/* 079 */       funcResult_2 = value_6.toString();\n",
      "/* 080 */       value_5 = (java.lang.String) funcResult_2;\n",
      "/* 081 */\n",
      "/* 082 */     }\n",
      "/* 083 */     if (isNull_5) {\n",
      "/* 084 */       values_0[2] = null;\n",
      "/* 085 */     } else {\n",
      "/* 086 */       values_0[2] = value_5;\n",
      "/* 087 */     }\n",
      "/* 088 */\n",
      "/* 089 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 090 */\n",
      "/* 091 */     return value_0;\n",
      "/* 092 */   }\n",
      "/* 093 */\n",
      "/* 094 */ }\n",
      "\n",
      "23/10/03 12:22:51 INFO CodeGenerator: Code generated in 4.864459 ms\n",
      "23/10/03 12:22:51 DEBUG SparkSqlParser: Parsing command: region\n"
     ]
    }
   ],
   "source": [
    "username = os.environ.get('USERNAME', 'tpch')\n",
    "password = os.environ.get('PASSWORD', 'tpch')\n",
    "dbname = os.environ.get('DBNAME', 'tpch')\n",
    "dbhost = os.environ.get('DBHOST', 'localhost')\n",
    "\n",
    "df_tables = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", f'jdbc:postgresql://{dbhost}:5432/{dbname}') \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"dbtable\", \"information_schema.tables\") \\\n",
    "    .option(\"user\", username) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .load()\n",
    "\n",
    "for idx, row in df_tables.toPandas().iterrows():\n",
    "        if row.table_schema == 'public':\n",
    "            table_name = row.table_name\n",
    "            df = spark.read.format(\"jdbc\") \\\n",
    "                .option(\"url\", f'jdbc:postgresql://{dbhost}:5432/{dbname}') \\\n",
    "                .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "                .option(\"dbtable\", table_name) \\\n",
    "                .option(\"user\", username) \\\n",
    "                .option(\"password\", password) \\\n",
    "                .load()\n",
    "    \n",
    "            print(table_name)\n",
    "            print(df.show())\n",
    "            df.createOrReplaceTempView(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ebbb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SET spark.sql.yannakakis.countGroupInLeaves = false\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa45949",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SET spark.sql.yannakakis.enabled = false\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a728c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SET spark.sql.yannakakis.enabled = true\").show()\n",
    "spark.sql(\"SET spark.sql.yannakakis.countGroupInLeaves = false\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2ddf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SET spark.sql.cbo.enabled = true\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5f51e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"ANALYZE TABLE part COMPUTE STATISTICS;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979d2464",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"\"\"select\n",
    "        /*+ FK(ps_partkey, p_partkey), FK(n_regionkey, r_regionkey) */\n",
    "\t\tMEDIAN(p_size)\n",
    "\t\tfrom\n",
    "            part,\n",
    "\t\t\tpartsupp,\n",
    "\t\t\tsupplier,\n",
    "\t\t\tnation,\n",
    "\t\t\tregion\n",
    "\t\twhere\n",
    "\t\t\tp_partkey = ps_partkey\n",
    "\t\t\tand s_suppkey = ps_suppkey\n",
    "\t\t\tand s_nationkey = n_nationkey\n",
    "\t\t\tand n_regionkey = r_regionkey\"\"\")\n",
    "\n",
    "df.show(500)\n",
    "\n",
    "df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c11b3515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(file):\n",
    "    with open(file, 'r') as f:\n",
    "        query = '\\n'.join(filter(lambda line: not line.startswith('limit') and not line.startswith('-'), f.readlines()))\n",
    "        \n",
    "        print(\"running query: \\n\" + query)\n",
    "        return spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5587e1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"\"\"\n",
    "SELECT ps_partkey, count(*) from part, partsupp\n",
    "WHERE p_partkey = ps_partkey\n",
    "GROUP BY ps_partkey\n",
    "\"\"\")\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d50f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT COUNT(p_size) / COUNT(DISTINCT p_size) FROM part\")\n",
    "df.show()\n",
    "df = spark.sql(\"SELECT COUNT(p_retailprice) / COUNT(DISTINCT p_retailprice) FROM part\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008e187f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_t1 = spark.createDataFrame([(1,1), (2,1), (2,2), (3,2), (3,3), (4,3), (4,3), (5,2), (5,1), (6,4)], schema=(\"a\",\"b\"))\n",
    "df_t1.createOrReplaceTempView(\"t1\")\n",
    "df_t2 = spark.createDataFrame([(1,1), (2,1), (3,2), (3,2), (3,3), (3,3), (4,3), (4,2), (5,1), (6,4)], schema=(\"c\",\"d\"))\n",
    "df_t2.createOrReplaceTempView(\"t2\")\n",
    "df_t3 = spark.createDataFrame([(1,1), (2,1), (3,2), (3,2), (3,3), (3,3), (4,3), (4,2), (5,1), (6,4)], schema=(\"e\",\"f\"))\n",
    "df_t3.createOrReplaceTempView(\"t3\")\n",
    "\n",
    "query = \"select median(a) from t1, t2 where b = c\"\n",
    "#query = \"select percentile(a, 0.5, b) from t1, t2 where b = c\"\n",
    "#query = \"select median(a) from t1 where EXISTS (SELECT 1 FROM t2 WHERE b = c)\"\n",
    "#query = \"select count(*) from t1, t2 where b = c\"\n",
    "#query = \"select *a from t1 where EXISTS (SELECT 1 FROM t2 WHERE b = c)\"\n",
    "\n",
    "spark.sql(\"SET spark.sql.yannakakis.enabled = false\").show()\n",
    "\n",
    "df = spark.sql(query)\n",
    "df.show()\n",
    "\n",
    "spark.sql(\"SET spark.sql.yannakakis.enabled = true\").show()\n",
    "\n",
    "df = spark.sql(query)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44762b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.setCommandRejectsSparkCoreConfs\",\"false\")\n",
    "#spark.conf.set(\"spark.executor.cores\", \"6\")\n",
    "#spark.conf.set(\"spark.executor.instances\", \"6\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead489e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def benchmark(query):\n",
    "    df0 = run_query(query)\n",
    "    df0.show()\n",
    "    \n",
    "    spark.sql(\"SET spark.sql.yannakakis.enabled = true\").show()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    df1 = run_query(query)\n",
    "    df1.show()\n",
    "    #df1.explain(True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    yannakakis_time = end_time - start_time\n",
    "\n",
    "    spark.sql(\"SET spark.sql.yannakakis.enabled = false\").show()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    df2 = run_query(query)\n",
    "    df2.show()\n",
    "    #df2.explain(True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    ref_time = end_time - start_time\n",
    "    \n",
    "    #return [query, ref_time, yannakakis_time]\n",
    "    return [query, ref_time, yannakakis_time]\n",
    "\n",
    "\n",
    "queries = ['tpch-kit/dbgen/queries/postgres/2.sql',\n",
    "           'tpch-kit/dbgen/queries/postgres/11.sql', \n",
    "           'tpch-kit/dbgen/queries/postgres/11-hint.sql',\n",
    "           'median-1.sql',\n",
    "           'median-2.sql', \n",
    "           'median-3.sql', \n",
    "           'median-4.sql', \n",
    "           'median-5.sql',\n",
    "            'median-1-hint.sql',\n",
    "           'median-2-hint.sql', \n",
    "           'median-3-hint.sql', \n",
    "           'median-4-hint.sql', \n",
    "           'median-5-hint.sql']\n",
    "\n",
    "results = [benchmark(q) for q in queries]\n",
    "\n",
    "df = pd.DataFrame(results, columns = ['query', 'ref_time', 'yannakakis_time'])\n",
    "\n",
    "print(df)\n",
    "\n",
    "df.to_csv(\"results.csv\")\n",
    "    \n",
    "\n",
    "#print(f'row count: {df1.count()} vs. {df2.count()}' )\n",
    "    #print(f'time ref: {ref_time}\\ntime yannakakis: {yannakakis_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1283025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.conf.set(\"spark.sql.legacy.setCommandRejectsSparkCoreConfs\",\"false\")\n",
    "#spark.conf.set(\"spark.executor.cores\", \"1\")\n",
    "#spark.conf.set(\"spark.executor.instances\", \"1\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bd2cc0e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                 key|value|\n",
      "+--------------------+-----+\n",
      "|spark.sql.yannaka...| true|\n",
      "+--------------------+-----+\n",
      "\n",
      "running query: \n",
      "select\n",
      "\n",
      "/*+ FK(ps_partkey, p_partkey), FK(n_regionkey, r_regionkey), FK(ps_suppkey, s_suppkey), FK(s_nationkey, n_nationkey), PK(ps_partkey, ps_suppkey) */\n",
      "\n",
      "median(ps_supplycost)\n",
      "\n",
      "\t\tfrom\n",
      "\n",
      "            part,\n",
      "\n",
      "\t\t\tpartsupp,\n",
      "\n",
      "\t\t\tsupplier,\n",
      "\n",
      "\t\t\tnation,\n",
      "\n",
      "\t\t\tregion\n",
      "\n",
      "\t\twhere\n",
      "\n",
      "\t\t\tp_partkey = ps_partkey\n",
      "\n",
      "\t\t\tand s_suppkey = ps_suppkey\n",
      "\n",
      "\t\t\tand s_nationkey = n_nationkey\n",
      "\n",
      "\t\t\tand n_regionkey = r_regionkey\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/03 12:36:44 WARN RewriteJoinsAsSemijoins: applying yannakakis rewriting to join: Aggregate [toprettystring(percentile(ps_supplycost#130, 0.5, 1, 0, 0, false), Some(Etc/UTC)) AS toprettystring(median(ps_supplycost))#551]\n",
      "+- Project [ps_supplycost#130]\n",
      "   +- FKHint [[ps_partkey#127L, p_partkey#24], [ps_suppkey#128L, s_suppkey#82], [n_regionkey#370L, r_regionkey#394], [s_nationkey#85L, n_nationkey#368]], [[ps_partkey#127L, ps_suppkey#128L]]\n",
      "      +- Join Inner, (n_regionkey#370L = cast(r_regionkey#394 as bigint))\n",
      "         :- Join Inner, (s_nationkey#85L = cast(n_nationkey#368 as bigint))\n",
      "         :  :- Join Inner, (cast(s_suppkey#82 as bigint) = ps_suppkey#128L)\n",
      "         :  :  :- Join Inner, (cast(p_partkey#24 as bigint) = ps_partkey#127L)\n",
      "         :  :  :  :- Project [p_partkey#24]\n",
      "         :  :  :  :  +- Filter isnotnull(p_partkey#24)\n",
      "         :  :  :  :     +- Relation [p_partkey#24,p_name#25,p_mfgr#26,p_brand#27,p_type#28,p_size#29,p_container#30,p_retailprice#31,p_comment#32] JDBCRelation(part) [numPartitions=1]\n",
      "         :  :  :  +- Project [ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130]\n",
      "         :  :  :     +- Filter (isnotnull(ps_partkey#127L) AND isnotnull(ps_suppkey#128L))\n",
      "         :  :  :        +- Relation [ps_partkey#127L,ps_suppkey#128L,ps_availqty#129,ps_supplycost#130,ps_comment#131] JDBCRelation(partsupp) [numPartitions=1]\n",
      "         :  :  +- Project [s_suppkey#82, s_nationkey#85L]\n",
      "         :  :     +- Filter (isnotnull(s_suppkey#82) AND isnotnull(s_nationkey#85L))\n",
      "         :  :        +- Relation [s_suppkey#82,s_name#83,s_address#84,s_nationkey#85L,s_phone#86,s_acctbal#87,s_comment#88] JDBCRelation(supplier) [numPartitions=1]\n",
      "         :  +- Project [n_nationkey#368, n_regionkey#370L]\n",
      "         :     +- Filter (isnotnull(n_nationkey#368) AND isnotnull(n_regionkey#370L))\n",
      "         :        +- Relation [n_nationkey#368,n_name#369,n_regionkey#370L,n_comment#371] JDBCRelation(nation) [numPartitions=1]\n",
      "         +- Project [r_regionkey#394]\n",
      "            +- Filter isnotnull(r_regionkey#394)\n",
      "               +- Relation [r_regionkey#394,r_name#395,r_comment#396] JDBCRelation(region) [numPartitions=1]\n",
      "\n",
      "23/10/03 12:36:44 WARN RewriteJoinsAsSemijoins: agg(project(join))\n",
      "23/10/03 12:36:44 WARN RewriteJoinsAsSemijoins: items: List(Project [p_partkey#24]\n",
      "+- Filter isnotnull(p_partkey#24)\n",
      "   +- Relation [p_partkey#24,p_name#25,p_mfgr#26,p_brand#27,p_type#28,p_size#29,p_container#30,p_retailprice#31,p_comment#32] JDBCRelation(part) [numPartitions=1]\n",
      ", Project [ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130]\n",
      "+- Filter (isnotnull(ps_partkey#127L) AND isnotnull(ps_suppkey#128L))\n",
      "   +- Relation [ps_partkey#127L,ps_suppkey#128L,ps_availqty#129,ps_supplycost#130,ps_comment#131] JDBCRelation(partsupp) [numPartitions=1]\n",
      ", Project [s_suppkey#82, s_nationkey#85L]\n",
      "+- Filter (isnotnull(s_suppkey#82) AND isnotnull(s_nationkey#85L))\n",
      "   +- Relation [s_suppkey#82,s_name#83,s_address#84,s_nationkey#85L,s_phone#86,s_acctbal#87,s_comment#88] JDBCRelation(supplier) [numPartitions=1]\n",
      ", Project [n_nationkey#368, n_regionkey#370L]\n",
      "+- Filter (isnotnull(n_nationkey#368) AND isnotnull(n_regionkey#370L))\n",
      "   +- Relation [n_nationkey#368,n_name#369,n_regionkey#370L,n_comment#371] JDBCRelation(nation) [numPartitions=1]\n",
      ", Project [r_regionkey#394]\n",
      "+- Filter isnotnull(r_regionkey#394)\n",
      "   +- Relation [r_regionkey#394,r_name#395,r_comment#396] JDBCRelation(region) [numPartitions=1]\n",
      ")\n",
      "23/10/03 12:36:44 WARN RewriteJoinsAsSemijoins: conditions: Set((cast(p_partkey#24 as bigint) = ps_partkey#127L), (cast(s_suppkey#82 as bigint) = ps_suppkey#128L), (s_nationkey#85L = cast(n_nationkey#368 as bigint)), (n_regionkey#370L = cast(r_regionkey#394 as bigint)))\n",
      "23/10/03 12:36:44 WARN RewriteJoinsAsSemijoins: agg: toprettystring(percentile(ps_supplycost#130, 0.5, 1, 0, 0, false), Some(Etc/UTC)) AS toprettystring(median(ps_supplycost))#551\n",
      "23/10/03 12:36:44 WARN RewriteJoinsAsSemijoins: is 0MA: false\n",
      "23/10/03 12:36:44 WARN RewriteJoinsAsSemijoins: is counting: false\n",
      "23/10/03 12:36:44 WARN RewriteJoinsAsSemijoins: is percentile: true\n",
      "23/10/03 12:36:44 WARN RewriteJoinsAsSemijoins: is sum: false\n",
      "23/10/03 12:36:44 WARN RewriteJoinsAsSemijoins: is non-agg: false\n",
      "23/10/03 12:36:44 WARN RewriteJoinsAsSemijoins: aggregate attributes: {ps_supplycost#130}\n",
      "23/10/03 12:36:44 WARN RewriteJoinsAsSemijoins: groupingExpressions: List()\n",
      "23/10/03 12:36:44 WARN RewriteJoinsAsSemijoins: group attributes: {}\n",
      "23/10/03 12:36:44 WARN RewriteJoinsAsSemijoins: counting aggregates: List()\n",
      "23/10/03 12:36:44 WARN Hypergraph: equivalence classes: Set(Set(p_partkey#24, ps_partkey#127L), Set(s_suppkey#82, ps_suppkey#128L), Set(s_nationkey#85L, n_nationkey#368), Set(n_regionkey#370L, r_regionkey#394))\n",
      "23/10/03 12:36:44 WARN Hypergraph: vertex to attribute mapping: Map(s_nationkey#85L -> Set(s_nationkey#85L, n_nationkey#368), p_partkey#24 -> Set(p_partkey#24, ps_partkey#127L), n_regionkey#370L -> Set(n_regionkey#370L, r_regionkey#394), s_suppkey#82 -> Set(s_suppkey#82, ps_suppkey#128L))\n",
      "23/10/03 12:36:44 WARN Hypergraph: attribute to vertex mapping: Map(ExprId(128,c67a52a4-5e1f-4517-95e5-f36fb4a3cc98) -> s_suppkey#82, ExprId(394,c67a52a4-5e1f-4517-95e5-f36fb4a3cc98) -> n_regionkey#370L, ExprId(370,c67a52a4-5e1f-4517-95e5-f36fb4a3cc98) -> n_regionkey#370L, ExprId(82,c67a52a4-5e1f-4517-95e5-f36fb4a3cc98) -> s_suppkey#82, ExprId(127,c67a52a4-5e1f-4517-95e5-f36fb4a3cc98) -> p_partkey#24, ExprId(85,c67a52a4-5e1f-4517-95e5-f36fb4a3cc98) -> s_nationkey#85L, ExprId(368,c67a52a4-5e1f-4517-95e5-f36fb4a3cc98) -> s_nationkey#85L, ExprId(24,c67a52a4-5e1f-4517-95e5-f36fb4a3cc98) -> p_partkey#24)\n",
      "23/10/03 12:36:44 WARN Hypergraph: join item: Project [p_partkey#24]\n",
      "+- Filter isnotnull(p_partkey#24)\n",
      "   +- Relation [p_partkey#24,p_name#25,p_mfgr#26,p_brand#27,p_type#28,p_size#29,p_container#30,p_retailprice#31,p_comment#32] JDBCRelation(part) [numPartitions=1]\n",
      "\n",
      "23/10/03 12:36:44 WARN Hypergraph: join item: Project [ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130]\n",
      "+- Filter (isnotnull(ps_partkey#127L) AND isnotnull(ps_suppkey#128L))\n",
      "   +- Relation [ps_partkey#127L,ps_suppkey#128L,ps_availqty#129,ps_supplycost#130,ps_comment#131] JDBCRelation(partsupp) [numPartitions=1]\n",
      "\n",
      "23/10/03 12:36:44 WARN Hypergraph: join item: Project [s_suppkey#82, s_nationkey#85L]\n",
      "+- Filter (isnotnull(s_suppkey#82) AND isnotnull(s_nationkey#85L))\n",
      "   +- Relation [s_suppkey#82,s_name#83,s_address#84,s_nationkey#85L,s_phone#86,s_acctbal#87,s_comment#88] JDBCRelation(supplier) [numPartitions=1]\n",
      "\n",
      "23/10/03 12:36:44 WARN Hypergraph: join item: Project [n_nationkey#368, n_regionkey#370L]\n",
      "+- Filter (isnotnull(n_nationkey#368) AND isnotnull(n_regionkey#370L))\n",
      "   +- Relation [n_nationkey#368,n_name#369,n_regionkey#370L,n_comment#371] JDBCRelation(nation) [numPartitions=1]\n",
      "\n",
      "23/10/03 12:36:44 WARN Hypergraph: join item: Project [r_regionkey#394]\n",
      "+- Filter isnotnull(r_regionkey#394)\n",
      "   +- Relation [r_regionkey#394,r_name#395,r_comment#396] JDBCRelation(region) [numPartitions=1]\n",
      "\n",
      "23/10/03 12:36:44 WARN Hypergraph: hyperedges: Set(E3(s_suppkey#82, s_nationkey#85L), E4(s_nationkey#85L, n_regionkey#370L), E2(p_partkey#24, s_suppkey#82), E5(n_regionkey#370L), E1(p_partkey#24))\n",
      "23/10/03 12:36:44 WARN Hypergraph: gyo edge: E3(s_suppkey#82, s_nationkey#85L)\n",
      "23/10/03 12:36:44 WARN Hypergraph: supersets: Set()\n",
      "23/10/03 12:36:44 WARN Hypergraph: parentNode: TreeNode(Set(E3(s_suppkey#82, s_nationkey#85L)))[Set({s_suppkey#82, s_nationkey#85L})] [[parent: false]]\n",
      "\n",
      "23/10/03 12:36:44 WARN Hypergraph: subsets: Set()\n",
      "23/10/03 12:36:44 WARN Hypergraph: gyo edge: E4(s_nationkey#85L, n_regionkey#370L)\n",
      "23/10/03 12:36:44 WARN Hypergraph: supersets: Set()\n",
      "23/10/03 12:36:44 WARN Hypergraph: parentNode: TreeNode(Set(E4(s_nationkey#85L, n_regionkey#370L)))[Set({n_nationkey#368, n_regionkey#370L})] [[parent: false]]\n",
      "\n",
      "23/10/03 12:36:44 WARN Hypergraph: subsets: Set(TreeNode(Set(E5(n_regionkey#370L)))[Set({r_regionkey#394})] [[parent: false]]\n",
      ")\n",
      "23/10/03 12:36:44 WARN Hypergraph: gyo edge: E1(p_partkey#24)\n",
      "23/10/03 12:36:44 WARN Hypergraph: supersets: Set(E2(p_partkey#24, s_suppkey#82))\n",
      "23/10/03 12:36:44 WARN Hypergraph: gyo edge: E2(p_partkey#24, s_suppkey#82)\n",
      "23/10/03 12:36:44 WARN Hypergraph: supersets: Set()\n",
      "23/10/03 12:36:44 WARN Hypergraph: parentNode: TreeNode(Set(E2(p_partkey#24, s_suppkey#82)))[Set({ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130})] [[parent: false]]\n",
      "\n",
      "23/10/03 12:36:44 WARN Hypergraph: subsets: Set(TreeNode(Set(E1(p_partkey#24)))[Set({p_partkey#24})] [[parent: false]]\n",
      ")\n",
      "23/10/03 12:36:44 WARN Hypergraph: gyo edge: E4(s_nationkey#85L)\n",
      "23/10/03 12:36:44 WARN Hypergraph: supersets: Set(E3(s_suppkey#82, s_nationkey#85L))\n",
      "23/10/03 12:36:44 WARN Hypergraph: gyo edge: E2(s_suppkey#82)\n",
      "23/10/03 12:36:44 WARN Hypergraph: supersets: Set(E3(s_suppkey#82, s_nationkey#85L))\n",
      "23/10/03 12:36:44 WARN Hypergraph: gyo edge: E3(s_suppkey#82, s_nationkey#85L)\n",
      "23/10/03 12:36:44 WARN Hypergraph: supersets: Set()\n",
      "23/10/03 12:36:44 WARN Hypergraph: parentNode: TreeNode(Set(E3(s_suppkey#82, s_nationkey#85L)))[Set({s_suppkey#82, s_nationkey#85L})] [[parent: false]]\n",
      "\n",
      "23/10/03 12:36:44 WARN Hypergraph: subsets: Set(TreeNode(Set(E4(s_nationkey#85L, n_regionkey#370L)))[Set({n_nationkey#368, n_regionkey#370L})] [[parent: false]]\n",
      "-- TreeNode(Set(E5(n_regionkey#370L)))[Set({r_regionkey#394})] [[parent: true]]\n",
      ", TreeNode(Set(E2(p_partkey#24, s_suppkey#82)))[Set({ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130})] [[parent: false]]\n",
      "-- TreeNode(Set(E1(p_partkey#24)))[Set({p_partkey#24})] [[parent: true]]\n",
      ")\n",
      "23/10/03 12:36:44 WARN RewriteJoinsAsSemijoins: join tree: \n",
      "TreeNode(Set(E3(s_suppkey#82, s_nationkey#85L)))[Set({s_suppkey#82, s_nationkey#85L})] [[parent: false]]\n",
      "-- TreeNode(Set(E4(s_nationkey#85L, n_regionkey#370L)))[Set({n_nationkey#368, n_regionkey#370L})] [[parent: true]]\n",
      "-- -- TreeNode(Set(E5(n_regionkey#370L)))[Set({r_regionkey#394})] [[parent: true]]\n",
      "\n",
      "-- TreeNode(Set(E2(p_partkey#24, s_suppkey#82)))[Set({ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130})] [[parent: true]]\n",
      "-- -- TreeNode(Set(E1(p_partkey#24)))[Set({p_partkey#24})] [[parent: true]]\n",
      "\n",
      "23/10/03 12:36:44 WARN HTNode: aggAttributes: {ps_supplycost#130}\n",
      "23/10/03 12:36:44 WARN HTNode: nodeAttributes: {s_suppkey#82, s_nationkey#85L}\n",
      "23/10/03 12:36:44 WARN HTNode: aggAttributes: {ps_supplycost#130}\n",
      "23/10/03 12:36:44 WARN HTNode: nodeAttributes: {n_nationkey#368, n_regionkey#370L}\n",
      "23/10/03 12:36:44 WARN HTNode: aggAttributes: {ps_supplycost#130}\n",
      "23/10/03 12:36:44 WARN HTNode: nodeAttributes: {r_regionkey#394}\n",
      "23/10/03 12:36:44 WARN HTNode: aggAttributes: {ps_supplycost#130}\n",
      "23/10/03 12:36:44 WARN HTNode: nodeAttributes: {ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130}\n",
      "23/10/03 12:36:44 WARN HTNode: found subset in:\n",
      "TreeNode(Set(E2(p_partkey#24, s_suppkey#82)))[Set({ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130})] [[parent: true]]\n",
      "-- TreeNode(Set(E1(p_partkey#24)))[Set({p_partkey#24})] [[parent: true]]\n",
      "\n",
      "23/10/03 12:36:44 WARN HTNode: parent: TreeNode(Set(E3(s_suppkey#82, s_nationkey#85L)))[Set({s_suppkey#82, s_nationkey#85L})] [[parent: false]]\n",
      "-- TreeNode(Set(E4(s_nationkey#85L, n_regionkey#370L)))[Set({n_nationkey#368, n_regionkey#370L})] [[parent: true]]\n",
      "-- -- TreeNode(Set(E5(n_regionkey#370L)))[Set({r_regionkey#394})] [[parent: true]]\n",
      "\n",
      "-- TreeNode(Set(E2(p_partkey#24, s_suppkey#82)))[Set({ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130})] [[parent: true]]\n",
      "-- -- TreeNode(Set(E1(p_partkey#24)))[Set({p_partkey#24})] [[parent: true]]\n",
      "\n",
      "23/10/03 12:36:44 WARN HTNode: p: TreeNode(Set(E3(s_suppkey#82, s_nationkey#85L)))[Set({s_suppkey#82, s_nationkey#85L})] [[parent: false]]\n",
      "-- TreeNode(Set(E4(s_nationkey#85L, n_regionkey#370L)))[Set({n_nationkey#368, n_regionkey#370L})] [[parent: true]]\n",
      "-- -- TreeNode(Set(E5(n_regionkey#370L)))[Set({r_regionkey#394})] [[parent: true]]\n",
      "\n",
      "-- TreeNode(Set(E2(p_partkey#24, s_suppkey#82)))[Set({ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130})] [[parent: true]]\n",
      "-- -- TreeNode(Set(E1(p_partkey#24)))[Set({p_partkey#24})] [[parent: true]]\n",
      "\n",
      "23/10/03 12:36:44 WARN HTNode: new child: TreeNode(Set(E3(s_suppkey#82, s_nationkey#85L)))[Set({s_suppkey#82, s_nationkey#85L})] [[parent: false]]\n",
      "-- TreeNode(Set(E4(s_nationkey#85L, n_regionkey#370L)))[Set({n_nationkey#368, n_regionkey#370L})] [[parent: true]]\n",
      "-- -- TreeNode(Set(E5(n_regionkey#370L)))[Set({r_regionkey#394})] [[parent: true]]\n",
      "\n",
      "23/10/03 12:36:44 WARN HTNode: c: TreeNode(Set(E2(p_partkey#24, s_suppkey#82)))[Set({ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130})] [[parent: true]]\n",
      "-- TreeNode(Set(E1(p_partkey#24)))[Set({p_partkey#24})] [[parent: true]]\n",
      "\n",
      "23/10/03 12:36:44 WARN RewriteJoinsAsSemijoins: rerooted: \n",
      "TreeNode(Set(E2(p_partkey#24, s_suppkey#82)))[Set({ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130})] [[parent: false]]\n",
      "-- TreeNode(Set(E1(p_partkey#24)))[Set({p_partkey#24})] [[parent: true]]\n",
      "\n",
      "-- TreeNode(Set(E3(s_suppkey#82, s_nationkey#85L)))[Set({s_suppkey#82, s_nationkey#85L})] [[parent: true]]\n",
      "-- -- TreeNode(Set(E4(s_nationkey#85L, n_regionkey#370L)))[Set({n_nationkey#368, n_regionkey#370L})] [[parent: true]]\n",
      "-- -- -- TreeNode(Set(E5(n_regionkey#370L)))[Set({r_regionkey#394})] [[parent: true]]\n",
      "\n",
      "23/10/03 12:36:44 WARN RewriteJoinsAsSemijoins: percentile aggregates: List(toprettystring(percentile(ps_supplycost#130, 0.5, 1, 0, 0, false), Some(Etc/UTC)) AS toprettystring(median(ps_supplycost))#551)\n",
      "23/10/03 12:36:44 WARN HTNode: keyRefs: List(List(ps_partkey#127L, p_partkey#24), List(ps_suppkey#128L, s_suppkey#82), List(n_regionkey#370L, r_regionkey#394), List(s_nationkey#85L, n_nationkey#368))\n",
      "23/10/03 12:36:44 WARN HTNode: edge: E2(p_partkey#24, s_suppkey#82)\n",
      "23/10/03 12:36:44 WARN HTNode: overlapping vertices: Set((ps_partkey#127L,p_partkey#24))\n",
      "23/10/03 12:36:44 WARN HTNode: can semi join: true\n",
      "23/10/03 12:36:44 WARN HTNode: join output: List(ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130, c#553)\n",
      "23/10/03 12:36:44 WARN HTNode: keyRefs: List(List(ps_partkey#127L, p_partkey#24), List(ps_suppkey#128L, s_suppkey#82), List(n_regionkey#370L, r_regionkey#394), List(s_nationkey#85L, n_nationkey#368))\n",
      "23/10/03 12:36:44 WARN HTNode: edge: E4(s_nationkey#85L, n_regionkey#370L)\n",
      "23/10/03 12:36:44 WARN HTNode: overlapping vertices: Set((n_regionkey#370L,r_regionkey#394))\n",
      "23/10/03 12:36:44 WARN HTNode: can semi join: true\n",
      "23/10/03 12:36:44 WARN HTNode: join output: List(n_nationkey#368, n_regionkey#370L, c#560)\n",
      "23/10/03 12:36:44 WARN HTNode: keyRefs: List(List(ps_partkey#127L, p_partkey#24), List(ps_suppkey#128L, s_suppkey#82), List(n_regionkey#370L, r_regionkey#394), List(s_nationkey#85L, n_nationkey#368))\n",
      "23/10/03 12:36:44 WARN HTNode: edge: E3(s_suppkey#82, s_nationkey#85L)\n",
      "23/10/03 12:36:44 WARN HTNode: overlapping vertices: Set((s_nationkey#85L,n_nationkey#368))\n",
      "23/10/03 12:36:44 WARN HTNode: can semi join: true\n",
      "23/10/03 12:36:44 WARN HTNode: join output: List(s_suppkey#82, s_nationkey#85L, c#559)\n",
      "23/10/03 12:36:44 WARN HTNode: keyRefs: List(List(ps_partkey#127L, p_partkey#24), List(ps_suppkey#128L, s_suppkey#82), List(n_regionkey#370L, r_regionkey#394), List(s_nationkey#85L, n_nationkey#368))\n",
      "23/10/03 12:36:44 WARN HTNode: edge: E2(p_partkey#24, s_suppkey#82)\n",
      "23/10/03 12:36:44 WARN HTNode: overlapping vertices: Set((ps_suppkey#128L,s_suppkey#82))\n",
      "23/10/03 12:36:44 WARN HTNode: can semi join: true\n",
      "23/10/03 12:36:44 WARN HTNode: join output: List(ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130, c#553)\n",
      "23/10/03 12:36:44 WARN RewriteJoinsAsSemijoins: new aggregate: Aggregate [toprettystring(percentile(ps_supplycost#130, 0.5, 1, 0, 0, false), Some(Etc/UTC)) AS toprettystring(median(ps_supplycost))#551]\n",
      "+- Project [ps_supplycost#130, c#553]\n",
      "   +- Project [ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130, c#553]\n",
      "      +- Join LeftSemi, (ps_suppkey#128L = cast(s_suppkey#82 as bigint))\n",
      "         :- Project [ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130, c#553]\n",
      "         :  +- Join LeftSemi, (ps_partkey#127L = cast(p_partkey#24 as bigint))\n",
      "         :     :- Project [ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130, 1 AS c#553]\n",
      "         :     :  +- Project [ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130]\n",
      "         :     :     +- Filter (isnotnull(ps_partkey#127L) AND isnotnull(ps_suppkey#128L))\n",
      "         :     :        +- Relation [ps_partkey#127L,ps_suppkey#128L,ps_availqty#129,ps_supplycost#130,ps_comment#131] JDBCRelation(partsupp) [numPartitions=1]\n",
      "         :     +- Project [p_partkey#24, 1 AS c#554]\n",
      "         :        +- Project [p_partkey#24]\n",
      "         :           +- Filter isnotnull(p_partkey#24)\n",
      "         :              +- Relation [p_partkey#24,p_name#25,p_mfgr#26,p_brand#27,p_type#28,p_size#29,p_container#30,p_retailprice#31,p_comment#32] JDBCRelation(part) [numPartitions=1]\n",
      "         +- Project [s_suppkey#82, s_nationkey#85L, c#559]\n",
      "            +- Join LeftSemi, (s_nationkey#85L = cast(n_nationkey#368 as bigint))\n",
      "               :- Project [s_suppkey#82, s_nationkey#85L, 1 AS c#559]\n",
      "               :  +- Project [s_suppkey#82, s_nationkey#85L]\n",
      "               :     +- Filter (isnotnull(s_suppkey#82) AND isnotnull(s_nationkey#85L))\n",
      "               :        +- Relation [s_suppkey#82,s_name#83,s_address#84,s_nationkey#85L,s_phone#86,s_acctbal#87,s_comment#88] JDBCRelation(supplier) [numPartitions=1]\n",
      "               +- Project [n_nationkey#368, n_regionkey#370L, c#560]\n",
      "                  +- Join LeftSemi, (n_regionkey#370L = cast(r_regionkey#394 as bigint))\n",
      "                     :- Project [n_nationkey#368, n_regionkey#370L, 1 AS c#560]\n",
      "                     :  +- Project [n_nationkey#368, n_regionkey#370L]\n",
      "                     :     +- Filter (isnotnull(n_nationkey#368) AND isnotnull(n_regionkey#370L))\n",
      "                     :        +- Relation [n_nationkey#368,n_name#369,n_regionkey#370L,n_comment#371] JDBCRelation(nation) [numPartitions=1]\n",
      "                     +- Project [r_regionkey#394, 1 AS c#561]\n",
      "                        +- Project [r_regionkey#394]\n",
      "                           +- Filter isnotnull(r_regionkey#394)\n",
      "                              +- Relation [r_regionkey#394,r_name#395,r_comment#396] JDBCRelation(region) [numPartitions=1]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/03 12:37:04 WARN RewriteJoinsAsSemijoins: applying yannakakis rewriting to join: Aggregate [percentile(ps_supplycost#130, 0.5, 1, 0, 0, false) AS median(ps_supplycost)#548]\n",
      "+- Project [ps_supplycost#130]\n",
      "   +- FKHint [[ps_partkey#127L, p_partkey#24], [ps_suppkey#128L, s_suppkey#82], [n_regionkey#370L, r_regionkey#394], [s_nationkey#85L, n_nationkey#368]], [[ps_partkey#127L, ps_suppkey#128L]]\n",
      "      +- Join Inner, (n_regionkey#370L = cast(r_regionkey#394 as bigint))\n",
      "         :- Join Inner, (s_nationkey#85L = cast(n_nationkey#368 as bigint))\n",
      "         :  :- Join Inner, (cast(s_suppkey#82 as bigint) = ps_suppkey#128L)\n",
      "         :  :  :- Join Inner, (cast(p_partkey#24 as bigint) = ps_partkey#127L)\n",
      "         :  :  :  :- Project [p_partkey#24]\n",
      "         :  :  :  :  +- Filter isnotnull(p_partkey#24)\n",
      "         :  :  :  :     +- Relation [p_partkey#24,p_name#25,p_mfgr#26,p_brand#27,p_type#28,p_size#29,p_container#30,p_retailprice#31,p_comment#32] JDBCRelation(part) [numPartitions=1]\n",
      "         :  :  :  +- Project [ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130]\n",
      "         :  :  :     +- Filter (isnotnull(ps_partkey#127L) AND isnotnull(ps_suppkey#128L))\n",
      "         :  :  :        +- Relation [ps_partkey#127L,ps_suppkey#128L,ps_availqty#129,ps_supplycost#130,ps_comment#131] JDBCRelation(partsupp) [numPartitions=1]\n",
      "         :  :  +- Project [s_suppkey#82, s_nationkey#85L]\n",
      "         :  :     +- Filter (isnotnull(s_suppkey#82) AND isnotnull(s_nationkey#85L))\n",
      "         :  :        +- Relation [s_suppkey#82,s_name#83,s_address#84,s_nationkey#85L,s_phone#86,s_acctbal#87,s_comment#88] JDBCRelation(supplier) [numPartitions=1]\n",
      "         :  +- Project [n_nationkey#368, n_regionkey#370L]\n",
      "         :     +- Filter (isnotnull(n_nationkey#368) AND isnotnull(n_regionkey#370L))\n",
      "         :        +- Relation [n_nationkey#368,n_name#369,n_regionkey#370L,n_comment#371] JDBCRelation(nation) [numPartitions=1]\n",
      "         +- Project [r_regionkey#394]\n",
      "            +- Filter isnotnull(r_regionkey#394)\n",
      "               +- Relation [r_regionkey#394,r_name#395,r_comment#396] JDBCRelation(region) [numPartitions=1]\n",
      "\n",
      "23/10/03 12:37:04 WARN RewriteJoinsAsSemijoins: agg(project(join))\n",
      "23/10/03 12:37:04 WARN RewriteJoinsAsSemijoins: items: List(Project [p_partkey#24]\n",
      "+- Filter isnotnull(p_partkey#24)\n",
      "   +- Relation [p_partkey#24,p_name#25,p_mfgr#26,p_brand#27,p_type#28,p_size#29,p_container#30,p_retailprice#31,p_comment#32] JDBCRelation(part) [numPartitions=1]\n",
      ", Project [ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130]\n",
      "+- Filter (isnotnull(ps_partkey#127L) AND isnotnull(ps_suppkey#128L))\n",
      "   +- Relation [ps_partkey#127L,ps_suppkey#128L,ps_availqty#129,ps_supplycost#130,ps_comment#131] JDBCRelation(partsupp) [numPartitions=1]\n",
      ", Project [s_suppkey#82, s_nationkey#85L]\n",
      "+- Filter (isnotnull(s_suppkey#82) AND isnotnull(s_nationkey#85L))\n",
      "   +- Relation [s_suppkey#82,s_name#83,s_address#84,s_nationkey#85L,s_phone#86,s_acctbal#87,s_comment#88] JDBCRelation(supplier) [numPartitions=1]\n",
      ", Project [n_nationkey#368, n_regionkey#370L]\n",
      "+- Filter (isnotnull(n_nationkey#368) AND isnotnull(n_regionkey#370L))\n",
      "   +- Relation [n_nationkey#368,n_name#369,n_regionkey#370L,n_comment#371] JDBCRelation(nation) [numPartitions=1]\n",
      ", Project [r_regionkey#394]\n",
      "+- Filter isnotnull(r_regionkey#394)\n",
      "   +- Relation [r_regionkey#394,r_name#395,r_comment#396] JDBCRelation(region) [numPartitions=1]\n",
      ")\n",
      "23/10/03 12:37:04 WARN RewriteJoinsAsSemijoins: conditions: Set((cast(p_partkey#24 as bigint) = ps_partkey#127L), (cast(s_suppkey#82 as bigint) = ps_suppkey#128L), (s_nationkey#85L = cast(n_nationkey#368 as bigint)), (n_regionkey#370L = cast(r_regionkey#394 as bigint)))\n",
      "23/10/03 12:37:04 WARN RewriteJoinsAsSemijoins: agg: percentile(ps_supplycost#130, 0.5, 1, 0, 0, false) AS median(ps_supplycost)#548\n",
      "23/10/03 12:37:04 WARN RewriteJoinsAsSemijoins: is 0MA: false\n",
      "23/10/03 12:37:04 WARN RewriteJoinsAsSemijoins: is counting: false\n",
      "23/10/03 12:37:04 WARN RewriteJoinsAsSemijoins: is percentile: true\n",
      "23/10/03 12:37:04 WARN RewriteJoinsAsSemijoins: is sum: false\n",
      "23/10/03 12:37:04 WARN RewriteJoinsAsSemijoins: is non-agg: false\n",
      "23/10/03 12:37:04 WARN RewriteJoinsAsSemijoins: aggregate attributes: {ps_supplycost#130}\n",
      "23/10/03 12:37:04 WARN RewriteJoinsAsSemijoins: groupingExpressions: List()\n",
      "23/10/03 12:37:04 WARN RewriteJoinsAsSemijoins: group attributes: {}\n",
      "23/10/03 12:37:04 WARN RewriteJoinsAsSemijoins: counting aggregates: List()\n",
      "23/10/03 12:37:04 WARN Hypergraph: equivalence classes: Set(Set(p_partkey#24, ps_partkey#127L), Set(s_suppkey#82, ps_suppkey#128L), Set(s_nationkey#85L, n_nationkey#368), Set(n_regionkey#370L, r_regionkey#394))\n",
      "23/10/03 12:37:04 WARN Hypergraph: vertex to attribute mapping: Map(s_nationkey#85L -> Set(s_nationkey#85L, n_nationkey#368), p_partkey#24 -> Set(p_partkey#24, ps_partkey#127L), n_regionkey#370L -> Set(n_regionkey#370L, r_regionkey#394), s_suppkey#82 -> Set(s_suppkey#82, ps_suppkey#128L))\n",
      "23/10/03 12:37:04 WARN Hypergraph: attribute to vertex mapping: Map(ExprId(128,c67a52a4-5e1f-4517-95e5-f36fb4a3cc98) -> s_suppkey#82, ExprId(394,c67a52a4-5e1f-4517-95e5-f36fb4a3cc98) -> n_regionkey#370L, ExprId(370,c67a52a4-5e1f-4517-95e5-f36fb4a3cc98) -> n_regionkey#370L, ExprId(82,c67a52a4-5e1f-4517-95e5-f36fb4a3cc98) -> s_suppkey#82, ExprId(127,c67a52a4-5e1f-4517-95e5-f36fb4a3cc98) -> p_partkey#24, ExprId(85,c67a52a4-5e1f-4517-95e5-f36fb4a3cc98) -> s_nationkey#85L, ExprId(368,c67a52a4-5e1f-4517-95e5-f36fb4a3cc98) -> s_nationkey#85L, ExprId(24,c67a52a4-5e1f-4517-95e5-f36fb4a3cc98) -> p_partkey#24)\n",
      "23/10/03 12:37:04 WARN Hypergraph: join item: Project [p_partkey#24]\n",
      "+- Filter isnotnull(p_partkey#24)\n",
      "   +- Relation [p_partkey#24,p_name#25,p_mfgr#26,p_brand#27,p_type#28,p_size#29,p_container#30,p_retailprice#31,p_comment#32] JDBCRelation(part) [numPartitions=1]\n",
      "\n",
      "23/10/03 12:37:04 WARN Hypergraph: join item: Project [ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130]\n",
      "+- Filter (isnotnull(ps_partkey#127L) AND isnotnull(ps_suppkey#128L))\n",
      "   +- Relation [ps_partkey#127L,ps_suppkey#128L,ps_availqty#129,ps_supplycost#130,ps_comment#131] JDBCRelation(partsupp) [numPartitions=1]\n",
      "\n",
      "23/10/03 12:37:04 WARN Hypergraph: join item: Project [s_suppkey#82, s_nationkey#85L]\n",
      "+- Filter (isnotnull(s_suppkey#82) AND isnotnull(s_nationkey#85L))\n",
      "   +- Relation [s_suppkey#82,s_name#83,s_address#84,s_nationkey#85L,s_phone#86,s_acctbal#87,s_comment#88] JDBCRelation(supplier) [numPartitions=1]\n",
      "\n",
      "23/10/03 12:37:04 WARN Hypergraph: join item: Project [n_nationkey#368, n_regionkey#370L]\n",
      "+- Filter (isnotnull(n_nationkey#368) AND isnotnull(n_regionkey#370L))\n",
      "   +- Relation [n_nationkey#368,n_name#369,n_regionkey#370L,n_comment#371] JDBCRelation(nation) [numPartitions=1]\n",
      "\n",
      "23/10/03 12:37:04 WARN Hypergraph: join item: Project [r_regionkey#394]\n",
      "+- Filter isnotnull(r_regionkey#394)\n",
      "   +- Relation [r_regionkey#394,r_name#395,r_comment#396] JDBCRelation(region) [numPartitions=1]\n",
      "\n",
      "23/10/03 12:37:04 WARN Hypergraph: hyperedges: Set(E2(p_partkey#24, s_suppkey#82), E1(p_partkey#24), E4(s_nationkey#85L, n_regionkey#370L), E3(s_suppkey#82, s_nationkey#85L), E5(n_regionkey#370L))\n",
      "23/10/03 12:37:04 WARN Hypergraph: gyo edge: E2(p_partkey#24, s_suppkey#82)\n",
      "23/10/03 12:37:04 WARN Hypergraph: supersets: Set()\n",
      "23/10/03 12:37:04 WARN Hypergraph: parentNode: TreeNode(Set(E2(p_partkey#24, s_suppkey#82)))[Set({ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130})] [[parent: false]]\n",
      "\n",
      "23/10/03 12:37:04 WARN Hypergraph: subsets: Set(TreeNode(Set(E1(p_partkey#24)))[Set({p_partkey#24})] [[parent: false]]\n",
      ")\n",
      "23/10/03 12:37:04 WARN Hypergraph: gyo edge: E5(n_regionkey#370L)\n",
      "23/10/03 12:37:04 WARN Hypergraph: supersets: Set(E4(s_nationkey#85L, n_regionkey#370L))\n",
      "23/10/03 12:37:04 WARN Hypergraph: gyo edge: E4(s_nationkey#85L, n_regionkey#370L)\n",
      "23/10/03 12:37:04 WARN Hypergraph: supersets: Set()\n",
      "23/10/03 12:37:04 WARN Hypergraph: parentNode: TreeNode(Set(E4(s_nationkey#85L, n_regionkey#370L)))[Set({n_nationkey#368, n_regionkey#370L})] [[parent: false]]\n",
      "\n",
      "23/10/03 12:37:04 WARN Hypergraph: subsets: Set(TreeNode(Set(E5(n_regionkey#370L)))[Set({r_regionkey#394})] [[parent: false]]\n",
      ")\n",
      "23/10/03 12:37:04 WARN Hypergraph: gyo edge: E3(s_suppkey#82, s_nationkey#85L)\n",
      "23/10/03 12:37:04 WARN Hypergraph: supersets: Set()\n",
      "23/10/03 12:37:04 WARN Hypergraph: parentNode: TreeNode(Set(E3(s_suppkey#82, s_nationkey#85L)))[Set({s_suppkey#82, s_nationkey#85L})] [[parent: false]]\n",
      "\n",
      "23/10/03 12:37:04 WARN Hypergraph: subsets: Set()\n",
      "23/10/03 12:37:04 WARN Hypergraph: gyo edge: E4(s_nationkey#85L)\n",
      "23/10/03 12:37:04 WARN Hypergraph: supersets: Set(E3(s_suppkey#82, s_nationkey#85L))\n",
      "23/10/03 12:37:04 WARN Hypergraph: gyo edge: E2(s_suppkey#82)\n",
      "23/10/03 12:37:04 WARN Hypergraph: supersets: Set(E3(s_suppkey#82, s_nationkey#85L))\n",
      "23/10/03 12:37:04 WARN Hypergraph: gyo edge: E3(s_suppkey#82, s_nationkey#85L)\n",
      "23/10/03 12:37:04 WARN Hypergraph: supersets: Set()\n",
      "23/10/03 12:37:04 WARN Hypergraph: parentNode: TreeNode(Set(E3(s_suppkey#82, s_nationkey#85L)))[Set({s_suppkey#82, s_nationkey#85L})] [[parent: false]]\n",
      "\n",
      "23/10/03 12:37:04 WARN Hypergraph: subsets: Set(TreeNode(Set(E4(s_nationkey#85L, n_regionkey#370L)))[Set({n_nationkey#368, n_regionkey#370L})] [[parent: false]]\n",
      "-- TreeNode(Set(E5(n_regionkey#370L)))[Set({r_regionkey#394})] [[parent: true]]\n",
      ", TreeNode(Set(E2(p_partkey#24, s_suppkey#82)))[Set({ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130})] [[parent: false]]\n",
      "-- TreeNode(Set(E1(p_partkey#24)))[Set({p_partkey#24})] [[parent: true]]\n",
      ")\n",
      "23/10/03 12:37:04 WARN RewriteJoinsAsSemijoins: join tree: \n",
      "TreeNode(Set(E3(s_suppkey#82, s_nationkey#85L)))[Set({s_suppkey#82, s_nationkey#85L})] [[parent: false]]\n",
      "-- TreeNode(Set(E4(s_nationkey#85L, n_regionkey#370L)))[Set({n_nationkey#368, n_regionkey#370L})] [[parent: true]]\n",
      "-- -- TreeNode(Set(E5(n_regionkey#370L)))[Set({r_regionkey#394})] [[parent: true]]\n",
      "\n",
      "-- TreeNode(Set(E2(p_partkey#24, s_suppkey#82)))[Set({ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130})] [[parent: true]]\n",
      "-- -- TreeNode(Set(E1(p_partkey#24)))[Set({p_partkey#24})] [[parent: true]]\n",
      "\n",
      "23/10/03 12:37:04 WARN HTNode: aggAttributes: {ps_supplycost#130}\n",
      "23/10/03 12:37:04 WARN HTNode: nodeAttributes: {s_suppkey#82, s_nationkey#85L}\n",
      "23/10/03 12:37:04 WARN HTNode: aggAttributes: {ps_supplycost#130}\n",
      "23/10/03 12:37:04 WARN HTNode: nodeAttributes: {n_nationkey#368, n_regionkey#370L}\n",
      "23/10/03 12:37:04 WARN HTNode: aggAttributes: {ps_supplycost#130}\n",
      "23/10/03 12:37:04 WARN HTNode: nodeAttributes: {r_regionkey#394}\n",
      "23/10/03 12:37:04 WARN HTNode: aggAttributes: {ps_supplycost#130}\n",
      "23/10/03 12:37:04 WARN HTNode: nodeAttributes: {ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130}\n",
      "23/10/03 12:37:04 WARN HTNode: found subset in:\n",
      "TreeNode(Set(E2(p_partkey#24, s_suppkey#82)))[Set({ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130})] [[parent: true]]\n",
      "-- TreeNode(Set(E1(p_partkey#24)))[Set({p_partkey#24})] [[parent: true]]\n",
      "\n",
      "23/10/03 12:37:04 WARN HTNode: parent: TreeNode(Set(E3(s_suppkey#82, s_nationkey#85L)))[Set({s_suppkey#82, s_nationkey#85L})] [[parent: false]]\n",
      "-- TreeNode(Set(E4(s_nationkey#85L, n_regionkey#370L)))[Set({n_nationkey#368, n_regionkey#370L})] [[parent: true]]\n",
      "-- -- TreeNode(Set(E5(n_regionkey#370L)))[Set({r_regionkey#394})] [[parent: true]]\n",
      "\n",
      "-- TreeNode(Set(E2(p_partkey#24, s_suppkey#82)))[Set({ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130})] [[parent: true]]\n",
      "-- -- TreeNode(Set(E1(p_partkey#24)))[Set({p_partkey#24})] [[parent: true]]\n",
      "\n",
      "23/10/03 12:37:04 WARN HTNode: p: TreeNode(Set(E3(s_suppkey#82, s_nationkey#85L)))[Set({s_suppkey#82, s_nationkey#85L})] [[parent: false]]\n",
      "-- TreeNode(Set(E4(s_nationkey#85L, n_regionkey#370L)))[Set({n_nationkey#368, n_regionkey#370L})] [[parent: true]]\n",
      "-- -- TreeNode(Set(E5(n_regionkey#370L)))[Set({r_regionkey#394})] [[parent: true]]\n",
      "\n",
      "-- TreeNode(Set(E2(p_partkey#24, s_suppkey#82)))[Set({ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130})] [[parent: true]]\n",
      "-- -- TreeNode(Set(E1(p_partkey#24)))[Set({p_partkey#24})] [[parent: true]]\n",
      "\n",
      "23/10/03 12:37:04 WARN HTNode: new child: TreeNode(Set(E3(s_suppkey#82, s_nationkey#85L)))[Set({s_suppkey#82, s_nationkey#85L})] [[parent: false]]\n",
      "-- TreeNode(Set(E4(s_nationkey#85L, n_regionkey#370L)))[Set({n_nationkey#368, n_regionkey#370L})] [[parent: true]]\n",
      "-- -- TreeNode(Set(E5(n_regionkey#370L)))[Set({r_regionkey#394})] [[parent: true]]\n",
      "\n",
      "23/10/03 12:37:04 WARN HTNode: c: TreeNode(Set(E2(p_partkey#24, s_suppkey#82)))[Set({ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130})] [[parent: true]]\n",
      "-- TreeNode(Set(E1(p_partkey#24)))[Set({p_partkey#24})] [[parent: true]]\n",
      "\n",
      "23/10/03 12:37:04 WARN RewriteJoinsAsSemijoins: rerooted: \n",
      "TreeNode(Set(E2(p_partkey#24, s_suppkey#82)))[Set({ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130})] [[parent: false]]\n",
      "-- TreeNode(Set(E1(p_partkey#24)))[Set({p_partkey#24})] [[parent: true]]\n",
      "\n",
      "-- TreeNode(Set(E3(s_suppkey#82, s_nationkey#85L)))[Set({s_suppkey#82, s_nationkey#85L})] [[parent: true]]\n",
      "-- -- TreeNode(Set(E4(s_nationkey#85L, n_regionkey#370L)))[Set({n_nationkey#368, n_regionkey#370L})] [[parent: true]]\n",
      "-- -- -- TreeNode(Set(E5(n_regionkey#370L)))[Set({r_regionkey#394})] [[parent: true]]\n",
      "\n",
      "23/10/03 12:37:04 WARN RewriteJoinsAsSemijoins: percentile aggregates: List(percentile(ps_supplycost#130, 0.5, 1, 0, 0, false) AS median(ps_supplycost)#548)\n",
      "23/10/03 12:37:04 WARN HTNode: keyRefs: List(List(ps_partkey#127L, p_partkey#24), List(ps_suppkey#128L, s_suppkey#82), List(n_regionkey#370L, r_regionkey#394), List(s_nationkey#85L, n_nationkey#368))\n",
      "23/10/03 12:37:04 WARN HTNode: edge: E2(p_partkey#24, s_suppkey#82)\n",
      "23/10/03 12:37:04 WARN HTNode: overlapping vertices: Set((ps_partkey#127L,p_partkey#24))\n",
      "23/10/03 12:37:04 WARN HTNode: can semi join: true\n",
      "23/10/03 12:37:04 WARN HTNode: join output: List(ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130, c#687)\n",
      "23/10/03 12:37:04 WARN HTNode: keyRefs: List(List(ps_partkey#127L, p_partkey#24), List(ps_suppkey#128L, s_suppkey#82), List(n_regionkey#370L, r_regionkey#394), List(s_nationkey#85L, n_nationkey#368))\n",
      "23/10/03 12:37:04 WARN HTNode: edge: E4(s_nationkey#85L, n_regionkey#370L)\n",
      "23/10/03 12:37:04 WARN HTNode: overlapping vertices: Set((n_regionkey#370L,r_regionkey#394))\n",
      "23/10/03 12:37:04 WARN HTNode: can semi join: true\n",
      "23/10/03 12:37:04 WARN HTNode: join output: List(n_nationkey#368, n_regionkey#370L, c#694)\n",
      "23/10/03 12:37:04 WARN HTNode: keyRefs: List(List(ps_partkey#127L, p_partkey#24), List(ps_suppkey#128L, s_suppkey#82), List(n_regionkey#370L, r_regionkey#394), List(s_nationkey#85L, n_nationkey#368))\n",
      "23/10/03 12:37:04 WARN HTNode: edge: E3(s_suppkey#82, s_nationkey#85L)\n",
      "23/10/03 12:37:04 WARN HTNode: overlapping vertices: Set((s_nationkey#85L,n_nationkey#368))\n",
      "23/10/03 12:37:04 WARN HTNode: can semi join: true\n",
      "23/10/03 12:37:04 WARN HTNode: join output: List(s_suppkey#82, s_nationkey#85L, c#693)\n",
      "23/10/03 12:37:04 WARN HTNode: keyRefs: List(List(ps_partkey#127L, p_partkey#24), List(ps_suppkey#128L, s_suppkey#82), List(n_regionkey#370L, r_regionkey#394), List(s_nationkey#85L, n_nationkey#368))\n",
      "23/10/03 12:37:04 WARN HTNode: edge: E2(p_partkey#24, s_suppkey#82)\n",
      "23/10/03 12:37:04 WARN HTNode: overlapping vertices: Set((ps_suppkey#128L,s_suppkey#82))\n",
      "23/10/03 12:37:04 WARN HTNode: can semi join: true\n",
      "23/10/03 12:37:04 WARN HTNode: join output: List(ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130, c#687)\n",
      "23/10/03 12:37:04 WARN RewriteJoinsAsSemijoins: new aggregate: Aggregate [percentile(ps_supplycost#130, 0.5, 1, 0, 0, false) AS median(ps_supplycost)#548]\n",
      "+- Project [ps_supplycost#130, c#687]\n",
      "   +- Project [ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130, c#687]\n",
      "      +- Join LeftSemi, (ps_suppkey#128L = cast(s_suppkey#82 as bigint))\n",
      "         :- Project [ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130, c#687]\n",
      "         :  +- Join LeftSemi, (ps_partkey#127L = cast(p_partkey#24 as bigint))\n",
      "         :     :- Project [ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130, 1 AS c#687]\n",
      "         :     :  +- Project [ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130]\n",
      "         :     :     +- Filter (isnotnull(ps_partkey#127L) AND isnotnull(ps_suppkey#128L))\n",
      "         :     :        +- Relation [ps_partkey#127L,ps_suppkey#128L,ps_availqty#129,ps_supplycost#130,ps_comment#131] JDBCRelation(partsupp) [numPartitions=1]\n",
      "         :     +- Project [p_partkey#24, 1 AS c#688]\n",
      "         :        +- Project [p_partkey#24]\n",
      "         :           +- Filter isnotnull(p_partkey#24)\n",
      "         :              +- Relation [p_partkey#24,p_name#25,p_mfgr#26,p_brand#27,p_type#28,p_size#29,p_container#30,p_retailprice#31,p_comment#32] JDBCRelation(part) [numPartitions=1]\n",
      "         +- Project [s_suppkey#82, s_nationkey#85L, c#693]\n",
      "            +- Join LeftSemi, (s_nationkey#85L = cast(n_nationkey#368 as bigint))\n",
      "               :- Project [s_suppkey#82, s_nationkey#85L, 1 AS c#693]\n",
      "               :  +- Project [s_suppkey#82, s_nationkey#85L]\n",
      "               :     +- Filter (isnotnull(s_suppkey#82) AND isnotnull(s_nationkey#85L))\n",
      "               :        +- Relation [s_suppkey#82,s_name#83,s_address#84,s_nationkey#85L,s_phone#86,s_acctbal#87,s_comment#88] JDBCRelation(supplier) [numPartitions=1]\n",
      "               +- Project [n_nationkey#368, n_regionkey#370L, c#694]\n",
      "                  +- Join LeftSemi, (n_regionkey#370L = cast(r_regionkey#394 as bigint))\n",
      "                     :- Project [n_nationkey#368, n_regionkey#370L, 1 AS c#694]\n",
      "                     :  +- Project [n_nationkey#368, n_regionkey#370L]\n",
      "                     :     +- Filter (isnotnull(n_nationkey#368) AND isnotnull(n_regionkey#370L))\n",
      "                     :        +- Relation [n_nationkey#368,n_name#369,n_regionkey#370L,n_comment#371] JDBCRelation(nation) [numPartitions=1]\n",
      "                     +- Project [r_regionkey#394, 1 AS c#695]\n",
      "                        +- Project [r_regionkey#394]\n",
      "                           +- Filter isnotnull(r_regionkey#394)\n",
      "                              +- Relation [r_regionkey#394,r_name#395,r_comment#396] JDBCRelation(region) [numPartitions=1]\n",
      "\n",
      "23/10/03 12:37:04 WARN ResolveHints$ResolveForeignKeyHints: cannot not apply foreign key hint if yannakakis is not enabled\n",
      "23/10/03 12:37:04 WARN ResolveHints$ResolveForeignKeyHints: cannot not apply foreign key hint if yannakakis is not enabled\n",
      "23/10/03 12:37:04 WARN ResolveHints$ResolveForeignKeyHints: cannot not apply foreign key hint if yannakakis is not enabled\n",
      "23/10/03 12:37:04 WARN ResolveHints$ResolveForeignKeyHints: cannot not apply foreign key hint if yannakakis is not enabled\n",
      "23/10/03 12:37:04 WARN ResolveHints$ResolveForeignKeyHints: cannot not apply primary key hint if yannakakis is not enabled\n",
      "23/10/03 12:37:04 WARN HintErrorLogger: Unrecognized hint: PK(ps_partkey, ps_suppkey)\n",
      "23/10/03 12:37:04 WARN HintErrorLogger: Unrecognized hint: FK(s_nationkey, n_nationkey)\n",
      "23/10/03 12:37:04 WARN HintErrorLogger: Unrecognized hint: FK(ps_suppkey, s_suppkey)\n",
      "23/10/03 12:37:04 WARN HintErrorLogger: Unrecognized hint: FK(n_regionkey, r_regionkey)\n",
      "23/10/03 12:37:04 WARN HintErrorLogger: Unrecognized hint: FK(ps_partkey, p_partkey)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|median(ps_supplycost)|\n",
      "+---------------------+\n",
      "|               500.47|\n",
      "+---------------------+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'UnresolvedHint FK, ['ps_partkey, 'p_partkey]\n",
      "+- 'UnresolvedHint FK, ['n_regionkey, 'r_regionkey]\n",
      "   +- 'UnresolvedHint FK, ['ps_suppkey, 's_suppkey]\n",
      "      +- 'UnresolvedHint FK, ['s_nationkey, 'n_nationkey]\n",
      "         +- 'UnresolvedHint PK, ['ps_partkey, 'ps_suppkey]\n",
      "            +- 'Project [unresolvedalias('median('ps_supplycost), None)]\n",
      "               +- 'Filter ((('p_partkey = 'ps_partkey) AND ('s_suppkey = 'ps_suppkey)) AND (('s_nationkey = 'n_nationkey) AND ('n_regionkey = 'r_regionkey)))\n",
      "                  +- 'Join Inner\n",
      "                     :- 'Join Inner\n",
      "                     :  :- 'Join Inner\n",
      "                     :  :  :- 'Join Inner\n",
      "                     :  :  :  :- 'UnresolvedRelation [part], [], false\n",
      "                     :  :  :  +- 'UnresolvedRelation [partsupp], [], false\n",
      "                     :  :  +- 'UnresolvedRelation [supplier], [], false\n",
      "                     :  +- 'UnresolvedRelation [nation], [], false\n",
      "                     +- 'UnresolvedRelation [region], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "median(ps_supplycost): double\n",
      "Aggregate [median(ps_supplycost#130) AS median(ps_supplycost)#548]\n",
      "+- FKHint [[ps_partkey#127L, p_partkey#24], [ps_suppkey#128L, s_suppkey#82], [n_regionkey#370L, r_regionkey#394], [s_nationkey#85L, n_nationkey#368]], [[ps_partkey#127L, ps_suppkey#128L]]\n",
      "   +- Filter (((cast(p_partkey#24 as bigint) = ps_partkey#127L) AND (cast(s_suppkey#82 as bigint) = ps_suppkey#128L)) AND ((s_nationkey#85L = cast(n_nationkey#368 as bigint)) AND (n_regionkey#370L = cast(r_regionkey#394 as bigint))))\n",
      "      +- Join Inner\n",
      "         :- Join Inner\n",
      "         :  :- Join Inner\n",
      "         :  :  :- Join Inner\n",
      "         :  :  :  :- SubqueryAlias part\n",
      "         :  :  :  :  +- View (`part`, [p_partkey#24,p_name#25,p_mfgr#33,p_brand#34,p_type#28,p_size#29,p_container#35,p_retailprice#31,p_comment#32])\n",
      "         :  :  :  :     +- Project [p_partkey#24, p_name#25, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_mfgr#26, 25, true, false, true) AS p_mfgr#33, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_brand#27, 10, true, false, true) AS p_brand#34, p_type#28, p_size#29, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_container#30, 10, true, false, true) AS p_container#35, p_retailprice#31, p_comment#32]\n",
      "         :  :  :  :        +- Relation [p_partkey#24,p_name#25,p_mfgr#26,p_brand#27,p_type#28,p_size#29,p_container#30,p_retailprice#31,p_comment#32] JDBCRelation(part) [numPartitions=1]\n",
      "         :  :  :  +- SubqueryAlias partsupp\n",
      "         :  :  :     +- View (`partsupp`, [ps_partkey#127L,ps_suppkey#128L,ps_availqty#129,ps_supplycost#130,ps_comment#131])\n",
      "         :  :  :        +- Relation [ps_partkey#127L,ps_suppkey#128L,ps_availqty#129,ps_supplycost#130,ps_comment#131] JDBCRelation(partsupp) [numPartitions=1]\n",
      "         :  :  +- SubqueryAlias supplier\n",
      "         :  :     +- View (`supplier`, [s_suppkey#82,s_name#89,s_address#84,s_nationkey#85L,s_phone#90,s_acctbal#87,s_comment#88])\n",
      "         :  :        +- Project [s_suppkey#82, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, s_name#83, 25, true, false, true) AS s_name#89, s_address#84, s_nationkey#85L, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, s_phone#86, 15, true, false, true) AS s_phone#90, s_acctbal#87, s_comment#88]\n",
      "         :  :           +- Relation [s_suppkey#82,s_name#83,s_address#84,s_nationkey#85L,s_phone#86,s_acctbal#87,s_comment#88] JDBCRelation(supplier) [numPartitions=1]\n",
      "         :  +- SubqueryAlias nation\n",
      "         :     +- View (`nation`, [n_nationkey#368,n_name#372,n_regionkey#370L,n_comment#371])\n",
      "         :        +- Project [n_nationkey#368, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, n_name#369, 25, true, false, true) AS n_name#372, n_regionkey#370L, n_comment#371]\n",
      "         :           +- Relation [n_nationkey#368,n_name#369,n_regionkey#370L,n_comment#371] JDBCRelation(nation) [numPartitions=1]\n",
      "         +- SubqueryAlias region\n",
      "            +- View (`region`, [r_regionkey#394,r_name#397,r_comment#396])\n",
      "               +- Project [r_regionkey#394, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, r_name#395, 25, true, false, true) AS r_name#397, r_comment#396]\n",
      "                  +- Relation [r_regionkey#394,r_name#395,r_comment#396] JDBCRelation(region) [numPartitions=1]\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [percentile(ps_supplycost#130, 0.5, 1, 0, 0, false) AS median(ps_supplycost)#548]\n",
      "+- Project [ps_supplycost#130]\n",
      "   +- Join LeftSemi, (ps_suppkey#128L = cast(s_suppkey#82 as bigint))\n",
      "      :- Project [ps_suppkey#128L, ps_supplycost#130]\n",
      "      :  +- Join LeftSemi, (ps_partkey#127L = cast(p_partkey#24 as bigint))\n",
      "      :     :- Project [ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130]\n",
      "      :     :  +- Filter (isnotnull(ps_partkey#127L) AND isnotnull(ps_suppkey#128L))\n",
      "      :     :     +- Relation [ps_partkey#127L,ps_suppkey#128L,ps_availqty#129,ps_supplycost#130,ps_comment#131] JDBCRelation(partsupp) [numPartitions=1]\n",
      "      :     +- Project [p_partkey#24]\n",
      "      :        +- Filter isnotnull(p_partkey#24)\n",
      "      :           +- Relation [p_partkey#24,p_name#25,p_mfgr#26,p_brand#27,p_type#28,p_size#29,p_container#30,p_retailprice#31,p_comment#32] JDBCRelation(part) [numPartitions=1]\n",
      "      +- Project [s_suppkey#82]\n",
      "         +- Join LeftSemi, (s_nationkey#85L = cast(n_nationkey#368 as bigint))\n",
      "            :- Project [s_suppkey#82, s_nationkey#85L]\n",
      "            :  +- Filter (isnotnull(s_suppkey#82) AND isnotnull(s_nationkey#85L))\n",
      "            :     +- Relation [s_suppkey#82,s_name#83,s_address#84,s_nationkey#85L,s_phone#86,s_acctbal#87,s_comment#88] JDBCRelation(supplier) [numPartitions=1]\n",
      "            +- Project [n_nationkey#368]\n",
      "               +- Join LeftSemi, (n_regionkey#370L = cast(r_regionkey#394 as bigint))\n",
      "                  :- Project [n_nationkey#368, n_regionkey#370L]\n",
      "                  :  +- Filter (isnotnull(n_nationkey#368) AND isnotnull(n_regionkey#370L))\n",
      "                  :     +- Relation [n_nationkey#368,n_name#369,n_regionkey#370L,n_comment#371] JDBCRelation(nation) [numPartitions=1]\n",
      "                  +- Project [r_regionkey#394]\n",
      "                     +- Filter isnotnull(r_regionkey#394)\n",
      "                        +- Relation [r_regionkey#394,r_name#395,r_comment#396] JDBCRelation(region) [numPartitions=1]\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- ObjectHashAggregate(keys=[], functions=[percentile(ps_supplycost#130, 0.5, 1, 0, 0, false)], output=[median(ps_supplycost)#548])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=1721]\n",
      "      +- ObjectHashAggregate(keys=[], functions=[partial_percentile(ps_supplycost#130, 0.5, 1, 0, 0, false)], output=[buf#709])\n",
      "         +- Project [ps_supplycost#130]\n",
      "            +- SortMergeJoin [ps_suppkey#128L], [cast(s_suppkey#82 as bigint)], LeftSemi\n",
      "               :- Sort [ps_suppkey#128L ASC NULLS FIRST], false, 0\n",
      "               :  +- Exchange hashpartitioning(ps_suppkey#128L, 200), ENSURE_REQUIREMENTS, [plan_id=1713]\n",
      "               :     +- Project [ps_suppkey#128L, ps_supplycost#130]\n",
      "               :        +- SortMergeJoin [ps_partkey#127L], [cast(p_partkey#24 as bigint)], LeftSemi\n",
      "               :           :- Sort [ps_partkey#127L ASC NULLS FIRST], false, 0\n",
      "               :           :  +- Exchange hashpartitioning(ps_partkey#127L, 200), ENSURE_REQUIREMENTS, [plan_id=1690]\n",
      "               :           :     +- Scan JDBCRelation(partsupp) [numPartitions=1] [ps_partkey#127L,ps_suppkey#128L,ps_supplycost#130] PushedFilters: [*IsNotNull(ps_partkey), *IsNotNull(ps_suppkey)], ReadSchema: struct<ps_partkey:bigint,ps_suppkey:bigint,ps_supplycost:decimal(38,18)>\n",
      "               :           +- Sort [cast(p_partkey#24 as bigint) ASC NULLS FIRST], false, 0\n",
      "               :              +- Exchange hashpartitioning(cast(p_partkey#24 as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=1691]\n",
      "               :                 +- Scan JDBCRelation(part) [numPartitions=1] [p_partkey#24] PushedFilters: [*IsNotNull(p_partkey)], ReadSchema: struct<p_partkey:int>\n",
      "               +- Sort [cast(s_suppkey#82 as bigint) ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(cast(s_suppkey#82 as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=1714]\n",
      "                     +- Project [s_suppkey#82]\n",
      "                        +- SortMergeJoin [s_nationkey#85L], [cast(n_nationkey#368 as bigint)], LeftSemi\n",
      "                           :- Sort [s_nationkey#85L ASC NULLS FIRST], false, 0\n",
      "                           :  +- Exchange hashpartitioning(s_nationkey#85L, 200), ENSURE_REQUIREMENTS, [plan_id=1705]\n",
      "                           :     +- Scan JDBCRelation(supplier) [numPartitions=1] [s_suppkey#82,s_nationkey#85L] PushedFilters: [*IsNotNull(s_suppkey), *IsNotNull(s_nationkey)], ReadSchema: struct<s_suppkey:int,s_nationkey:bigint>\n",
      "                           +- Sort [cast(n_nationkey#368 as bigint) ASC NULLS FIRST], false, 0\n",
      "                              +- Exchange hashpartitioning(cast(n_nationkey#368 as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=1706]\n",
      "                                 +- Project [n_nationkey#368]\n",
      "                                    +- SortMergeJoin [n_regionkey#370L], [cast(r_regionkey#394 as bigint)], LeftSemi\n",
      "                                       :- Sort [n_regionkey#370L ASC NULLS FIRST], false, 0\n",
      "                                       :  +- Exchange hashpartitioning(n_regionkey#370L, 200), ENSURE_REQUIREMENTS, [plan_id=1697]\n",
      "                                       :     +- Scan JDBCRelation(nation) [numPartitions=1] [n_nationkey#368,n_regionkey#370L] PushedFilters: [*IsNotNull(n_nationkey), *IsNotNull(n_regionkey)], ReadSchema: struct<n_nationkey:int,n_regionkey:bigint>\n",
      "                                       +- Sort [cast(r_regionkey#394 as bigint) ASC NULLS FIRST], false, 0\n",
      "                                          +- Exchange hashpartitioning(cast(r_regionkey#394 as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=1698]\n",
      "                                             +- Scan JDBCRelation(region) [numPartitions=1] [r_regionkey#394] PushedFilters: [*IsNotNull(r_regionkey)], ReadSchema: struct<r_regionkey:int>\n",
      "\n",
      "+--------------------+-----+\n",
      "|                 key|value|\n",
      "+--------------------+-----+\n",
      "|spark.sql.yannaka...|false|\n",
      "+--------------------+-----+\n",
      "\n",
      "running query: \n",
      "select\n",
      "\n",
      "/*+ FK(ps_partkey, p_partkey), FK(n_regionkey, r_regionkey), FK(ps_suppkey, s_suppkey), FK(s_nationkey, n_nationkey), PK(ps_partkey, ps_suppkey) */\n",
      "\n",
      "median(ps_supplycost)\n",
      "\n",
      "\t\tfrom\n",
      "\n",
      "            part,\n",
      "\n",
      "\t\t\tpartsupp,\n",
      "\n",
      "\t\t\tsupplier,\n",
      "\n",
      "\t\t\tnation,\n",
      "\n",
      "\t\t\tregion\n",
      "\n",
      "\t\twhere\n",
      "\n",
      "\t\t\tp_partkey = ps_partkey\n",
      "\n",
      "\t\t\tand s_suppkey = ps_suppkey\n",
      "\n",
      "\t\t\tand s_nationkey = n_nationkey\n",
      "\n",
      "\t\t\tand n_regionkey = r_regionkey\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 67:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|median(ps_supplycost)|\n",
      "+---------------------+\n",
      "|               500.47|\n",
      "+---------------------+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'UnresolvedHint FK, ['ps_partkey, 'p_partkey]\n",
      "+- 'UnresolvedHint FK, ['n_regionkey, 'r_regionkey]\n",
      "   +- 'UnresolvedHint FK, ['ps_suppkey, 's_suppkey]\n",
      "      +- 'UnresolvedHint FK, ['s_nationkey, 'n_nationkey]\n",
      "         +- 'UnresolvedHint PK, ['ps_partkey, 'ps_suppkey]\n",
      "            +- 'Project [unresolvedalias('median('ps_supplycost), None)]\n",
      "               +- 'Filter ((('p_partkey = 'ps_partkey) AND ('s_suppkey = 'ps_suppkey)) AND (('s_nationkey = 'n_nationkey) AND ('n_regionkey = 'r_regionkey)))\n",
      "                  +- 'Join Inner\n",
      "                     :- 'Join Inner\n",
      "                     :  :- 'Join Inner\n",
      "                     :  :  :- 'Join Inner\n",
      "                     :  :  :  :- 'UnresolvedRelation [part], [], false\n",
      "                     :  :  :  +- 'UnresolvedRelation [partsupp], [], false\n",
      "                     :  :  +- 'UnresolvedRelation [supplier], [], false\n",
      "                     :  +- 'UnresolvedRelation [nation], [], false\n",
      "                     +- 'UnresolvedRelation [region], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "median(ps_supplycost): double\n",
      "Aggregate [median(ps_supplycost#130) AS median(ps_supplycost)#730]\n",
      "+- Filter (((cast(p_partkey#24 as bigint) = ps_partkey#127L) AND (cast(s_suppkey#82 as bigint) = ps_suppkey#128L)) AND ((s_nationkey#85L = cast(n_nationkey#368 as bigint)) AND (n_regionkey#370L = cast(r_regionkey#394 as bigint))))\n",
      "   +- Join Inner\n",
      "      :- Join Inner\n",
      "      :  :- Join Inner\n",
      "      :  :  :- Join Inner\n",
      "      :  :  :  :- SubqueryAlias part\n",
      "      :  :  :  :  +- View (`part`, [p_partkey#24,p_name#25,p_mfgr#33,p_brand#34,p_type#28,p_size#29,p_container#35,p_retailprice#31,p_comment#32])\n",
      "      :  :  :  :     +- Project [p_partkey#24, p_name#25, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_mfgr#26, 25, true, false, true) AS p_mfgr#33, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_brand#27, 10, true, false, true) AS p_brand#34, p_type#28, p_size#29, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, p_container#30, 10, true, false, true) AS p_container#35, p_retailprice#31, p_comment#32]\n",
      "      :  :  :  :        +- Relation [p_partkey#24,p_name#25,p_mfgr#26,p_brand#27,p_type#28,p_size#29,p_container#30,p_retailprice#31,p_comment#32] JDBCRelation(part) [numPartitions=1]\n",
      "      :  :  :  +- SubqueryAlias partsupp\n",
      "      :  :  :     +- View (`partsupp`, [ps_partkey#127L,ps_suppkey#128L,ps_availqty#129,ps_supplycost#130,ps_comment#131])\n",
      "      :  :  :        +- Relation [ps_partkey#127L,ps_suppkey#128L,ps_availqty#129,ps_supplycost#130,ps_comment#131] JDBCRelation(partsupp) [numPartitions=1]\n",
      "      :  :  +- SubqueryAlias supplier\n",
      "      :  :     +- View (`supplier`, [s_suppkey#82,s_name#89,s_address#84,s_nationkey#85L,s_phone#90,s_acctbal#87,s_comment#88])\n",
      "      :  :        +- Project [s_suppkey#82, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, s_name#83, 25, true, false, true) AS s_name#89, s_address#84, s_nationkey#85L, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, s_phone#86, 15, true, false, true) AS s_phone#90, s_acctbal#87, s_comment#88]\n",
      "      :  :           +- Relation [s_suppkey#82,s_name#83,s_address#84,s_nationkey#85L,s_phone#86,s_acctbal#87,s_comment#88] JDBCRelation(supplier) [numPartitions=1]\n",
      "      :  +- SubqueryAlias nation\n",
      "      :     +- View (`nation`, [n_nationkey#368,n_name#372,n_regionkey#370L,n_comment#371])\n",
      "      :        +- Project [n_nationkey#368, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, n_name#369, 25, true, false, true) AS n_name#372, n_regionkey#370L, n_comment#371]\n",
      "      :           +- Relation [n_nationkey#368,n_name#369,n_regionkey#370L,n_comment#371] JDBCRelation(nation) [numPartitions=1]\n",
      "      +- SubqueryAlias region\n",
      "         +- View (`region`, [r_regionkey#394,r_name#397,r_comment#396])\n",
      "            +- Project [r_regionkey#394, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, r_name#395, 25, true, false, true) AS r_name#397, r_comment#396]\n",
      "               +- Relation [r_regionkey#394,r_name#395,r_comment#396] JDBCRelation(region) [numPartitions=1]\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [percentile(ps_supplycost#130, 0.5, 1, 0, 0, false) AS median(ps_supplycost)#730]\n",
      "+- Project [ps_supplycost#130]\n",
      "   +- Join Inner, (n_regionkey#370L = cast(r_regionkey#394 as bigint))\n",
      "      :- Project [ps_supplycost#130, n_regionkey#370L]\n",
      "      :  +- Join Inner, (s_nationkey#85L = cast(n_nationkey#368 as bigint))\n",
      "      :     :- Project [ps_supplycost#130, s_nationkey#85L]\n",
      "      :     :  +- Join Inner, (cast(s_suppkey#82 as bigint) = ps_suppkey#128L)\n",
      "      :     :     :- Project [ps_suppkey#128L, ps_supplycost#130]\n",
      "      :     :     :  +- Join Inner, (cast(p_partkey#24 as bigint) = ps_partkey#127L)\n",
      "      :     :     :     :- Project [p_partkey#24]\n",
      "      :     :     :     :  +- Filter isnotnull(p_partkey#24)\n",
      "      :     :     :     :     +- Relation [p_partkey#24,p_name#25,p_mfgr#26,p_brand#27,p_type#28,p_size#29,p_container#30,p_retailprice#31,p_comment#32] JDBCRelation(part) [numPartitions=1]\n",
      "      :     :     :     +- Project [ps_partkey#127L, ps_suppkey#128L, ps_supplycost#130]\n",
      "      :     :     :        +- Filter (isnotnull(ps_partkey#127L) AND isnotnull(ps_suppkey#128L))\n",
      "      :     :     :           +- Relation [ps_partkey#127L,ps_suppkey#128L,ps_availqty#129,ps_supplycost#130,ps_comment#131] JDBCRelation(partsupp) [numPartitions=1]\n",
      "      :     :     +- Project [s_suppkey#82, s_nationkey#85L]\n",
      "      :     :        +- Filter (isnotnull(s_suppkey#82) AND isnotnull(s_nationkey#85L))\n",
      "      :     :           +- Relation [s_suppkey#82,s_name#83,s_address#84,s_nationkey#85L,s_phone#86,s_acctbal#87,s_comment#88] JDBCRelation(supplier) [numPartitions=1]\n",
      "      :     +- Project [n_nationkey#368, n_regionkey#370L]\n",
      "      :        +- Filter (isnotnull(n_nationkey#368) AND isnotnull(n_regionkey#370L))\n",
      "      :           +- Relation [n_nationkey#368,n_name#369,n_regionkey#370L,n_comment#371] JDBCRelation(nation) [numPartitions=1]\n",
      "      +- Project [r_regionkey#394]\n",
      "         +- Filter isnotnull(r_regionkey#394)\n",
      "            +- Relation [r_regionkey#394,r_name#395,r_comment#396] JDBCRelation(region) [numPartitions=1]\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- ObjectHashAggregate(keys=[], functions=[percentile(ps_supplycost#130, 0.5, 1, 0, 0, false)], output=[median(ps_supplycost)#730])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=2767]\n",
      "      +- ObjectHashAggregate(keys=[], functions=[partial_percentile(ps_supplycost#130, 0.5, 1, 0, 0, false)], output=[buf#736])\n",
      "         +- Project [ps_supplycost#130]\n",
      "            +- SortMergeJoin [n_regionkey#370L], [cast(r_regionkey#394 as bigint)], Inner\n",
      "               :- Sort [n_regionkey#370L ASC NULLS FIRST], false, 0\n",
      "               :  +- Exchange hashpartitioning(n_regionkey#370L, 200), ENSURE_REQUIREMENTS, [plan_id=2759]\n",
      "               :     +- Project [ps_supplycost#130, n_regionkey#370L]\n",
      "               :        +- SortMergeJoin [s_nationkey#85L], [cast(n_nationkey#368 as bigint)], Inner\n",
      "               :           :- Sort [s_nationkey#85L ASC NULLS FIRST], false, 0\n",
      "               :           :  +- Exchange hashpartitioning(s_nationkey#85L, 200), ENSURE_REQUIREMENTS, [plan_id=2751]\n",
      "               :           :     +- Project [ps_supplycost#130, s_nationkey#85L]\n",
      "               :           :        +- SortMergeJoin [ps_suppkey#128L], [cast(s_suppkey#82 as bigint)], Inner\n",
      "               :           :           :- Sort [ps_suppkey#128L ASC NULLS FIRST], false, 0\n",
      "               :           :           :  +- Exchange hashpartitioning(ps_suppkey#128L, 200), ENSURE_REQUIREMENTS, [plan_id=2743]\n",
      "               :           :           :     +- Project [ps_suppkey#128L, ps_supplycost#130]\n",
      "               :           :           :        +- SortMergeJoin [cast(p_partkey#24 as bigint)], [ps_partkey#127L], Inner\n",
      "               :           :           :           :- Sort [cast(p_partkey#24 as bigint) ASC NULLS FIRST], false, 0\n",
      "               :           :           :           :  +- Exchange hashpartitioning(cast(p_partkey#24 as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=2735]\n",
      "               :           :           :           :     +- Scan JDBCRelation(part) [numPartitions=1] [p_partkey#24] PushedFilters: [*IsNotNull(p_partkey)], ReadSchema: struct<p_partkey:int>\n",
      "               :           :           :           +- Sort [ps_partkey#127L ASC NULLS FIRST], false, 0\n",
      "               :           :           :              +- Exchange hashpartitioning(ps_partkey#127L, 200), ENSURE_REQUIREMENTS, [plan_id=2736]\n",
      "               :           :           :                 +- Scan JDBCRelation(partsupp) [numPartitions=1] [ps_partkey#127L,ps_suppkey#128L,ps_supplycost#130] PushedFilters: [*IsNotNull(ps_partkey), *IsNotNull(ps_suppkey)], ReadSchema: struct<ps_partkey:bigint,ps_suppkey:bigint,ps_supplycost:decimal(38,18)>\n",
      "               :           :           +- Sort [cast(s_suppkey#82 as bigint) ASC NULLS FIRST], false, 0\n",
      "               :           :              +- Exchange hashpartitioning(cast(s_suppkey#82 as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=2744]\n",
      "               :           :                 +- Scan JDBCRelation(supplier) [numPartitions=1] [s_suppkey#82,s_nationkey#85L] PushedFilters: [*IsNotNull(s_suppkey), *IsNotNull(s_nationkey)], ReadSchema: struct<s_suppkey:int,s_nationkey:bigint>\n",
      "               :           +- Sort [cast(n_nationkey#368 as bigint) ASC NULLS FIRST], false, 0\n",
      "               :              +- Exchange hashpartitioning(cast(n_nationkey#368 as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=2752]\n",
      "               :                 +- Scan JDBCRelation(nation) [numPartitions=1] [n_nationkey#368,n_regionkey#370L] PushedFilters: [*IsNotNull(n_nationkey), *IsNotNull(n_regionkey)], ReadSchema: struct<n_nationkey:int,n_regionkey:bigint>\n",
      "               +- Sort [cast(r_regionkey#394 as bigint) ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(cast(r_regionkey#394 as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=2760]\n",
      "                     +- Scan JDBCRelation(region) [numPartitions=1] [r_regionkey#394] PushedFilters: [*IsNotNull(r_regionkey)], ReadSchema: struct<r_regionkey:int>\n",
      "\n",
      "time ref: 15.388477563858032\n",
      "time yannakakis: 20.129226207733154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Compare result\n",
    "import time\n",
    "#query = 'tpch-kit/dbgen/queries/postgres/2.sql'\n",
    "#query = 'tpch-kit/dbgen/queries/postgres/13.sql'\n",
    "#query = 'count-3.sql'\n",
    "#query = 'tpch-kit/dbgen/queries/postgres/11.sql'\n",
    "#query = '11-simple.sql'\n",
    "query = 'median-2-hint.sql'\n",
    "#query = 'tpch-kit/dbgen/queries/postgres/7.sql'\n",
    "#query = '13-simple.sql'\n",
    "\n",
    "spark.sql(\"SET spark.sql.yannakakis.enabled = true\").show()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df1 = run_query(query)\n",
    "df1.show()\n",
    "df1.explain(mode=\"extended\")\n",
    "\n",
    "end_time = time.time()\n",
    "yannakakis_time = end_time - start_time\n",
    "\n",
    "spark.sql(\"SET spark.sql.yannakakis.enabled = false\").show()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df2 = run_query(query)\n",
    "df2.show()\n",
    "df2.explain(mode=\"extended\")\n",
    "\n",
    "end_time = time.time()\n",
    "ref_time = end_time - start_time\n",
    "\n",
    "#print(f'row count: {df1.count()} vs. {df2.count()}' )\n",
    "print(f'time ref: {ref_time}\\ntime yannakakis: {yannakakis_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09492e05",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67544c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
